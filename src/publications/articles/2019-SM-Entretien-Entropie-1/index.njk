---
excerpt: Entretien entre B. Stiegler et M. Montévil sur l'entropie et l'anti-entropie dans l'étude du vivant et des techniques et pour les enjeux de l'Anthropocène.
---
<p class="indent titleHead" id="entretien-sur-lentropie-le-vivant-et-la-technique-premiere-partie">Entretien sur l’entropie, le vivant et la technique, première partie</p>

<p class="authors" >Bernard Stiegler<a class="sdfootnoteanc" href="#sdfootnote2sym" id="sdfootnote2anc"><sup>[2]</sup></a>, Maël Montévil<a class="sdfootnoteanc" href="#sdfootnote3sym" id="sdfootnote3anc"><sup>[3]</sup></a></p>

<p class="indent">
  S - Giuseppe Longo, Francis Bailly et toi, avancez le concept d'anti-entropie pour le distinguer du concept de néguentropie, tout en conservant celui-ci. Wiener utilisait lui aussi l’expression « anti-entropique ».
  Qu’est-ce que cela t’inspire ?
</p>
<p class="indent">
  M - Wiener ne parle pas exactement d'anti-entropie, il parle de <i>processus</i> anti-entropiques. J’entends ça comme des processus qui luttent contre l’augmentation d’entropie dans un système. Ce qui diffère du geste théorique
  consistant à poser une anti-entropie comme une quantité « positive », en quelque sorte.
</p>
<p class="indent">
  S - Je comprends cela, et ça m’intéresse d’autant plus d'avoir ton point de vue sur l'histoire de ces notions d’entropie, de néguentropie et d’anti-entropie. Si ces questions sont en rapport avec un travail auquel je m'essaye en ce
  moment, je me les pose depuis beaucoup plus longtemps. Il y a trente ans, quand j’étais à l'université de Compiègne, je m’y suis intéressé en les mettant en relation avec une critique des sciences dites « cognitives », et plus
  généralement, du comportementalisme en découlant. Mais je ne connaissais pas alors le concept d’anti-entropie, jusqu'à ce que je lise le texte de Bailly et Longo<a href="#ref1" ><sup>[1]</sup></a>. Sans avoir jamais abandonné le sujet, je me tenais en réserve
  sur ces questions, sur lesquelles j'avais le sentiment, comme beaucoup de nos collègues, que l’on en venait à dire un peu n'importe quoi, hormis peut-être les thermodynamiciens. Mais eux s’en tenaient à la thermodynamique.
</p>
<p class="indent">
  M - Eux aussi se disputaient beaucoup, et ils continuent à le faire. Ce qu’on accepte en thermodynamique, c’est que l'entropie est bien définie par l'équilibre thermodynamique. Les physiciens, gens subtils, parlent de changement d’un
  système étant en permanence à l’équilibre. Ils regardent des changements dits quasi-statiques où l'on passe d'une situation d'équilibre à une autre de manière infiniment lente. Second aspect du cadre théorique qui rend cela
  véritablement utile : on peut faire un bilan entre deux situations à l’équilibre indépendamment du chemin parcouru de l’une à l’autre. Par exemple, on peut utiliser un chemin calculable entre les deux pour faire ce bilan. Alors que
  le système qui nous intéresse suit un tout autre chemin que l’on ne sait pas nécessairement décrire mathématiquement. Pour les systèmes restant <i>loin de l’équilibre</i>, par contre, la situation théorique n’est pas stable du tout.
  Certains plaident pour un principe de maximisation de la production d'entropie, d'autres pour une minimisation de celle-ci. Ce sont des principes opposés, même s’il s’agit dans les deux cas d'une forme d'optimalité, donc de principes de
  même nature.
</p>
<p class="indent">
  S - Le concept d’entropie est « une bouteille à l’encre », comme le disait déjà Von Neumann à Shannon<a href="#ref2" ><sup>[2]</sup></a>. Il y a cinq ou six ans, il m’est apparu que l’impact biosphérique des activités humaines, l’Anthropocène,
  imposait d’y revenir, et centralement. Avec l’association Ars Industrialis, et ce que nous appelons l’<i>économie de la contribution</i>, on tente d’ailleurs d’apporter une réponse à cette insoutenabilité, réponse précisément basée sur
  la valorisation de l’anti-entropie et de la néguentropie<a href="#ref3" ><sup>[3]</sup></a>.
</p>
<p class="indent">
  Selon le philosophe Mathieu Triclot<a href="#ref4" ><sup>[4]</sup></a>, la confusion vient tout d’abord de malentendus advenus entre Shannon et Wiener portant sur l’information et les fonctions des calculs de probabilité dans sa production, hâtivement
  assimilées au différentiel entropie/néguentropie. Ces concepts ont été repris par une biologie elle-même cognitiviste qui revendique cette notion d’information (définie en référence à l’entropie) et y agglutine les théories des
  structures dissipatives et de l'ordre par le bruit. Cette « synthèse » se voit dans <i>Le cristal et la fumée </i>d’Henri Atlan. Quant à l’idée de Prigogine selon laquelle les structures dissipatives produisent de la
  néguentropie, c’est pour moi un malentendu formel. Une structure dissipative ne produit pas de néguentropie, parce que la néguentropie est produite par le vivant. Processus à la fois temporel et spatial de ce que Jacques Derrida
  appelle une différance, agençant ce que Husserl appelait des <i>rétentions</i> [ce qui est retenu ou recueilli par la conscience, ndr] et des <i>protentions</i> [désirs - et attentes - de l’à venir, ndr]. La différance est la retenue
  d’une mémoire où la flèche du temps (de Prigogine) n’est pas réductible au devenir entropique, mais se constitue précisément comme possibilité d’une bifurcation dans ce devenir, et, en cela, comme mise en réserve d’un avenir
  (néguentropique) irréductible au devenir (entropique). Ce qui ne correspond pas aux structures dissipatives de Prigogine.
</p>
<p class="indent">
  Cependant, quand Bailly, Longo et toi introduisez le concept d'anti-entropie comme potentiel dynamique pour le distinguer de la néguentropie en tant que description d’un niveau de complexité, cela permet de décrire aussi, si j’ai bien
  compris, l’ordre constitué par les structures dissipatives comme un niveau de néguentropie. Et ce n'est plus gênant car il y a cet autre concept, l'anti-entropie, qui prend en charge la « différance », c’est-à-dire la
  temporalité et l’historicité spécifique au vivant. Ce concept s’apparente aussi au <i>diachronique</i> de Saussure. Avant d’étudier la philosophie, j'ai envisagé de faire de la linguistique saussurienne. Mon ambition était de trouver
  une issue au problème méthodologique de la diachronie dans sa linguistique, pour laquelle – un peu comme dans les relations dites d’incertitudes en mécanique quantique –, plus on gagne en capacités de description synchronique, moins on
  peut décrire le diachronique, et réciproquement. Une autre avancée conceptuelle, au-delà de l’<i>opposition</i> diachronique/synchronique, est la théorie de l'individuation de Gilbert Simondon. Le
  <i>saut quantique de l'individuation</i>, comme il le nomme, correspond au nœud de la diachronie chez Saussure. Et ce qui rend possible ce saut, c’est ce qu’il appelle la sursaturation du système en tant que celui-ci constitue un
  « fond préindividuel » [fond supposé par tout processus d’individuation et partagé par tous les individus psychiques]. Le synchronique et le diachronique ont en outre tout à voir avec l’entropie et la néguentropie, même si
  c’est de façon toujours éminemment paradoxale. Mais Simondon s’est égaré avec sa « notion d’information »<a href="#ref5" ><sup>[5]</sup></a> [pour lui non-quantifiable et subjective]. Le concept d’anti-entropie semble donc permettre de résoudre la
  difficulté.
</p>
<p class="indent">
  Ces questions reviennent d’ailleurs au premier plan. Car une critique de l’économie politique contemporaine, c'est-à-dire du capitalisme actuel, passe par un réarmement conceptuel de l’économie autour de ce que Longo et toi appelez
  l'anti-entropie. Étendre ces questions-là au champ de l’économie pose cependant un problème très spécifique, immense, passionnant. Car là, on n’a pas simplement affaire à du vivant, mais à de la matière inorganique
  organisée<a href="#ref6" ><sup>[6]</sup></a>: à des organes artificiels ne répondant pas aux lois du vivant [l’organique se dote d’organes inorganiques, poursuivant sa différenciation par d’autres moyens que la vie]. Cette situation produit des
  <i>états de fait</i> qui ne correspondent à aucune loi ou principe rationnel car la science ou le savoir qui permettraient de les décrire n'existent pas. Simondon a fait une tentative via ce qu’il a appelé la mécanologie [science des
  machines], dont je reprends moi-même des éléments dans le cadre de ce que j’appelle <i>l'organologie </i>[analyse conjointe de l’histoire et du devenir des organes physiologiques, des organes artificiels et des organisations sociales].
  Ce qui n'est pas une science, mais juste un corps de concepts ayant pour but avant tout de permettre des agréments méthodologiques entre des sciences du vivant, de la technique et des organisations.
</p>
<p class="indent">
  Cette méthode est au cœur du projet d’économie contributive. Le principe de l'organologie générale est qu'un organe artificiel est certes au service d'un être vivant, et est donc bien inscrit dans une problématique d'horizon vital, mais
  qu’il ne s'agit pas d'une réalité biologique. Ainsi l’économiste et mathématicien Georgescu Roegen soutient que cet organe n'est pas vivant mais est vital pour une espèce dont il modifie la trajectoire évolutive des organes organiques
  (au sein des organismes), et il est ce dont les caractéristiques sont réglées par l’économie qui constitue elle-même un processus d’<i>exosomatisation</i>. Les organes artificiels s’agencent avec des organes vivants, des organisations
  vivantes, au sein desquelles ils forment ce que j’appelle moi-même des <i>exorganismes</i>, en référence à la terminologie du mathématicien Alfred <u>Lotka</u>, qui parle d’organes exosomatiques. De même Georgescu-Rœgen voit l’économie
  en tant qu’activité de production et d’échange de ces organes. Ce qui vient se substituer à la biologie, pour le meilleur et pour le pire : se présentant ici comme entropie, néguentropie et anti-entropie. Le « meilleur »
  permet, par cette « exosomatisation », d’améliorer la vie, c’est-à-dire sa teneur néguentropique et ses potentiels anti-entropiques. Le « pire » la dégrade par de mauvais agencements économiques, ou plutôt
  déséconomiques. Soit des effets entropiques qu’il faut aussi dire anthropiques au sens où le rapport 2014 du GIEC<a href="#ref7" ><sup>[7]</sup></a> (Groupe d’experts intergouvernemental sur l’évolution du climat) parle des « 
  <i>anthropogenic forcings</i> ».
</p>
<p class="indent">
  L'anti-entropie, si j’ai bien compris, installe une nouvelle dynamique dans un ordre. Elle y apparaît d’abord comme une espèce de désordre, un écart, telle la diachronie saussurienne, un écart qui est au service d’une évolution, et non
  seulement d’un maintien de l’organisation existante. Cette anti-entropie renvoie à un processus diachronique dans un ordre qui est synchronique. Le diachronique vient créer une perturbation dans l’ordre synchronique. C’est là que
  Simondon nous intéresse derechef. Avec les concepts de métastabilité et de saut quantique présents dans l'individuation, il montre que l'ordre synchronique, en étant métastable, est donc dynamique. Mais il montre aussi que cette
  métastabilité est polarisée par des tendances à l’équilibre et au déséquilibre. Que l’ordre soit un tel procès, cela signifie que c’est un ordre <i>dans le temps</i>. Il y a des écarts dans le temps par rapport au procès, qui vont
  enrichir le procès et y ajouter des fonctions anti-entropiques. Cela, on peut le décrire dans le champ de la langue comme dans ceux d’autres systèmes sociaux. En outre, on est ici dans le social et le symbolique, et non dans le
  biologique. Donc en tant qu’organe socialement élaboré, la langue appartient déjà à l’exosomatisation.
</p>
<p class="indent">
  Comment penser alors une anti-anthropie qui viendrait modifier les ordres anthropiques et étendre la distinction entre néguentropie et anti-entropie aux agencements d’organes endosomatiques et exosomatiques formant des exorganismes — et
  dans un champ que l’on appellerait néguanthropologique ? C’est la question que nous tentons d’instruire en vue de penser l’économie d’un « Néguanthropocène » à venir. Dans une telle perspective,
  l'« anti-anthropie » est une fonction du savoir, et un savoir est ce qui produit des bifurcations dans un réel qui est entropique (et anthropique) et où il s’agit de maintenir et d’entretenir une néguentropie (et une
  néguanthropie). Un état n’est jamais réellement stationnaire parce qu'il est pris dans un processus irréversiblement entropique, il est toujours en dégradation même si ce n'est pas sensible et mesurable dans l'instant. Le savoir, quel
  qu’il soit (savoir vivre, faire, concevoir) vient toujours soigner un état apparemment stationnaire, et créer les conditions pour que cet état stationnaire s'enrichisse et devienne non-stationnaire ; non pour se dégrader et aller
  vers le désordre, mais pour devenir plus riche et s'augmenter de fonctions lui permettant de lutter mieux contre le désordre. La question de l'anti-anthropie, c'est celle de l’exercice du savoir – qui est cependant dilué et désintégré
  par l’information, et le modèle capitaliste devenu à présent ultra-computationnel où le savoir est dissout dans l’information elle-même de part en part calculable et intrinsèquement entropique – et c’est ce dont l’Anthropocène est le
  résultat.
</p>
<p class="indent">
  M - Le concept d’anti-entropie vient d'abord de Bailly et Longo<a href="#ref8" ><sup>[8]</sup></a>. D’ailleurs, si Bailly n'est pas un disciple de Prigogine, il a travaillé quelques années dans son laboratoire dans les années 70. Il y a donc une légère
  filiation. La première version du concept d'anti-entropie porte surtout sur l’idée de développer un concept différent de l'entropie et pas juste une entropie négative, comme est généralement comprise la néguentropie en physique. On
  aborde le rapport entre l’entropie et l’anti-entropie par analogie avec celui qu’a la matière avec l’anti-matière. L’anti-matière est similaire à la matière mais possède certaines propriétés qui lui sont opposées et, surtout, elle a une
  existence en propre et n’est pas une absence de matière. De même, l’anti-entropie est similaire à l’entropie, opposée à elle, mais distincte tant que l’organisme est vivant. La mort se caractérise alors par la transformation de
  l’anti-entropie en néguentropie physique laquelle est suivie par une production d’entropie car la complexité de l’être vivant n’est plus maintenue. L'anti-entropie est donc associée à plusieurs aspects biologiques, notamment
  morphologiques. Sa première application est la question de la complexité du vivant. Pour certains biologistes de l'évolution, notamment, il n’y a pas de différence de complexité entre, à la limite, une bactérie et un éléphant. Si l’on
  considère la taille des génomes, les organismes plus complexes sont ceux des plantes : certaines possèdent des génomes très longs, le blé par exemple a un génome plus de mille fois plus grand que le nôtre. Mais est-ce cela, la
  complexité biologiquement pertinente ? Dans la lignée du biologiste de l’évolution Stephen J. Gould, on cherche d’abord à définir une complexité qui corresponde à des aspects morphologiques et aussi, dans une certaine mesure,
  physiologiques, et de discuter ses changements et ses conséquences. Par exemple, regarder les changements de cette complexité dans l'évolution en considérant que ces variations sont purement aléatoires dans une lignée<a href="#ref9" ><sup>[9]</sup></a>. Ces
  hypothèses conduisent à une augmentation de la complexité moyenne et ceci sans faire l’hypothèse d’une sélection naturelle qui favoriserait la complexité. Ensuite, il y a une deuxième idée, qui vient plutôt de Bailly, qui est d'ajouter
  l’anti-entropie aux équations faisant les bilans thermodynamiques, en lien avec les travaux de Prigogine. Dans ce cas-là, on obtient les situations physiques lorsque l’anti-entropie est nulle, autrement dit le physique est un cas limite
  du vivant, qui apparaît lorsque les aspects liés au vivant disparaissent. Il s’agit donc d’une extension de la physique par une quantité supplémentaire proprement biologique. Ce geste théorique permet d'étudier un certain nombre de
  conséquences à différents niveaux, en particulier en faisant des bilans métaboliques, c’est un sujet sur lequel Boris Saulnier a travaillé aussi pendant sa thèse, avec Longo et Bailly. Ensuite, j'ai proposé une relecture de ces
  questions dans ma propre thèse, relecture qui vient notamment de ma confrontation avec la question de la diachronicité et de la synchronicité en biologie, à laquelle je suis arrivé par un chemin assez détourné. Soit la question de ce
  que Bailly et Longo appellent la criticité étendue. Soit encore l'idée est que le vivant est en permanence dans une situation qui, en physique, est un état de transition ponctuel.
</p>
<p class="indent">S - Il me semble que cette notion de criticité étendue est très proche de ce que Simondon désignait comme processus d’individuation. Longo connaît-il Simondon ?</p>
<p class="indent">
  M - Il le connaît au moins par le philosophe Paul-Antoine Miquel qui travaille sur les liens entre les deux notions<a href="#ref10" ><sup>[10]</sup></a>, par contre je ne sais pas si Bailly connaissait Simondon. Longo et Bailly se concentraient sur la
  criticité étendue à travers la question de la cohérence entre les différentes parties d'un organisme<a href="#ref11" ><sup>[11]</sup></a>. En effet, un système dans un état critique est typiquement dans une situation entre ordre et désordre conduisant à la
  formation d'une structure de cohérence multi-échelle. Moi, je me suis concentré sur un second aspect des situations critiques en physique : le fait qu’elles constituent un point de passage d'une configuration macroscopique à une
  autre. Mathématiquement, il s'agit typiquement d'un changement de symétrie. Dans les situations critiques étendues, on est alors confronté à des points de passages un peu partout, des bifurcations, en un sens, même s'il y a des
  différences avec le sens mathématique précis de bifurcation qui renvoie au cadre mathématique des systèmes dynamiques. On a alors un problème qui est d’abord un problème épistémologique<a href="#ref12" ><sup>[12]</sup></a>. Ce sont en effet les symétries qui
  permettent d'encadrer théoriquement la description d'un objet en physique et de faire de la physique théorique par les mathématiques. C’est donc bien ici la méthodologie de la théorisation qui est en jeu. Derrière ces cascades de
  « bifurcations », il y a l'historicité du vivant. Cette historicité fondamentale résonne avec la théorie de l'évolution, mais ce n’était pas mon point de départ. Je venais des mathématiques et les physiciens et même
  certains biologistes ont l'idée que l'on peut séparer l'analyse d’une situation à un moment donné de son inscription dans une histoire naturelle, c'est-à-dire séparer les aspects synchroniques des aspects diachroniques. Cela m'a conduit
  à réinterpréter l'anti-entropie via cette notion d’historicité et donc via l’idée d’une cascades de changements de symétries. Cette idée était quand même sous-jacente dès l’origine puisque l’idée d’une augmentation de la complexité
  implique l'introduction de nouveaux éléments au sein de cette complexité. Mais l’apport consiste à insister sur la dépendance à l'histoire. L'historicité se présente de deux manières différentes : quand on regarde vers l'avenir et
  quand on regarde vers le passé. Vers le passé, tous les êtres vivants sont issus de trois milliards et demi d'années d'évolution, et cette histoire importe. Il y a une histoire massive qui est nécessaire pour comprendre l'organisation
  du vivant, laquelle n’est pas optimale. Si elle l’était, on pourrait se passer de l’histoire - ce que font Prigogine et les économistes néoclassiques -, pour prendre deux exemples dans des registres et avec des cadres techniques très
  différents. Les organisations biologiques ne sont pas optimales. Ainsi certains nerfs ont des trajets dans le corps qui ne peuvent être compris que par cette histoire. Par rapport à l'avenir, se pose la question des possibles dont on
  reparlera après.
</p>
<p class="indent">S - Il serait intéressant quant à cette question des possibles de revenir à ce que dit Kant dans la <i>Critique de la Raison Pure</i> sur la cosmologie rationnelle et sur les séries vers le passé et les séries vers l'avenir.</p>
<p class="indent">
  M - J'ai réinterprété l'anti-entropie sur la base de ces idées-là. C’est un travail en cours avec quelques éléments déjà publiés. Pour moi, l'anti-entropie dépend nécessairement d'une histoire sous-jacente. Les structures dissipatives
  au sens de Prigogine, par exemple, n’ont pas d’anti-entropie ou sinon ont une anti-entropie minimale de l'ordre de 0+ comme disent les mathématiciens. Avec ces objets, on n'est pas encore dans quelque chose qui a une histoire. Nous
  avons critiqué l'idée de voir les organismes comme des structures dissipatives et ce genre de paradigmes sur cette base-là<a href="#ref13" ><sup>[13]</sup></a>. Les physiciens et Prigogine en particulier cherchent une fonction tendant vers un maximum (ou un
  minimum) ce qui leur permet de déduire mathématiquement le comportement du système. C’est à la base de leurs raisonnements physico-mathématiques. Or raisonner ainsi est à l'opposé d'une situation vraiment historique, car cela suppose
  qu’il n’y ait pas d’événements décisifs. Pour nous, il n'y a pas de fonction mathématique ayant ce rôle théorique en biologie, et l'histoire est décisive pour savoir ce qui est. Dans la manière dont j'envisage actuellement
  l’anti-entropie, j'utilise aussi des réflexions menées par la suite avec le philosophe Matteo Mossio sur l'organisation biologique - vue comme clôture entre contraintes, c’est-à-dire à travers l'interdépendance des parties d'un
  organisme<a href="#ref14" ><sup>[14]</sup></a> - et cette manière de voir les fonctions, mais sans perdre de vue les aspects diachroniques. Ce que j'envisage comme anti-entropie pour la biologie, c'est une notion de complexité en tant qu'elle est
  fonctionnelle. Ici, dire que cette complexité est fonctionnelle renvoie à la fois au fait qu’elle provienne d’une histoire et soit constituée par elle, et au fait aussi qu'elle corresponde à des interdépendances systémiques. L’idée
  <i>in fine</i> est de développer un point de vue entre diachronique et synchronique. L'augmentation d’anti-entropie implique des bifurcations et des changements de symétrie, mais seules sont pertinentes celles qui changent les
  interdépendances des organismes. En ce sens, l’anti-entropie renvoie à la constitution d'une histoire et non à l’agrégation aléatoire d’éléments homogènes. Par exemple, dans le développement, il y a augmentation de l'anti-entropie. Le
  développement implique des bifurcations correspondant à la mise en place de fonctions. Le développement biologique n'est pas le dépliement de quelque chose de plié, ce qui renverrait à un préformationnisme dont l’étendard classique est
  l’homonculus, le futur petit homme, imaginé dans le spermatozoïde, et dont l’avatar moderne se retrouve dans certains usages de la notion d’information en biologie. De la même manière, le petit être humain n’acquiert pas les capacités
  d’attention, de mouvement et d’orientation dans l’espace, etc., par la nécessité d’un programme développemental. Au contraire, dans la petite enfance, les relations avec les jouets et les parents sont nécessaires à la mise en place de
  ces capacités, or ces relations sont parfois coupées par les écrans comme la télévision ou encore les smartphones. Nous travaillons sur ce problème avec la pédopsychiatre Marie-Claude Bossière à Plaine Commune, dans la
  Seine-Saint-Denis. En biologie, le groupe travaillant sur une théorie des organismes auquel je participe s’est aussi penché sur un cas particulier intéressant : le cas du cancer qui montre l’intérêt de la notion d’anti-entropie en
  lien avec la fonctionnalité. Dans une tumeur, il y a augmentation de complexité, on pourrait dire augmentation de néguentropie au sens physique du terme, mais il y a une baisse de la fonctionnalité parce que, par exemple, dans le cas du
  cancer du sein, les canaux des glandes mammaires sont obstrués par la morphogenèse cancéreuse dont le résultat ne permet plus le passage du lait. Donc dans un cancer il y a augmentation de néguentropie physique (complexité
  morphologique) et baisse d’anti-entropie (fonctionnelle)<a href="#ref15" ><sup>[15]</sup></a>.
</p>
<p class="indent">
  S - De semblables situations s’observent dans les organisations sociales souffrant de « déficits fonctionnels », et pas seulement les institutions publiques comme le montre l’analyse de l’anthropologue David Graeber sur les
  entreprises du capitalisme<a href="#ref16" ><sup>[16]</sup></a>. Mais tu parles de clôture organisationnelle : quel est le rapport avec la « clôture opérationnelle » de Francisco Varela ?
</p>
<p class="indent">
  M - Les deux notions se placent dans la même tradition. En fait le premier à avoir utilisé le terme de clôture pour ce genre d'idée est Jean Piaget. Dans tous les cas, il s'agit de mettre en évidence une circularité pour un système qui
  est par ailleurs ouvert au sens thermodynamique du terme, c'est-à-dire qui est traversé par des flux d'énergie et de matière. L’idée, avec la notion de clôture, est en général de concilier une circularité causale avec l’ouverture du
  système au sens thermodynamique. La clôture s’oppose donc à la fermeture. Elle permet aussi de concilier l’autonomie et l’hétéronomie. L’autonomie correspond à la circularité causale de la clôture et l’hétéronomie correspond à
  l’existence de contraintes qui ne sont pas maintenue par l’organisme, mais dont des parties de l’organisme dépendent. Ainsi la température impacte de nombreux processus chimiques ayant lieu dans un organisme. Il s’agit d’une quantité
  indépendante de l’organisme dans le cas des bactéries, par exemple, mais cette contrainte peut être internalisée, c’est-à-dire maintenue par l’organisme dans le cas des mammifères, par exemple. Chez Varela, le concept d'autopoïèse
  désigne l'idée que les composants d'un organisme sont produits par l'activité de cet organisme. La limite de ce concept est qu'il dépend fondamentalement de la définition de ce que sont ces composants, ce qui, en général, est interprété
  en termes chimiques. Autrement dit, maintenir ses composants, pour l’organisme, ce serait essentiellement produire les molécules qui constituent l'organisme. Ce qui laisse de côté toutes sortes d’aspects mésoscopiques ou macroscopiques,
  par exemple l’organisation dans l’espace de la matrice extracellulaire, ou la forme d’un os, qui peuvent être très différents à composition chimique égale. Le cadre que Matteo Mossio et moi-même avons développé décrit une clôture entre
  contraintes où les contraintes sont un type d'entité théorique distincte de ce qu'on a appelé processus<a href="#ref17" ><sup>[17]</sup></a>. Il faut bien préciser que quand on parle d'entités théoriques, il ne s'agit pas de désigner théoriquement des entités
  matérielles, comme une molécule par exemple, il s'agit d’un aspect d'un objet alors qu'un autre aspect peut être catégorisé comme processus. Nous considérons donc d'un côté les contraintes qui ont une certaine stabilité formelle à une
  échelle donnée et qui, à une autre échelle, peuvent au contraire disparaître spontanément. De l’autre côté, nous avons des processus qui sont des processus de transformation et qui peuvent maintenir ou produire ces contraintes. La
  clôture entre contraintes décrit alors une situation où on a typiquement une contrainte agissant sur un processus, lequel produit une autre contrainte agissant sur un autre processus et ainsi de suite jusqu'à une contrainte qui agit sur
  un processus produisant ou maintenant la contrainte de départ, ce qui nous amène à une circularité. Cette circularité permet de décrire les contraintes en termes de fonction puisque <i>in fine</i> l'existence même d’une contrainte dans
  ce type de système va dépendre de ses effets via le reste du système et sa structure causale circulaire.
</p>
<p class="indent">
  S - J’aimerais revenir à la théorie de l’autopoïèse de Humberto Maturana et Varela et à son usage dans les sciences cognitives, mais aussi dans la théorie des systèmes sociaux du sociologue Niklas Luhmann. Le problème que me pose la
  convocation de ce concept dans les sciences cognitives est qu’il fait l’impasse sur ce qu’il faut appeler l’hétéropoïèse que constituent les organes et organisations exosomatiques pour les organes et organismes endosomatiques. Et il en
  va de même quant aux systèmes sociaux de Luhmann dans leurs rapports au système technique, qui reste chez lui inexistant. Si ce que j’ai pu avancer précédemment à propos des savoirs est vrai, ceux-ci ne sauraient être rapportés à la
  cognition pensée à partir de l’individu. Que celui-ci soit conçu d’un point de vue computationnaliste, connexionniste ou autopoïétique et énactif. Les savoirs sont toujours le fait de groupes, et ceux-ci se constituent à partir
  d’organes techniques qu’ils produisent, échangent, partagent, ou au contraire monopolisent, etc. Tout cela toujours en fonction de règles qui sont précisément fournies par ces savoirs, lesquels légitiment mais aussi critiquent et
  parfois combattent des institutions et systèmes sociaux, etc. C’est à partir de telles considérations que je parle depuis quelques années d’anti-anthropie hétéropoïétique, c'est-à-dire une anti-anthropie qui se produit dans ce qui n'est
  plus un organisme mais une organisation, une organisation au sens courant du terme — c'est-à-dire une organisation sociale. La question est alors de penser et de <i>panser</i> la relation entre organisation biologique et organisation
  exosomatique. Ce que tu dis au sujet du cancer et d’une augmentation de complexité qui paradoxalement produit <i>in fine</i> une augmentation de l'entropie, c'est ce qui frappe d’innombrables institutions, particulièrement dans le
  contexte contemporain de crise et de dysfonctionnements liés aux désordres de l’Anthropocène et à la fermeture apparente de l’avenir. Quant au rôle inventif de la maladie – qu’il faudrait rapprocher de la quasi-causalité que Gilles
  Deleuze évoque à partir de la logique des Stoïciens<a href="#ref18" ><sup>[18]</sup></a> -, c’est la base du raisonnement du philosophe Georges Canguilhem<a href="#ref19" ><sup>[19]</sup></a> tel qu’il concentre aussi bien la vie endosomatique que la vie exosomatique.
</p>
<p class="indent">
  M - Oui, il y a d’ailleurs une difficulté ou, en tout cas, une subtilité lorsqu'on met ensemble l'aspect diachronique et l'aspect synchronique. La difficulté est qu'il y a toujours la possibilité que quelque chose qui est pathologique
  ou qui n'est pas fonctionnel devienne fonctionnel par la suite, c’est-à-dire que l'organisme ou l’évolution arrive à lui conférer une fonction. Le cas du cancer n’est pas le plus facile pour illustrer cette idée. Néanmoins un
  biophysicien et un sociologue ont travaillé ensemble pour reconsidérer le caractère monstrueux du cancer à travers l'idée qu’en biologie, en général, le monstre est aussi le lieu de l'évolution<a href="#ref20" ><sup>[20]</sup></a>.
</p>
<p class="indent">
  S - Il me semble qu’ici il faudrait revenir sur la notion d’infidélité du milieu de Canguilhem, sur la pathogenèse et la normativité qui en procède, et sur les sens différents qu’on peut lui donner selon qu’il s’agit de la vie
  endosomatique ou de la vie exosomatique<a href="#ref21" ><sup>[21]</sup></a>. Il n’y a pas tellement de gens qui s'y intéressent vraiment et, par ailleurs, je me pose beaucoup de questions sur Canguilhem.
</p>
<p class="indent">
  M - Il y a des exceptions. Des biologistes tels Ana Soto et Carlos Sonnenschein avec qui je travaille, ont lu attentivement Canguilhem. Certes ils l'utilisent un peu explicitement. Mais Canguilhem est surtout une référence pour eux et
  ce qu'ils font et ce qu'ils disent est compatible et va dans une direction similaire à la sienne, en tout premier lieu sur ce qu’est une norme biologique.
</p>
<p class="indent">
  S - Il me semble que ce que tu viens de dire sur l'organe déficient pouvant acquérir une nouvelle fonction est un cas de normativité au sens de Canguilhem, mais chez lui, cela concerne aussi les milieux techniques ; et c'est la
  base de l'évolution de la technique. Par exemple, quand Simondon analyse les moteurs thermiques et le passage du moteur Lenoir au moteur Diesel, c'est la « maladie » du moteur Lenoir qui invente le moteur diesel. Je pense
  qu'il y a beaucoup de choses à réfléchir là-dessus.
</p>
<p class="indent">
  M - Il y a un autre cas dont je ne sais pas s’il peut être analysé exactement comme cela historiquement, mais qui illustre bien l’utilisation fonctionnelle <i>a posteriori</i> des écarts par rapport au fonctionnement normal, tout en
  étant intéressant pour les rapports entre entropie et fonction. Le calcul informatique correspond à un processus déterministe et prédictible, modélisé par exemple par la machine de Turing. L’aléatoire, utilisé par exemple pour faire des
  simulations, est en fait du pseudo-aléatoire utilisant des fonctions déterministes, prenant une variable en entrée appelée « seed », et ayant des propriétés mathématiques reproduisant certaines propriétés d’un tirage aléatoire
  lorsque cette variable change (la variable est incrémentée à chaque utilisation de la fonction lors d’une session). Mais en cryptographie, cela n’est pas suffisant car on veut générer aléatoirement des clefs secrètes et si l’adversaire
  utilise la même fonction avec la même seed, alors il obtiendra exactement la même clef. Il suffit que le nombre de seed probables soit relativement petit, pour qu’il y ait une voie utilisable pour casser le cryptage. Quand on utilise le
  logiciel de cryptographie libre Gnupg, qui fait référence, l’interface demande, dans certaines situations, à l’utilisateur de bouger la souris, de faire n'importe quoi au clavier, etc., pour augmenter l'entropie du système. L’idée est
  qu’il faut introduire de l’aléatoire provenant d'autre chose que du calcul numérique, au sens d’une machine de Turing, et cet aléatoire est évalué en termes d'entropie. En pratique, cette entropie vient de sources diverses, regroupées
  par exemple par un <i>Entropy Gathering Daemon</i> (un processus collecteur d’entropie) ou par le noyau Linux lui-même, qui utilise, en plus des activités de l’utilisateur, la température, la vitesse des ventilateurs et d'autres
  variables matérielles, analogiques. On obtient ainsi de l’aléatoire utilisable pour la cryptographie au sens où il ne peut pas être produit à l'identique en parallèle. Dans ce cas-là, les aspects analogiques de l’ordinateur, le fait que
  le matériel ne soit pas purement digital, ce qui devrait être pathologique lorsque l’on prend la machine de Turing comme norme, devient fonctionnel. De plus, une production d'entropie à un niveau, celui du matériel et plus généralement
  celui des données collectées par le <i>daemon</i>, est fonctionnel à un autre niveau, celui de la cryptographie et de son rôle social. Au niveau où l'entropie est une dispersion maximale, il n'y a pas de fonction, mais au niveau du
  dispositif cryptographique, l'entropie du premier niveau devient fonctionnelle, car elle permet le secret. Ce genre de situations est fréquent en biologie. Par exemple, une molécule qui est produite dans une cellule à un endroit de la
  cellule diffuse dans le cytoplasme ce qui va lui permettre de rencontrer un récepteur ou une autre molécule partenaire et donc d’avoir un rôle fonctionnel. Or la diffusion est bien un exemple paradigmatique de production d’entropie. La
  production d'entropie participe au fonctionnement du système au-delà de la notion d’énergie libre, donc la production d'entropie, ici, participe à l'anti-entropie, ce qui ne pose pas de problème une fois que les termes de la discussion
  sont bien posés.
</p>
<p class="indent">
  S - Cela ouvre des questions très intéressantes d'une philosophie de la fonction, il faudrait aujourd'hui relancer l'analyse de ce qu’est la fonctionnalité, en particulier avec le philosophe et mathématicien Alfred Whitehead et son
  discours sur la « fonction de la raison » [vivre, vivre bien, vivre mieux], en intégrant les nouvelles notions fonctionnalistes requises par la prise en compte de l’exosomatisation, de son fonctionnement exorganique et de ses
  dysfonctionnements. Ceci permettrait de surmonter les fonctionnalismes souvent sommaires issus du behaviourisme et du cognitivisme – au moment même où l’on parle d'économie de la fonctionnalité – notamment dans le programme de Plaine
  Commune.
</p>
<p class="indent">
  M – Quand on étudie la philosophie analytique on trouve beaucoup de choses sur les fonctions en biologie. Deux grandes voies sont suivies. La première, dominante, est de dire que telle chose est une fonction parce qu’elle a été
  sélectionnée positivement à cause de ses effets. Mais cette définition est très peu opératoire en pratique parce qu’argumenter empiriquement sur l’origine d’un trait est difficile. Une autre définition est plus systémique au sens
  physique de système, donc synchronique et sans réelle diachronicité. Une des versions les plus fines a été formulée par Matteo Mossio en termes organisationnels : l’organisation est pris dans la lignée des travaux de Varela ou du
  théoricien de la biologie Robert Rosen, mais aussi de la pensée de Kant, avec une perspective philosophique certes différente. Mossio avance que l'organisation au sens de l’interdépendance des parties d'un organisme permet de fonder la
  notion de fonction, parce qu’à travers la circularité, l’existence d’une partie va dépendre de ses conséquences. Je pense qu'il faudrait faire se rejoindre ces deux cadres, l'un plus diachronique, l'autre plus synchronique. Ce qui est
  extrêmement difficile. J’ai déjà travaillé dans cette direction, notamment avec Mossio, mais plutôt par la question d’un cadre théorique général pour les organismes que directement par la question des fonctions.
</p>
<p class="indent">
  S - Ce qui fait qu'un objet technique est un organe, c’est le fait qu'il fonctionne. Là, on n’emploie évidemment pas le terme de fonction dans le même sens qu'un biologiste ou qu'un mathématicien. Mais il faudrait une théorie des
  fonctionnalités permettant de rendre compte des agencements possibles de fonctionnalités hétérogènes et cependant cohérentes d’un point de vue « néguanthropique ».
</p>
<p class="indent">
  M - Le théoricien de la biologie Stuart Kauffman est aussi un auteur intéressant pour la question des fonctions. Il lie d’ailleurs les aspects exosomatiques et les aspects somatiques, puisqu’une question qu’il utilise souvent est celle
  des usages possibles d'un tournevis. Question qui est proche de celle de la normativité, même si, ici, il n'y a pas la dimension de la pathologie. Cette question est utilisée pour discuter la nature des possibles en biologie. Ce que
  l'on affirme, avec Longo et Kauffman, c'est que cet ensemble des usages possibles est de taille indéfinie, et non pas infinie, ce qui est en un sens beaucoup plus difficile<a href="#ref22" ><sup>[22]</sup></a>.
</p>
<h2 class="sectionHead" id="references">Références</h2>
  <ol class="thebibliography ">
    
    <li class="bibitem" id="ref1" ><sup>1</sup> Bailly, F., &amp; Longo, G. (2009). Biological organization and anti-entropy. <i>Journal of Biological Systems</i>,
      <i>17</i>, 63–96.
    </li>
<li class="bibitem" id="ref2" ><sup>2</sup> M. Tribus, E.C. McIrvine (1971), Energy and information, <i>Scientific American</i>, 224.
    </li>
<li class="bibitem" id="ref3" ><sup>3</sup> Sur l’économie de la contribution
      [soit un modèle de création de valeur basé sur la contribution, ndr]
      et sa mise en œuvre sur le territoire de Plaine Commune, cf.
      <i>recherchecontributive.org</i>
       ; sur la première définition de l’économie de la contribution, cf.
      http://arsindustrialis.org/vocabulaire-economie-de-la-contribution, et sur sa définition la plus récente (2017), cf. le
      <i>Dictionnaire des communs</i>, Presses universitaires de France.
    </li>
<li class="bibitem" id="ref4" ><sup>4</sup> Mathieu Triclot,
      <i>Le moment cybernétique : La constitution de la notion d'information</i>, Champ-Vallon.
    </li>
<li class="bibitem" id="ref5" ><sup>5</sup> Gilbert Simondon,
      <i>L’individuation à la lumière des notions de forme et d’information</i>, Jérome Muillon.
    </li>
<li class="bibitem" id="ref6" ><sup>6</sup> Sur cette notion, cf. Bernard Stiegler,
      <i>La technique et le temps 1. La faute d’Épiméthée</i>, Galilée.
    </li>
<li class="bibitem" id="ref7" ><sup>7</sup> Myhre, G., D. Shindell, F.-M. Bréon, W. Collins, J. Fuglestvedt, J. Huang, D. Koch, J.-F. Lamarque, D. Lee, B. Mendoza, T.
      Nakajima, A. Robock, G. Stephens, T. Takemura and H. Zhang, (2013): Anthropogenic and Natural Radiative Forcing. In: <i>Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change</i>
      [Stocker, T.F., D. Qin, G.-K. Plattner, M. Tignor, S.K. Allen, J. Boschung, A. Nauels, Y. Xia, V. Bex and P.M. Midgley (eds.)]. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA.
    </li>
<li class="bibitem" id="ref8" ><sup>8</sup> Bailly, F., &amp; Longo, G. (2009). Biological organization and anti-entropy, <i>op. cit</i>.
    </li>
<li class="bibitem" id="ref9" ><sup>9</sup> Longo, G., &amp; Montévil, M. (2014). Biological order as a consequence of randomness: Anti-entropy and symmetry changes. In <i>Perspectives on Organisms</i>
      Lecture Notes in Morphogenesis (pp. 215–248). Springer Berlin Heidelberg.
    </li>
<li class="bibitem" id="ref10" ><sup>10</sup> Miquel, P. A., &amp; Hwang, S. Y. (2016).
      From physical to biological individuation. <i>Progress in biophysics and molecular biology</i>, <i>122</i>
      (1), 51-57.
    </li>
<li class="bibitem" id="ref11" ><sup>11</sup> Bailly, F., &amp; Longo, G. (2008). Extended critical situations: the physical singularity of life phenomena. <i>Journal of Biological Systems</i>,
      <i>16</i>, 309.
    </li>
<li class="bibitem" id="ref12" ><sup>12</sup> Bailly, F., &amp; Longo, G. (2008). Extended critical situations: the physical singularity of life phenomena. <i>Journal of Biological Systems</i>,
      <i>16</i>, 309.
    </li>
<li class="bibitem" id="ref13" ><sup>13</sup> Longo, G., Montévil, M., Sonnenschein, C., &amp; Soto, A. M. (2015). In search of principles for a theory of organisms. <i>Journal of Biosciences</i>, (p. 1–14).
    </li>
<li class="bibitem" id="ref14" ><sup>14</sup> Montévil, M., &amp; Mossio, M. (2015). Biological organisation as closure of constraints. <i>Journal of Theoretical Biology</i>,
      <i>372</i>, 179 – 191.
    </li>
<li class="bibitem" id="ref15" ><sup>15</sup> Longo, G., Montévil, M., Sonnenschein, C., &amp; Soto, A. M. (2015). In search of principles for a theory of organisms, <i>op. cit</i>.
    </li>
<li class="bibitem" id="ref16" ><sup>16</sup> Graeber, D. (2017)
      <i>Bureaucratie
</i>[« The Utopia of Rules: On Technology, Stupidity, and the Secret Joys of Bureaucracy »], Les liens qui libèrent, 304  (ISBN 9791020902917), Actes Sud.
    </li>
<li class="bibitem" id="ref17" ><sup>17</sup> Montévil, M., &amp; Mossio, M. (2015). Biological organisation as closure of constraints, <i>op. cit</i>. ; Mossio, M., Montévil, M., &amp; Longo, G. (2016). Theoretical principles for biology: Organization. <i>Progress in Biophysics and Molecular Biology</i>, <i>122</i>, 24 – 35.
    </li>
<li class="bibitem" id="ref18" ><sup>18</sup> Gilles Deleuze,
      <i>Logique du sens</i>, 10/18.
    </li>
<li class="bibitem" id="ref19" ><sup>19</sup> Georges Canguilhem,
      <i>Le normal et le pathologique</i>, PUF.
    </li>
<li class="bibitem" id="ref20" ><sup>20</sup> Stewart, S., &amp; Rauch, C. (2016). Rethinking therapeutic strategies in cancer: Wars, fields, anomalies and monsters. <i>Social Theory &amp; Health</i>, <i>14</i>
      (4), 475-492.
    </li>
<li class="bibitem" id="ref21" ><sup>21</sup> Alfred Lotka, The law of evolution as a maximal principle, Human Biology, vol. 17, n°3, 1945.</li>
<li class="bibitem" id="ref22" ><sup>22</sup> Longo, G., Montévil, M., &amp; Kauffman, S. (2012). No entailing laws, but enablement in the evolution of the biosphere. In <i>Genetic and Evolutionary Computation Conference</i>. GECCO’12 New York, NY, USA: ACM.
    </li>
  </ol>
  <div class="footnotes"> 
  <hr />
 
    <div id="sdfootnote2">
      <p class="indent"><a class="sdfootnotesym" href="#sdfootnote2anc" id="sdfootnote2sym">2</a>Professeur Université de Technologie de Compiègne et directeur de l’IRI.</p>
    </div>
    <div id="sdfootnote3">
      <p class="indent"><a class="sdfootnotesym" href="#sdfootnote3anc" id="sdfootnote3sym">3</a>Institut de Recherche et d’Innovation (IRI), centre Pompidou.</p>
    </div>
  </div>
