---
excerpt: "On pourrait donner un sens historique à la notion de simplexité dans (Berthoz, 2009): le simple résulte d’une histoire complexe et n’est jamais élémentaire."
lang: fr
---
<style type="text/css">

.T11 {
font-style: italic;
font-weight: bold;
}

.T10 ,.T14 ,
.T17 ,
.T18 ,
.T2 {
font-weight: bold;
}


.T3 , .T4 {
font-weight: bold;
}
</style>
<h2 class="titleHead" id="lincompressible-complexité-du-réel-et-la-construction-évolutive-du-simple">L’incompressible complexité du réel et la construction évolutive du simple</h2>
<div class="indent">
<p class="P3" style="text-align:center">Giuseppe Longo<br />Centre Cavaillès, CIRPHLESS UMR 3308, CNRS et École Normale Supérieure, Paris, France</p>
<p class="center">Maël Montévil<br />Soto and Sonnenschein Lab, Anatomy and cell biology, Tufts University, Boston, MA, USA</p>
<p class="center">Arnaud Pocheville<br />École doctorale 474 FDV, Paris 5 Descartes, Paris, France 5<br />Laboratoire Ecologie and Evolution, École normale supérieure, CNRS, UMR 7625, Paris</p>
</div>

<h3 class="sectionHead" id="le-sens-de-ce-texte"><span class="T4">Le sens de ce texte</span></h3>
<p class="indent">
  <span class="T1">En parcourant un fil conducteur de l’évolution darwinienne, on trouve </span><span class="T8">çà et là</span>
  la formation du simple, comme résultat de la complexité des trajectoires évolutives : par exemple, la variété, la richesse, la … complexité des bauplan de la faune de Burgess et Ediacara (Gould, 1989) s’est transformée en la
  « simplicité » des bauplan qui suivront et de l’activité qu’ils rendent possible. Tout en prolongeant l’évolution des espèces, l’évolution de l’homme, jusqu’à son histoire, paraît aussi fournir,
  <span class="T8">çà et là</span><span class="T1">, des éléments de cette simplification qui </span><span class="T8">choisit</span>
  <span class="T1">, transforme, organise l’action dans le monde, dont nous parlerons. On pourrait alors donner un sens historique à la notion de </span><span class="cmti-10">simplexité </span><span class="T1">dans (Berthoz, 2009):</span>
</p>
<p class="indent"><span class="T1">- c’est le simple qui résulte d’une histoire complexe,</span></p>
<p class="indent"><span class="T1">- du simple qui n’est jamais élémentaire (atomique, irréductible).</span></p>
<p class="indent"><span class="T1">En physique, cette histoire peut être remplacée par une dynamique de modèles mathématiques qui aide à passer d'un système d'interactions locales, très complexe, à une situation globale, relativement plus simple, limite de la
dynamique considérée. Ces dynamiques permettent de traiter les transitions critiques. Dans ce cas aussi, mais de façon fortement mathématisée, l'intégration globale de réseaux localement intelligibles, mais trop riches pour être
saisis comme un tout, peut proposer une autre exemple de simplexité, plus technique, un exemple qui trouve son sens à la limite asymptotique. Les méthodes de renormalisation, auxquels nous ferons informellement référence, en donnent
un aperçu de grande puissance physico-mathématique. Nous considérons le passage de l'analyse mathématique de la criticité physique à l'analyse du biologique, en tant que situation critique étendue, une transition conceptuelle
possible de la théorisation physique à celle de l'état vivant de la matière.</span></p>
<h3 class="sectionHead" id="réduction-et-unification-en-physique"><span class="T3">Réduction et unification en physique</span></h3>
<p class="indent">
  <span class="T1">Un fantasme hante les esprits de nombreux chercheurs en biologie et en physique : que la complexité du vivant soit intelligible seulement par sa réduction à un prétendu « élémentaire et simple » de la physique. Or, la
réduction n’existe pas, à
</span><span class="T8">l’intérieur</span>
  <span class="T1">de la physique elle-même. Quelques lois de la chimie ont été « réduites » à la Mécanique Quantique, mais à part ce cas historique, on ne trouve, en physique, que de la recherche de fondements autonomes de différents
niveaux phénoménaux, ainsi que des unifications de grande originalité.</span>
</p>
<p class="indent">
  <span class="T1">Newton ne réduit pas les trajectoires des planètes aux lois Galiléennes de la chute d’une pomme. Il invente un nouveau cadre conceptuel, voire des nouvelles mathématiques, pour </span><span class="cmti-10">unifier</span>
  <span class="T1">tous ces mouvements gravitationnels. Par conséquent, il unifie, par une nouvelle théorie, la partition de l’univers entre sub-lunaire et supra-lunaire, encore présente chez Galilée.</span>
</p>
<p class="indent"><span class="T1">Gay-Lussac, Boyle et les autres fondateurs de la thermodynamique négligent les atomes de Démocrite dans l’étude des gaz. Ils s’adressent au niveau phénoménal qui les intéresse, en se donnant un espace de phases (paramètres et
observables) original et pertinent : p, V et T (pression, volume et température). Dans cet espace, fort éloigné des trajectoires des particules, ils
</span><span class="T8">analysent</span><span class="T1"> les trajectoires et les cycles thermodynamiques et proposent un nouveau principe, qui deviendra un des plus important de la physique, le deuxième principe de la thermodynamique.</span></p>
<p class="indent"><span class="T1">Boltzmann ne réduira absolument pas cette nouvelle science à la physique existante : il inventera un nouveau cadre limite, qui unifiera le trajectoires de Newton-Laplace avec les nouveaux principes. En ajoutant l’hypothèse
ergodique, voire le chaos moléculaire, il s’éloigne nettement des théories classiques et, surtout, il les unifie « à la limite asymptotique » : l’intégrale thermodynamique, une limite du rapport n/V (nombre des
particules sur le volume), permet de déduire le deuxième principe. Ainsi naît une nouvelle théorie, la physique statistique, qui
</span><span class="T8">saisit</span><span class="T1"> les deux cadres </span><span class="T8">existants</span><span class="T1"> par un nouveau regard, à partir de l’horizon, de la limite mathématique.</span></p>
<p class="indent"><span class="T1">Du début du XXème siècle jusqu’aujourd’hui les physiciens s’acharnent au grand thème de l’unification entre physique relativiste et quantique. Einstein croyait pouvoir comprendre la deuxième par les principes de la première et, en
cherchant à le faire par une preuve de l’incomplétude de la théorie quantique (Einstein, Podolsky et Rosen,1935), il dérive de l’équation de Schrödinger et de l’indétermination de la mesure, un des faits les plus intéressants du
quantique : l’intrication. Faillite remarquable d’une réduction impossible, erreur d’un grand qui, même en se trompant, invente du nouveau. L’erreur dans EPR n’est pas celui d’un réalisme naïf, mais celle d’une tentative de
réduction à une théorie de l’espace-temps topologiquement séparable par la mesure de propriétés des objets quantiques, tout comme il l’est par la mesure des objets classiques (et relativistes). Son hypothèse implicite (tellement il
la considère fondamentale) est que la séparabilité de la topologie (la séparabilité de deux points quelconque de l’espace - topologie au moins de Hausdorff) est un absolu de la mesure ; erreur surprenante de la part d’un
scientifique qui avait bouleversé la conception de l’espace-temps en unifiant gravitation et inertie grâce à la géométrie riemannienne, qui relativise la métrique (relation entre densité énergie-matière, tenseur métrique et courbure
de l’espace).</span></p>
<p class="indent">
  <span class="T1">Aujourd’hui, les meilleures tentatives </span><span class="cmti-10">d’unifier</span>
  <span class="T1">ces deux cadres incompatibles, champs quantiques et relativistes, changent radicalement l’espace-temps (géométrie non-commutative), ou les objets (théorie des cordes). Malheureusement, ces deux pistes sont incompatibles entre elles.
Mais la richesse et la variété des regards, la pertinence des différents niveaux phénoménaux, astrophysique et microphysique, font la force même de la théorisation physique et de son histoire.</span>
</p>
<p class="indent">
  <span class="T8">Dans ces </span><span class="T1">trois cas, s’il n’y avait pas </span><span class="T8">eu </span><span class="T10">deux</span><span class="T11"> </span><span class="cmti-10">cadres</span>
  <span class="T1">conceptuels ou théoriques à unifier, il n’y aurait pas eu ces succès ni ces tentatives remarquables.</span>
</p>
<p class="indent"><span class="T1">Considérons enfin un cas apparemment plus simple. Il suffit de s’intéresser à l’état fluide et incompressible de la matière inerte, dans un continu, pour que l’analyse physico-mathématique se donne l’hydrodynamique, avec ses propres
observables et équations. Peut-on la comprendre en terme de particules, voire la « réduire » à la physique quantique ? Pas du tout, pour le moment : le continu et les symétries de la fluidité et incompressibilité
se sont pas dérivables des propriétés de la physique quantique. Une autre unification à inventer.</span></p>
<p class="indent"><span class="T1">Peut-on qualifier de « simplexes » les solution limites de Newton et Boltzman ? Peut-être … si l’immense difficulté conceptuelle qui mène à des notions mathématiques asymptotiques, le calcul infinitésimal,
</span><span class="T8">l’intégrale</span><span class="T1"> thermodynamique, est à considérer </span><span class="T8">comme</span><span class="T14"> </span><span class="T1">un parcours complexe qui produit une intelligibilité </span><span class="T1">« simple », par un regard synthétique, </span><span class="T8">non</span><span class="T1"> point élémentaire, sur les phénomènes à partir de l’horizon mathématique ainsi construit. Un regard </span><span class="T8">simplexe</span><span class="T1">.</span></p>
<h3 class="sectionHead" id="vers-la-biologie"><span class="T3">Vers la biologie</span></h3>
<p class="indent">
  <span class="T1">Malheureusement, en s’adressant au vivant, certains collègues oublient l’histoire de la physique, voire son actualité qui cherche encore une unité. Ils croient pouvoir utiliser, disions-nous, des lois de la physique (simples, si
possible, voire élémentaires) pour saisir la phénoménalité biologique.
</span><span class="T8">Au bout du compte, </span><span class="T1">disent-ils, nous ne sommes que des gros sac de molécules. Banalité métaphysique, car, même si cela </span><span class="T8">est vrai du</span>
  <span class="T1">point de vue de l’analyse chimique, la difficulté reste entière car, nous, les plantes et les animaux, nous sommes des sac de molécules aux phénoménalités fort drôles. Le problème qui se pose est alors un autre : quelle
</span><span class="cmti-10">théorie</span><span class="T1"> peut alors rendre intelligible cet état singulier, l’</span><span class="cmti-10">état vivant de la matière </span><span class="T1">?</span>
</p>
<p class="indent">
  <span class="T1">Observons que, face à un seul « drôle de phénomène », par exemple l’invariance de la vitesse de la lumière, on a tout bousculé, en physique, tout changé (la relativité). Un problème sur la mesure en microphysique et sur le
spectre de l’énergie de l’atome de l’hydrogène
</span><span class="T8">, qui résulte discret</span>
  <span class="T1">et on change tout, de façon encore plus radicale (la physique quantique). En fait, en physique, il n’a même pas toujours été nécessaire de se trouver face à un phénomène original : n’importe qui aurait pu laisser tomber une
pierre ou deux, bien avant Galilée. Tout est question de regard. Si on ne saisit pas, par un regard « pertinent » le phénomène « intéressant », on ne fait pas de science. Le regard propre aux théories physiques
existantes n’est pas pertinent pour les phénomènes biologiques : au mieux, il les complexifie à l’infini, car il n’arrive pas à saisir la simplexité historique du vivant. Plus bas, on donnera un sens précis à cet usage de
l’infini, dans le passage de la théorisation physique à des tentatives en biologie, par l’analyse de la renormalisation dans les phénomènes critiques.</span>
</p>
<p class="indent">
  <span class="T1">Quant à l’origine de la biologie moderne, il se trouve que la longue vie active de Darwin se soit superposée à la plus brève de Hamilton. Ce dernier a proposé un regard unifiant sur un grand pan de la physique classique par le
« principe de Hamilton ». Une fois que l’on se donne le bon espace des phases, on peut comprendre toute trajectoire classique, dans cet espace, par un
</span><span class="cmti-10">principe de moindre action</span><span class="T1"> (ou géodésique)</span><span class="cmti-10">.</span><span class="T1"> Par son </span><span class="cmti-10">calcul des</span><span class="T1"> </span><span class="cmti-10">variations</span>
  <span class="T1">, Hamilton donne des bases mathématiques solides à ce principe dû à Maupertuis. Techniquement, on le précisera par l’extrêmisation d’une fonctionnelle, le Lagrangien ; encore une fois, un passage à la limite. En terme moderne,
ces principes et méthodes réalisent un principe de conservation, de l’énergie, en particulier : la moindre action (= énergie x temps) est une minimisation de la
</span><span class="cmti-10">variation de l’énergie</span><span class="T1"> dans le temps, pour conserver cette dernière.</span>
</p>
<p class="indent">
  <span class="T1">Observons à cet égard que la physique moderne avait démarré par un principe de conservation : le principe d’inertie de Galilée (la conservation de l’impulsion). Un mouvement, le mouvement inertiel, rectiligne et uniforme, qui
n’existe pas dans le monde, qui est à la limite de tous les mouvements possibles. Encore une fois, une transition
</span><span class="T8">conceptuelle</span><span class="T1"> à la limite, fort difficile, qui </span><span class="T8">rend</span>
  <span class="T1">tous les mouvements intelligibles, à partir de leur horizon. Une fois constituée, on a produit le simplexe.</span>
</p>
<p class="indent"><span class="T1">En effet, des principes de conservation (de l’énergie, typiquement) seront au cœur de toutes les théories physiques. Hamilton en a donné le cadre physico-mathématique – et E. Noether et H. Weyl, un siècle plus tard,
l’intelligibilité mathématiques en termes de symétries dans les équations.</span></p>
<p class="indent">
  <span class="T1">Or, à la même époque que Hamilton, Darwin regarde d’autres phénomènes, par d’autres principes : les organismes et leur phénotypes, dans le temps de l’évolution. Dans cet espace des phases, il ose proposer une principe
opposé : un principe de
</span><span class="cmti-10">non-conservation</span>
  <span class="T1">, voire de variation « maximale compatible ». En fait, on oublie souvent que Darwin pose deux principes, pas un seul, comme fondements de « L’origine des espèces » :</span>
</p>
<ol class="indent">
  <li>
    <p><span class="T1">reproduction avec variation (« descent with modification »)</span><span class="odfLiEnd"> </span></p>
  </li>
  <li>
    <p><span class="T1">sélection.</span><span class="odfLiEnd"> </span></p>
  </li>
</ol>
<p class="indent">
  <span class="T1">Le deuxième, </span><span class="T8">le</span>
  <span class="T1">plus cité, n’aurait pas de sens sans le premier, qui, en plus, est tout aussi révolutionnaire : le changement à toujours lieu, à chaque passage reproductif, voire </span><span class="T8">à</span>
  <span class="T1">chaque mitose ; il n’est pas seulement la </span><span class="T1">conséquence de l’influence de l’environnement. Il permet à la variabilité de s’étaler dans l’écosystème au « maximum » du possible, un possible difficile à définir car co-constitué par cette variabilité même et limité
seulement par la sélection, en tant qu’exclusion de l’incompatible.</span>
</p>
<p class="indent"><span class="T1">Le regard moderne sur le vivant démarre donc par un principe de non-conservation : le vivant se reproduit toujours avec variation, voire les phénotypes changent continuellement. Bien évidemment, il y a une organisation qui se
transmet, qui est héritée, mais à chaque fois le nouvel ordre est le résultat d’un héritage de l’ordre avec un peu de désordre. L’aléatoire, quantique/classique, voire proprement biologique, joue un rôle fondamental à chaque mitose,
(Buiatti et Longo, 2013).</span></p>
<p class="indent">
  <span class="T1">Aucun principe géodésique ne régit une trajectoire phylogénétique. La phylogenèse n’optimise pas, elle conserve une « structure de base » tant que des nouvelles compatibilités se constituent : les organismes survivent
</span><span class="T8">en changeant, et ce dès que et tant que ces changements sont</span>
  <span class="T1">compatibles avec leur structure de cohérence interne et leurs interactions avec l’écosystème (les variations corrélées de Darwin). Il est aussi difficile de cerner quelle structure est préservée, exactement. Certainement
</span><span class="T8">l’ADN</span><span class="T1"> est un « invariant fondamental » (Monod), toutefois il change, dans sa composition et structure, au cours de l’évolution, voire dans le passage entre </span><span class="T8">générations</span><span class="T1">, même si d’une moindre façon. Les « bauplan » des animaux ont aussi une certaine invariance : loin d’être toutefois exacte, elle est plutôt globale.</span>
</p>
<p class="indent">
  <span class="T1">Peut-on comprendre l’histoire évolutive par un lagrangien appliqué aux molécules ? Peut-être un jour lointain, mais, comme dans l’histoire de la physique, il s’agira alors d’unifier des univers conceptuels fort différents, par
une nouvelle théorie qui
</span><span class="T8">voit</span>
  <span class="T1">les deux en perspective. Et pour unifier, il faut avoir deux théories, en principe. Entre physique et biologie, il nous manque au moins, à coté d’une excellente théorie de l’évolution, une bonne théorie de l’organisme, voire de
l’ontogenèse intégrée à la phylogenèse. Que l'on se dote d'une telle théorie, et ensuite peut-être pourra-t-on parler d’unification avec quelques unes des nombreuses théories physiques, surtout si elles ne
</span><span class="T8">sont</span><span class="T14"> </span><span class="T1">plus incompatibles entre elles …. Sinon, on aura du physique infiniment compliqué, non pas un cadre unifiant dans la simplexité.</span>
</p>
<h3 class="sectionHead" id="principes-géodésiques-en-biologie-"><span class="T4">Principes géodésiques en biologie ?</span></h3>
<p class="indent">
  <span class="T1">Revenons à ce lagrangien, cœur mathématique du principe de moindre action. Il est légitime de dire que le principe de moindre action est un principe qui force la « solution la plus simple », dans un contexte donné :
le parcours optique - une droite, les géodésiques des astres dans les variétés riemanniennes de la relativité, la trajectoire, fort simple, d’une loi de probabilité en mécanique quantique (donnée par l’équation de Schrödinger, en
tant que dérivable de l’Hamiltonien). Les mathématiques, qui nous ont donné cette intelligibilité remarquable et unifiée des théories de l’inerte, résultent d’un parcours historique complexe : une histoire très riche de la
pensée humaine, avec de nombreux
</span><span class="T8">allers</span>
  <span class="T1">/retours, des erreurs, des explorations, des succès. Mais la trajectoire physique, une fois comprise, est simple, elle n’est pas simplexe : le mouvement des objets physiques ne résulte pas de cette histoire, qui ne fait
qu’organiser la nature autour de nous, corréler, donner du sens à nos frictions sur le monde, découper de l’arrière plan les trajectoires et les observables invariants qui nous intéressent.</span>
</p>
<p class="indent"><span class="T1">Par contre, l’action (dans le sens de l’activité) et une structure du vivant peuvent être simplexes. Elles peuvent dépendre d’un parcours évolutif complexe et être relativement simples. Elles ne sont jamais élémentaires. Expliquons
nous, en se référant d’abord et encore une fois à la physique.</span></p>
<p class="indent">
  <span class="T1">La physique classique et relativiste sont des « systèmes déterminés par leurs états » : les conditions initiales et au contour, avec les équations pertinentes, si disponibles, déterminent toute dynamique (ce qui ne
veut pas dire, bien évidemment, que la dynamique en question soit prédictible). Rappelons que la physique quantique ne peut déterminer que la dynamique d’une densité de probabilité, ce qui n'est pas incompatible avec notre propos.
Dans tous les cas, ces dynamiques ne dépendent pas, en principe, d’une histoire, mais seulement de « l’état ». Quelques exceptions sont possibles, et des aspects « historiques » peuvent compter :
</span><span class="T1">en thermodynamique, dans certaines cascades de singularités, dans </span><span class="T8">les</span>
  <span class="T1">transitions critiques … justement des thèmes de transitions, pour nous, vers la théorisation biologique, on en parlera.</span>
</p>
<p class="indent">En général donc, il y a peu, voire point d’histoire dans un système physique<a href="#ftn1" id="body_ftn1"><sup>1</sup></a>. En biologie on pourrait dire le contraire : il n’y a que l’histoire phylo- et ontogénétique qui compte.</p>
<p class="indent">
  <span class="T1">Alors, quel sens attribuer à ces analyses, fort intéressantes, de certaines structures du vivant en termes d’optimalité physique ? </span><span class="T8">Il en existe</span>
  <span class="T1">en phyllotaxie (Jean, 1994), dans l’étude de la genèse des organes animaux (Fleury et Gordon, 2012). Parmi ces analyses, mathématiquement remarquables, les plus efficaces regardent toutes des organes d’échange/production
d’énergie/matière : les poumons, le système vasculaire, certains organes des plantes. Dans ces organes, la physique « prime » : elle domine leur formation et façonne l’organe. Les surfaces et les volumes d’échange
sont optimisés - ou
</span><span class="cmti-10">presque</span>
  <span class="T1">. Ou presque … car si on regarde de près, ces organes sont très irréguliers : la fractalité ou les branchements d’un poumon ou d’un système vasculaire n’ont que très peu de la régularité d’une géodésique. Du point de vue de
leur formation, la différence se joue dans le fait, fondamental, que les cellules qui forment ces systèmes ne sont pas des « boulettes inertes », comme les traitent même les meilleures de ces analyses. Elles sont des
cellules qui se
</span><span class="cmti-10">reproduisent avec variation</span><span class="T1"> (premier principe de Darwin) et </span><span class="cmti-10">mobiles </span><span class="T1">(autre principe </span><span class="T8">fondamental</span>
  <span class="T1">, en biologie, des organismes aux cellules dans un organisme, (Sonnenschein et Soto, 1999)).</span>
</p>
<p class="indent"><span class="T1">Du point de vue de leur fonctionnalité, l’efficacité est aussi due à l’irrégularité : des différences locales contribuent à la plasticité adaptative de l’organisme, voire de la population. Prenez un champ de tournesols. Des
très élégantes analyses en phyllotaxie dérivent les structures spiralisantes selon la série de Fibonacci par des critères d’optimalité (interface plante/lumière). Or, regardez de près un tournesol, voire un champ. Chaque
individu présente des fortes irrégularités dans ces spirales ; les individus sont différents entre eux. Est-ce un défaut ? du « bruit » ? mais pas du tout : la variabilité des surfaces d’échange
rend adaptatif chaque tournesol face à des conditions changeantes de lumière ; la diversité entre les individus rend la population plus stable, dans un écosystème toujours changeant – lors d’un changement, une partie de la
population aura des chances de survivre. On pourrait dire de même de nos poumons, individuels et dans une population. Toutefois … une histoire différente produit chez la grenouille un poumon sphérique, loin de l’optimalité
fractale des poumons des mammifères. Et il marche très bien, dans son contexte, dans les limites de son histoire évolutive. Les analyses dynamiques, en termes d’optimalité physique, ne sont point inutiles : elles sont
incomplètes. Expliquons-nous.</span></p>
<p class="indent">
  <span class="T1">La reproduction avec variation, un principe de non-conservation au niveau des phénotypes, est donc au cœur de l’évolution des espèces, si on y ajoute la sélection. Tout comme il est au cœur de la variabilité, donc de la
diversité, donc de la stabilité d’un écosystème, d’une population, d’un individu, aussi par la formation et la structure de ses organes. Une meilleure intégration avec les analyses physiques pourrait se faire si les équations
des dynamiques en question ne sont plus vues comme des déterminations formelles de la causalité, mais des
</span><span class="cmti-10">contraintes</span><span class="T1">. La </span><span class="T8">poussée</span><span class="T1"> du cœur embryonnaire n’est pas la </span><span class="cmti-10">cause</span>
  <span class="T1">de la formation du système vasculaire, mais une (très importante) contrainte dynamique qui façonne l’activité propre des cellules, qui se reproduisent avec variation et bougent. La motilité des organismes et des populations,
même cellulaire dans un organisme, disions-nous, est un autre principe fondamental
</span><a href="#ftn2" id="body_ftn2"><sup>2</sup></a><span class="T1">, soumis aux contraintes physiques (et biologiques, bien évidemment : les variations corrélées de Darwin).</span>
</p>
<p class="indent">
  <span class="T1">Pour le dire en toute généralité, en référence à une force physique omniprésente en biologie, la gravitation est une </span><span class="cmti-10">cause</span>
  <span class="T1">en physique classique, mais elle n’est qu’une </span><span class="cmti-10">contrainte</span><span class="T1"> en phylogénèse et ontogénèse</span>
  <a href="#ftn3" id="body_ftn3"><sup>3</sup></a><span class="T1">.</span>
</p>
<p class="indent">
  <span class="T1">Un jour lointain on unifiera les principes biologiques avec ceux de la physique quantique-relativiste-classique-hydrodynamique (il y a beaucoup d’eau dans un organisme), elle-même enfin </span><span class="T8">unifiée </span>
  <span class="T1">; pour le moment, on les garde comme tels, dans le but de travailler à une théorie de l’onto-phylogénèse. L’unification (asymptotique ?) proposera un regard différent sur toutes ces théories, à partir d’un nouvel horizon,
même pour la physique. Et quand on aura une grande théorie
</span><span class="T8">unifiée</span>
  <span class="T1">, peut-être pourra-t-on voir la physique comme un cas particulier du grand « cadre bio-physique »: la physique ne sera que du biologique quand l’activité du vivant est </span><span class="cmti-10">nulle</span>
  <span class="T1">, c’est à dire tout le vivant est mort …. Notons que si cette perspective peut sembler étonnante, elle paraîtra peut-être plus naturelle en considérant des domaines telle que la climatologie ou la géologie, domaines ou la
biologie et la physique interviennent (voire aussi les sciences humaines), mais qui font sens aussi sans activité biologique (tant en théorie qu'en pratique sur d'autre planètes par exemple). Du reste, la géométrie Euclidienne,
considérée pendant 25 siècles comme le fondement ultime des mathématiques, n’est devenu qu’un cas particulier de la géométrie Riemannienne : une variété à courbure
</span><span class="cmti-10">nulle</span><span class="T1"> …. </span>
</p>
<h3 class="sectionHead" id="lactivité-le-vivant-lhistoire">L’activité, le vivant, l’histoire</h3>
<p class="indent"><span class="T1">Les deux principes « dynamiques » du vivant, la reproduction avec variation et la motilité, sont des « formes de l’action ». La matière vivante est toujours active, elle n’est justement pas inerte. Elle n’a
pas besoin d’une force pour changer d’état, comme la pierre de Galilée, l’astre de Newton et Einstein (bon, pour ce dernier, il faut un changement de la courbure de l’espace temps), le quanton en microphysique. Elle est toujours
loin de l’équilibre et utilise un flot d’énergie matière.</span></p>
<p class="indent"><span class="T1">Quand la physique domine la dynamique d’une forme biologique, par cette superposition d’histoire, de contraintes physiques et d’activité reproductive changeante, on obtient alors des formes qui ne sont pas des
« simples » géodésiques physiques, mais des structures que l’on pourrait appeler simplexes, disions-nous, somme de simplicité géodésique, optimisante, et de désordre
</span><span class="cmti-10">contingent</span><span class="T1">, </span><span class="cmti-10">historique</span><span class="T1">, </span><span class="cmti-10">complexifiant</span><span class="T1">.</span></p>
<p class="indent"><span class="T1">Est-ce que la main des primates est aussi simplexe ? Elle est un organe très complexe, où l’optimisation physique ne façonne pas la forme globale, au plus des sous-structures locales. Par contre, certains gestes de
</span><span class="T8">la main peuvent être considérés simplexes. Voyons des exemples.</span></p>
<p class="indent">
  <span class="T1">La main est, tout d’abord et essentiellement, le résultat un parcours évolutif possible des podia antérieurs d’un tétrapode, comme la </span><span class="T8">patte</span>
  <span class="T1">d’un kangourou ou l’aile d’une chauve-souris. Aucun de ces organes n’est optimal, en aucun sens. La contingence évolutive et historique domine largement sur la détermination et les contraintes physiques, toujours présentes, bien
naturellement, mais comme contraintes
</span><span class="T8">non-façonnantes</span><span class="T1"> : les fonctions de la main ne sont même pas partiellement analysables par l’échange/production d’un quelconque observable physique.</span>
</p>
<p class="indent">
  <span class="T1">Toutefois, l’action du primate, voire de l’homme, peut utiliser cet organe, et le bras qui la soutient, de façon simple, avec ses modalités d’optimalité physique. Le geste qui trace une courbe est simplexe car, dans notre
approche, il est le résultat « simple » d’une histoire développementale et évolutive complexe. En fait, il minimise les secousses (la
</span><span class="T8">dérivée</span>
  <span class="T1">d’une accélération) et il n’est pas élémentaire, car on peut le décomposer par une analyse mathématique fine, (Bennequin et al., 2009). Grâce à une histoire complexe, on peut donc avoir du simplexe dans l’activité d’un
organisme, lui même très complexe, à l’interface d’un écosystème complexe.</span>
</p>
<p class="indent"><span class="T1">L’humain, par le langage, permet bien d’autres gestes très simples, possibles seulement par la richesse de sa socialité. Apparemment, les singes ne pointent pas du doigt. Ils ont un regard très mobile : quand il se fixe,
les congénères regardent dans la même direction. Les primates ont une anatomie qui rendrait possible aussi le geste du bras, de la main, du doigt qui pointe ; avons nous l’habitude d'avoir des mains plus libres ? Est-ce le
langage qui nous permet de construire ensemble ce geste et sa signification ? Qu’elle est complexe la simplicité de ce geste simplexe !</span></p>
<p class="indent"><span class="T1">Dans les peintures du paléolithique, à Lascaux, par exemple, on voit des animaux rendus par des taches de couleur, très réalistes : ils se détachent du fond en occupant toute une surface. D’autres, et ça deviendra une
constante du dessin humain, ne sont que des contours, des lignes, tout aussi efficaces pour évoquer l’animal à des hommes, au moins depuis l’expérience construite de ces dessins. Voilà un geste d’une immense simplicité, le tracé
d’un contour d’un animal, d’un visage. Pensez à la simplicité du profil de Chirac ou Sarkozy chez Plantu, Berthoz en parle ailleurs : un trait essentiel qui synthétise en fait un mouvement, du nez, du menton ; et tous
ceux qui partagent le même monde reconnaissent le personnage.</span></p>
<p class="indent">
  <span class="T1">Or, le contours ne sont pas là, il n’y a pas une entité tiers entre l’objet et le fond, une ligne épaisse dans l’espace physique : il n’y a qu’une singularité, une transition entre la surface de l’objet et le fond. La
neurophysiologie et la neurophénoménologie nous disent que le cortex visuel primaire
</span><span class="cmti-10">force</span>
  <span class="T1">ce contour par un jeu complexe de recollement entre neurones sensibles aux directions (voir Petitot, 2008). Toutefois, nous partageons ce même cortex visuel avec les autres primates, voire avec maints mammifères. Seulement
l’homme, apparemment, arrive à communiquer à l’autre homme, dans le rite social, le sens d’un contour à peine tracé sur une surface. Quel rôle alors faut-il attribuer au langage, à la socialité dans la communication riche de
sens par un trait si simple ? Voilà du simple qui résulte d’une histoire évolutive et humaine très complexe.</span>
</p>
<p class="indent">
  <span class="T1">La géométrie grecque ira encore plus loin : la ligne est sans épaisseur, dit Euclide (définition beta). Les </span><span class="T8">grecs</span>
  <span class="T1">arrivent à nous proposer la notion de bord, notion mathématiquement difficile, car ils inventent une théorie générale des surfaces (une belle observation de Bernard Teissier), et à en faire l’outil pour tracer toute figure
géométrique : les figures d’Euclide ne sont faites que de bords. Dans ce but, dans un dialogue avec les dieux, leur géométrie, ils posent ce bord in abstracto, en lui donnant un sens autonome : non plus un bord, mais
une ligne en soi, sans épaisseur, leur invention fondamentale (le point est construit, chez Euclide : il est aux extrêmes d’un segment, définition gamma, à l’intersection de lignes sans épaisseur, théorème 1, chapitre 1).
Tout le monde comprend (ou presque ?) ce geste si simple, mais quelle audace dans cette transition conceptuelle : où sont-elles ces lignes sans épaisseur, ce concept asymptotique à toute ligne réellement tracée ?
Par cette construction à l’apparence si simple, mais qui résulte d’un parcours si complexe, commence la danse des mathématiques, une science qui se posera toujours à la limite du monde, si simple et si complexe au même temps.</span>
</p>
<p class="indent"><span class="T1">La construction commune du sens est au cœur de tous ces gestes et ces démarches propre à l’homme, une construction ancrée dans l’intentionnalité à partager avec autrui : l’analyse de la simplexité participe de la
reconstruction de ce sens (Berthoz, 2009, p. 35).</span></p>
<h3 class="sectionHead" id="des-passerelles-possibles-entre-physiques-et-biologie"><span class="T17">Des passerelles possibles entre physiques et biologie</span></h3>
<p class="indent"><span class="T1">Nous allons maintenant nous pencher sur la notion de niveau d'organisation et son lien avec le complexe et le simple. Nous entendons ici la notion de niveau d'organisation en un sens fort et général : un changement de
niveau ne se produit que s'il est “théoriquement nécessaire”. Or une telle nécessité ne peut arriver que si la détermination équationnelle du premier niveau, lorsqu'elle existe, en physique mathématique typiquement, perd sa
capacité à déterminer la trajectoire du système. Cette détermination passe par un rapport entre quantité qui doit rester fini, et c'est donc précisément lorsque certaines quantités deviennent infinies, formant une singularité,
qu'un changement de niveau est possible (Bailly, 1991, Longo, Montévil et Pocheville, 2012).</span></p>
<p class="indent">
  <span class="T8">Nombre d'</span><span class="T1">exemples de transitions critiques sont étudiés en physique. La transition paramagnétisme/ferromagnétisme a été </span><span class="T8">analysée</span>
  <span class="T1">par Pierre Curie, en premier. Dans ce cas, la divergence de la dérivée d'un observable, l'orientation magnétique, lors d'une baisse de température, est la signature mathématique d'un changement global : la formation d'un
aimant (spins orientés) à partir d'une situation désordonnée des spins. On peut analyser de façon similaire la formation soudaine (elle a lieu en un point, mathématiquement parlant) d'un
</span><span class="T8">cristal</span><span class="T1">, d'un flocon de neige, la percolation (un passage soudain d'un </span><span class="T1">liquide, par exemple, à travers un milieu poreux). On change alors d'observable, voire de niveau d'organisation : on ''voit'', on mesure, autre chose ; le phénomène à examiner change.</span>
</p>
<p class="indent">
  <span class="T1">La situation paradigmatique faisant intervenir de </span><span class="T8">tels changements</span>
  <span class="T1">de niveau fait intervenir un jeu technique entre complexe et élémentaire (et simple), au cœur d’une méthode, mathématiquement difficile, née en théorie quantique des champs et étendue ensuite à l’analyse des transitions
critiques classiques : la méthode de renormalisation. Cette méthode permet de passer d’une situation où le nombre des composantes élémentaires d’un phénomène, voire des observables à considérer, peut-être immense (les spins
désordonnés, les gouttelettes d'eau...), et ingérable car ils forment des structures à toutes les échelles, à une situation où un « regard global » intègre ces éléments et leurs interactions dans un « tout »
plus maniable mathématiquement (l'aimant, le
</span><span class="T8">cristal</span><span class="T1">).</span>
</p>
<p class="indent">
  <span class="T1">Dans un sens, le très complexe, comme </span><span class="T8">somme</span>
  <span class="T1">d’un nombre énorme de parties qui interagissent, devient simple si on suit un changement de regard mathématique, un parcours de reconstruction des objets pertinents de l’analyse. Techniquement, cela peut être
</span><span class="T8">vu</span>
  <span class="T1">comme une « dynamique de modèles mathématiques », un ensemble ordonné de systèmes d’équations paramétrées où la dynamique n'est pas temporelle mais en fonction des échelles. C'est précisément ce changement de regard
que l'on peut analyser comme un changement de niveau d'organisation, dont l'objectivité provient des singularités rencontrées dans le niveau initial (Bailly, 1991, Longo, Montévil et Pocheville, 2012). Ce changement de niveau
permet d'appréhender le complexe comme du simple, relativement à ce changement de niveau. Expliquons-nous.</span>
</p>
<p class="indent">
  <span class="T1">La méthode de renormalisation permet de gérer mathématiquement le global, sans faire référence à un élémentaire simple et ultime. En fait, on remarquera, elle permet d’analyser des </span><span class="T8">situations</span>
  <span class="T1">« sans fond » en physique de la matière (i.e. sans échelle minimale). Elle peut avoir une saveur bottom-up, mais cette saveur est perdue dans le traitement mathématique de l’ensemble des modèles, dont aucun ne possède
un statut privilégié, sauf à la limite, pour les grandes échelles, quand un point fixe est atteint. Mais, au point fixe, on sera dans la situation globale, la seule relativement stable car ils concerne toutes les grandes
échelles. Nous allons discuter cette méthode avec plus de
</span><span class="T8">détails</span><span class="T1"> dans la section suivante, après un passage par le biologique.</span>
</p>
<p class="indent">
  <span class="T1">Ce jeu entre local et global a été considéré informatif pour analyser les organismes vivants, où « presque tout est corrélé à presque tout » et la structure globale rétro-agit sur la structure locale (voir Kauffman,
1993 et Bak, Tang et Wiesenfeld, 1988). Suivant une piste ouverte par Francis Bailly, nous pensons que, dans les analyses du vivant, on doit aller plus loin. Les transitions
</span><span class="T8">critiques</span>
  <span class="T1">étudiées en physique sont ponctuelles, c’est à dire la limite de la classe de modèles mentionnée est un point mathématique, où l’on trouve un point fixe, essentiel à la convergence de la méthode de renormalisation. C’est là que
les changements de symétries
</span><span class="T8">ont</span>
  <span class="T1">lieu. Or, un organisme est plutôt dans une transition critique permanente, il est toujours en train de se reconstruire en changeant, en particulier en changeant ses symétries : chaque mitose cellulaire est une transition
critique, une reconstruction de la structure de cohérence locale, interne et tissulaire. Cela justifie l’idée d’analyser l’état vivant de la matière comme « transition critique étendue », voire comme étant dans un
intervalle non-
</span><span class="T8">nul</span><span class="T1"> de criticité, contrairement à la ponctualité mathématique des transitions critiques en physique (Bailly, Longo, 2011 ; Longo, Montévil, 2011).</span>
</p>
<h3 class="sectionHead" id="bottom-up-top-down-et-renormalisation"><span class="T4">Bottom-up, top-down et renormalisation</span><span class="T18"></span></h3>
<p class="indent"><span class="T19">Pour mieux comprendre la question des changements de niveaux et le caractère singulier, dans le contexte de la physique, de la méthode de renormalisation nous allons maintenant discuter avec quelques détails le sens qualitatif
de cette méthode. L'enjeu ici est avant tout comment se fait la compréhension d'un système comme compositions d'interactions. La complexité est souvent vue, en effet, comme due à l'existence de nombreuses interactions ; un
regard simplifiant doit saisir de façon unifiée ces interactions, comme système et à la bonne échelle.</span></p>
<p class="indent"><span class="T19">La façon paradigmatique de comprendre un phénomène macroscopique dans les termes d'un modèle microscopique (ici microscopique et macroscopique sont à entendre dans un sens relatif) est d'utiliser une moyenne statistique des
propriétés microscopiques - à condition toutefois que le nombre d'entités microscopiques soit suffisant. La reconstruction de la thermodynamique à partir de la mécanique statistique est sans doute le meilleur exemple d'un tel
procédé
</span><span class="T22">.</span></p>
<p class="indent">
  <span class="T19">Il y a cependant certaines situations en physique où des approches par moyennage statistique échouent. C'est le cas en particulier lors de certaines transitions de phase (de second ordre, voir Toulouse et al 1977 ;
Zinn-Justin, 2007). Dans de telles transitions, les longueurs de corrélations tendent vers l'infini à l'approche du point critique.
</span><span class="cmti-10">A</span><span class="cmti-10">u point</span>
  <span class="T19">critique, le comportement du système montre des fluctuations à toutes les échelles et celle-ci dominent la détermination du système. Or, dans une approche par moyennage statistique, la convergence vers la moyenne est basée sur
une certaine indépendance des degrés de liberté des éléments microscopiques (théorème de la limite centrale). Quand les fluctuations dominent, le système montre une structure mésoscopique cohérente (non-indépendance mais au
contraire fortement corrélé), ce qui rend l'approche par moyennage macroscopique inopérante (voir le critère de Ginzburg, Als-Nielsen and Birgeneau, 1977). Par contre, on a une transition vers une situation globale qui permet de
gérer de façon relativement simple des effets locaux dont la somme est (infiniment) complexe.</span>
</p>
<p class="indent"><span class="T19">Pour étudier les transitions de phase du second ordre, on utilise des méthodes de renormalisation. On part d'un système décrit par un modèle à une certaine échelle, qui peut être choisie arbitrairement (par exemple celle de la
résolution de l'appareil de mesure). Ce modèle est composé d'un ensemble de paramètres et d'une fonction qui détermine le comportement du système (par exemple, l'hamiltonien). Au lieu de résoudre le modèle à cette échelle comme
c'est l'usage, on regarde de quelle façon les paramètres et la fonction changent quand le système est décrit à une échelle supérieure. La façon dont les modèles changent en fonction des échelles est formalisée par un opérateur
mathématique, l'opérateur de renormalisation.</span></p>
<p class="indent"><span class="T19">Dans les transitions de phase de second ordre habituelles (comme la transition para-ferromagnétique), le modèle est asymptotiquement invariant par renormalisation. Cela signifie que quand on considère des échelles de plus en
plus grandes, les modèles obtenus convergent vers un point fixe. Cette invariance asymptotique montre et définie l'invariance d'échelle (exhibée par ces systèmes). Dans ces cas, les propriétés physiques du système sont
déterminées par le comportement de l'opérateur de renormalisation dans le voisinage du point fixe.</span></p>
<p class="indent">
  <span class="T19">Conceptuellement, utiliser la renormalisation dans les situations critiques signifie la chose suivante. On ne peut pas obtenir mathématiquement ce à quoi conduit la combinaison des constituants élémentaires qui interagissent,
parce que cette combinaison génère des singularités (à la limite thermodynamique) ; en d'autres termes, cette combinaison ne converge pas. En termes de fluctuations, la situation n'est pas résoluble parce que le
comportement local est dominé par des fluctuations se produisant aussi à des échelles plus grandes que l'échelle considérée (quelle qu'elle soit), ou d'un autre point de vue considérer un système plus grand conduit le système à
former des pattern à plus grandes échelle. On peut cependant considérer une partie limitée des interactions au sein du système, délimitée par des bornes arbitraires (en termes d'échelles), c'est-à-dire correspondant au choix
d'une certaine échelle de description. Ce « morceau » d'interactions est ensuite intégré (résolu) et le résultat constitue l'un des nouveaux constituants élémentaires d'un nouveau modèle à une échelle supérieure. Ce
nouveau modèle, cependant, est aussi complexe que le modèle de départ, puisque le système a un nombre infini de degrés de liberté : en général, on réalise l'opération de renormalisation de façon
</span><span class="cmti-10">à conserver la forme équationnelle de la détermination</span>
  <span class="T19">, mais avec des paramètres et des variables différents, dits renormalisés. Plus précisément,  habituellement, certains paramètres se dissipent asymptotiquement lors des itérations de l'opération de renormalisation,  le
point fixe est alors  plus simple que le modèle de départ. Il est beaucoup plus imple quand on applique la renormalisation à des situations non fortement critiques (par exemple, cette simplification de la détermination aux
hautes échelles justifie les comportements lisses obtenus dans la théorie de Landau, voir (Zinn-Justin, 2007)).</span>
</p>
<p class="indent"><span class="T19">A l'inverse, dans le cas des phénomènes critiques, la détermination au point fixe demeure essentiellement aussi complexe que la détermination initiale: on ne s'affranchit pas des fluctuations lors de la renormalisation parce
qu'elles dominent le système à toutes les échelles (certains aspects non-essentiels peuvent disparaître néanmoins).</span></p>
<p class="indent">
  <span class="T19">En quoi la renormalisation représente-t-elle donc un progrès vers la détermination du comportement global du système étudié? Dans quel sens simplifie-t-elle du complexe ? Quand on est au point fixe, par définition, la
détermination équationnelle n'est plus changée au cours des itérations de renormalisation. Or, la renormalisation consiste à prendre plus d'interactions en compte. Par conséquent, au point fixe, on considère une situation
stabilisée par rapport à la contribution d'interactions supplémentaires. Quand on ajoute des interactions par renormalisation, leurs effets sur la détermination sont définis par l'opérateur de renormalisation: l'opérateur de
renormalisation est donc une description de ces interactions. En ce sens, aux larges échelles, toutes les interactions sont des images (copies) des interactions qui sont prises en compte par la renormalisation au point fixe.
Ainsi, on obtient une approche qui
</span><span class="cmti-10">explicite toutes</span><span class="T19"> les interactions </span><span class="cmti-10">pertinentes</span>
  <span class="T19">(aux larges échelles) dans le système, même si cette approche ne donne pas, et ne peut pas, donner une description du système, par un modèle à une seule échelle. Cette approche ne donne donc  pas une </span><span class="cmti-10">combinaison</span><span class="T19"> </span><span class="cmti-10">effective </span><span class="T19">de toutes les interactions pertinentes du système.</span>
</p>
<p class="indent"><span class="T19">Enfin, il est bon de préciser que l'on peut considérer différents modèles comme points de départ (éventuellement, mais pas obligatoirement, à différentes échelles), car si ils ont le même comportement asymptotique par
renormalisation, alors ils mènent aux mêmes propriétés physiques objectives. Ceci définit la notion de classe d'universalité, regroupant les modèles ayant le même comportement par renormalisation, ce qui constitue le
comportement objectif du système. Il y a donc une infinité de modèles dans une classe d'universalité puisque, parmi une pléthore d'autres choix, le modèle renormalisé à n'importe quelle échelle peut être indifféremment considéré
comme un point de départ, (Lesne 2003).</span></p>
<p class="indent"><span class="T19">On peut alors considérer que les méthodes de renormalisation sont des approches « bottom-up », étant donné que l'étude du système dépend du point de départ, qui, normalement, est le modèle à plus petite échelle.
Cependant, ce point de départ est largement contingent et arbitraire, à la fois parce qu'on peut partir de n'importe quelle échelle comme échelle minimale, et parce qu'on peut changer de modèle de départ aussi longtemps que l'on
reste dans la même classe d'universalité. C'est dans les propriétés génériques de la classe d'universalité qu'est fondée l'objectivité de la renormalisation et la compréhension du phénomène.</span></p>
<p class="indent"><span class="T19">La renormalisation a donc aussi une saveur holistique, dans le sens où la situation locale au point critique dépend de la situation globale (la structure de cohérence, due aux longueurs de corrélation infinies). Plus
précisément, le système est « tellement global » qu'on ne peut pas combiner ses interactions entièrement, on peut seulement (mais explicitement) trouver la forme des contributions de toutes les interactions (aux larges
échelles), grâce au point fixe.</span></p>
<p class="indent">
  <span class="T19">L'aspect radicalement nouveau des méthodes de renormalisation est qu'on n'essaie plus, </span><span class="cmti-10">stricto sensu</span>
  <span class="T19">, de se donner et de résoudre un modèle, mais plutôt de comprendre le comportement de la transition d'un modèle à un autre, défini à une échelle supérieure. En cela, le système est étudié au niveau d'un meta-modèle: ce qui
compte,
</span><span class="cmti-10">in fine</span><span class="T19">, ce ne sont pas les relations intra-modèle (entre constituants élémentaires à une échelle donnée), mais le comportement </span><span class="cmti-10">à travers les échelles</span>
  <span class="T19">des relations inter-modèles. Ce meta-modèle (l'opérateur de renormalisation) permet de partir d'un modèle riche d'arbitraire (façonné par le choix d’un niveau d’observation, des approximations et des contraintes pragmatiques) et
d'atteindre, grâce aux propriétés asymptotiques, une connaissance objective de la classe d'universalité et des propriétés physiques du système.</span>
</p>
<p class="indent"><span class="T19">La théorie quantique des champs traite de situations qui nécessitent également l'usage des méthodes de renormalisation (qui ont d'ailleurs été inventées à ces fins). L'essentiel est que, quand on considère des interactions de
plus en plus microscopiques, on se trouve face à des divergences (comparables aux divergences des phénomènes critiques). Cela signifie que le comportement à une échelle ne peut pas être
</span><span class="T19">donné, dans cette théorie, par la contribution d'objets d'une échelle spatiale arbitrairement petite et/ou grande (ce qui provoquerait la rupture de la structure équationnelle, par l'apparition des infinis). Cependant, on peut
traiter une partie des interactions et considérer la stabilité des transformations des formes équationnelles et des « constantes » lorsque l'on prend en compte de plus en plus d'interactions.</span></p>
<p class="indent"><span class="T19">La possibilité pour les théories en physiques quantiques des champs  d'être renormalisales est une condition de leur validité théorique. Nous voudrions noter que le modèle standard traite trois des quatre forces
fondamentales de la physique de cette manière, où il n'y a aucune échelle objectivement fondamentale, « la plus petite possible ». Au contraire, l'introduction d'un comportement « particulier » à une échelle
microscopique spécifique (petite échelle) est contraire à la manière dont la théorie comprend les phénomènes quantique, sur la base des relations entre échelles (Zinn-Justin, 2007). Bien sûr, cela n'exclut pas des changements
futures de paradigmes, en particulier parce que l'introduction de la quatrième force, la gravité, conduit à l'impossibilité de renormaliser (prendre plus d'interactions en compte conduit à une complexification de la forme
équationnelle, par l'introduction de nouvelles variables). En ce sens, la compréhension actuelle des phénomènes microscopiques passe par l'insondable en termes d'échelles les plus petites possibles (voire
</span><span class="cmti-10">bottomless </span><span class="T19">ou sans fond).</span></p>
<p class="indent">
  <span class="T19">En conclusion, la renormalisation permet d'obtenir une appréhension explicite (et mesurable) de </span><span class="cmti-10">toutes</span>
  <span class="T19">les interactions pertinentes dans un système là où la combinaison </span><span class="T20">effective</span><span class="T19"> de </span><span class="cmti-10">toutes</span>
  <span class="T19">ces interactions est mathématiquement impossible. En d'autres termes, du point de vue de la détermination théorique, </span><span class="cmti-10">le tout n'est pas la somme des parties (la somme diverge), mais ce tout peut être appréhendé par des sommes partielles successives</span>
  <span class="T19">(qui deviennent symétriques à toutes les sommes partielles aux larges échelles). De cette façon, le tout, </span><span class="cmti-10">in fine</span><span class="T19">, n'est pas appréhendé </span><span class="cmti-10">comme la</span><span class="T19"> </span><span class="cmti-10">somme</span><span class="T19"> de ses parties, mais </span><span class="cmti-10">par</span><span class="T19"> </span><span class="cmti-10">des sommes</span>
  <span class="T19">de ses parties. Dans cette opération, la modélisation des échelles microscopiques apparaît pour une large part contingente et arbitraire : la renormalisation permet d'en extraire les aspects objectifs,
</span><span class="cmti-10">via </span><span class="T19">les invariants de l'opération. De ce fait, nous considérons les méthodes de renormalisation comme étant à la frontière des stratégies « bottom-up », de type réductionniste, et d’ouvrir vers des visions du
</span><span class="cmti-10">tout pertinent</span>
  <span class="T19">, la situation globale, en tant qu'ensemble des invariants d’un processus. Ces méthodes apparaissent donc comme un moyen d'aller au delà des approches bottom-up standardes, tout en conservant une saveur bottom-up à la théorie
(par l'intégration des interactions). Il s'agît toutefois d'une saveur considérablement enrichie par l’approche globale : le regard sur la cascade de modèles sans niveau privilégiés ou ultime. En particulier, la
renormalisation dans la théorie quantique des champs est associée à une situation « sans fond » (
</span><span class="cmti-10">bottomless</span><span class="T19">), disions-nous.</span>
</p>
<p class="indent"><span class="T19">La ''simplicité'' que la renormalisation permet d'exhiber n'est ici permise que par la symétrie d'échelle, propre aux situations critiques. Cette simplicité n'est cependant pas basée sur une échelle élémentaire ou se
trouveraient des objets élémentaires, mais au contraire sur la complexité des interactions du système à toute les échelles.</span></p>
<p class="indent"><span class="T19">A partir du cas des transitions critiques physiques, il y a donc deux formes de simplexité que l'on peut dégager.</span></p>
<p class="indent"><span class="T19"> La première est celle associée au caractère global de la détermination d'un ''tout'', comme limite d'une cascade de modèles, en physique mathématique. Encore une fois, on atteint du simple à partir d'une dynamique de
modèles complexe (infinie, du point de vue mathématique) : la suite des systèmes d'interactions aux différentes échelles. En biologie, l'organisme est aussi constitué par l'apparition de plusieurs niveaux d'organisation,
qui de plus peuvent se situer à des échelles différentes : la situation critique étendue en est l'intégration et la limite théorique.</span></p>
<p class="indent"><span class="T19">La seconde est associée aux changements de symétrie eux-mêmes, dus au  passage d'un état à l'autre. Dans le cas des transitions critiques étendues, il s'agit des symétries contingentes qui résultent de la transition. En
effet, si en physique les symétries théoriques fondamentales sont posées comme principes, les symétries biologiques résultent, comme nous l'avons discuté, de l'histoire (onto- et phylogénétique) de l'organisme
</span><span class="T19">considéré (Longo, Montévil, 2011). Elles sont données par l'activité biologique et sa complexité sous-jacente, qui produit, grâce à une histoire constitutive, des structures (relativement) simple, donc, simplexes.</span></p>
<p class="indent"><span class="T19">Enfin, en considérant que les transitions critiques étendues peuvent être représentées, dans nos approches, comme des intervalles non-nulles avec des ensembles denses de transitions critiques (de type physiques, donc
ponctuelles), on saisirait le biologique comme une structure simplexe, que la physique, voire la théorie de la criticité qui déjà suppose des passages à la limite infinie, ne peut analyser qu'en termes de complexité infinie.
Bref, le simplex du biologique est infiniment complexe pour l'analyse physique.</span></p>
<h3 class="sectionHead" id="références">Références</h3>
<ol class="thebibliography">
  <li class="bibitem">
    Als-Nielsen, J., and R. J. Birgeneau. 1977. « Mean field theory, the Ginzburg criterion, and marginal dimensionality of phase transitions ». <span class="cmti-10">American Journal of Physics </span>45 (6): 554–560. ISSN: 00029505. doi:
    <a  href="https://dx.doi.org/10.1119/1.11019">10.1119/1.11019</a>.
  </li>
<li class="bibitem">Bailly, F. 1991. « L’anneau des disciplines ». <span class="cmti-10">Revue Internationale de Systémique </span>5 (3).</li>
<li class="bibitem"><span class="T26">Bailly F., Longo G., 2011, </span><span class="T27">Mathematics and the natural sciences; The Physical Singularity of Life. Imperial College Press (version française, Hermann, 2006).</span></li>
<li class="bibitem">
    Bak, P., C. Tang, and K. Wiesenfeld. 1988. « Self-organized criticality ». <span class="cmti-10">Physical review A </span>38 (1): 364–374. doi:
    <a class="ListLabel_20_1" href="https://dx.doi.org/10.1103/PhysRevA.38.364">10.1103/PhysRevA.38.364
</a>.
  </li>
<li class="bibitem">
    <span class="T31">Bennequin D., Fuchs R., Berthoz A., Flash T.</span><span class="T28"> « </span>
    <span class="T31">Movement Timing and Invariance Arise from Several Geometries » PLoS Computational Biology, Volume 5, Issue 7, July 2009.</span>
  </li>
<li class="bibitem">Berthoz, A. 2009. <span class="cmti-10">La simplexité. </span>Odile Jacob</li>
<li class="bibitem">Buiatti, M., and G. Longo. 2013. « Randomness and Multi-level Interactions in Biology ». TIBI-D-12-00030R1 to appear, <span class="cmti-10">Theory of Biosciences.</span></li>
<li class="bibitem">
    Einstein, A., B. Podolsky, and N. Rosen. 1935. « Can quantum-mechanical description of physical reality be considered complete ? » <span class="cmti-10">Physical Review </span>47 (10): 777–780. doi:
    <a class="ListLabel_20_1" href="https://dx.doi.org/10.1103/PhysRev.47.777">10.1103/ PhysRev.47.777
</a>.
  </li>
<li class="bibitem">
    Fleury, V., and R. Gordon. 2012. « Coupling of Growth, Differentiation and Morphogenesis : An Integrated Approach to Design in Embryogenesis »  In <span class="cmti-10">Origin(s) of Design</span>
    <span class="cmti-10">in Nature, </span>edited by Liz Swan, Richard Gordon, and Joseph Seckbach, 385–428. Volume 23. Cellular Origin, Life in Extreme Habitats and Astrobiology. Springer Netherlands. ISBN
    : 978-94-007-4155-3. doi:
    <a class="ListLabel_20_1" href="https://dx.doi.org/10.1007/978-94-007-4156-0_22">10.1007/978-94-007-4156-0_22
</a>.
  </li>
<li class="bibitem">Gould, S.J. 1989. <span class="cmti-10">Wonderful life. </span>Norton.</li>
<li class="bibitem">Jean, R.v. 1994. <span class="cmti-10">Phyllotaxis</span> <span class="cmti-10">: A Systemic Study in Plant Morphogenesis. </span>Cambridge Studies in Mathematics.</li>
<li class="bibitem">Kauffman, S.A. 1993. <span class="cmti-10">The origins of order. </span>Oxford U. P.</li>
<li class="bibitem">Lesne, A. 2003. « Approches multi-échelles en physique et en biologie ». Thèse d’habilitation à diriger des recherches. Université Pierre et Marie Curie.</li>
<li class="bibitem"><span class="T26">Longo G., Montévil M., 2011, From physics to biology by extending criticality and symmetry</span> b<span class="T26">reakings. Progress in Biophysics and Molecular Biology, </span>106(2):340 – 347.</li>
<li class="bibitem">
    Longo, G., and M. Montévil. 2012. « The Inert vs. the Living State of Matter : Extended Criticality, Time Geometry, Anti-Entropy — an overview ». <span class="cmti-10">Frontiers in Physiology </span>3 (0039). ISSN: 1664-042X. doi:
    <a class="ListLabel_20_1" href="https://dx.doi.org/10.3389/fphys.2012.00039">10.3389/fphys.2012.00039
</a>.
  </li>
<li class="bibitem">
    Longo, G., M. Montévil, and A. Pocheville. 2012. « From bottom-up approaches to levels of organization and extended critical transitions ». <span class="cmti-10">Frontiers in Physiology </span>3 (00232). ISSN: 1664-042X. doi:
    <a class="ListLabel_20_1" href="https://dx.doi.org/10.3389/fphys.2012.00232">10.3389/fphys.2012.00232
</a>.
  </li>
<li class="bibitem">Petitot, J. 2008. <span class="cmti-10">Neurogéométrie de la vision. </span>Ed de l’École Polytechnique.</li>
<li class="bibitem">Sonnenschein, C., and A.M. Soto. 1999. <span class="cmti-10">The society of cells</span> <span class="cmti-10">: cancer and control of cell</span> <span class="cmti-10">proliferation. </span>New York: Springer Verlag.</li>
<li class="bibitem">Testa F., Alessandro Palombo, Simona Dinicola, Fabrizio D'Anselmi, Sara Proietti, Alessia 4 Pasqualato, Maria Grazia Masiello, Pierpaolo Coluccia, Alessandra Cucina, Mariano Bizzarri 2013. <span class="cmti-10">Fractal analysis of shape changes in murine osteoblasts MC3T3-E1 cultured under simulated microgravity. </span><span class="T29">Prepirint</span><span class="cmti-10">, </span>Department of Department of Experimental Medicine, University La Sapienza, Rome, Italy.</li>
<li class="bibitem">Toulouse, G., P. Pfeuty, and G. Barton. 1977. <span class="cmti-10">Introduction to the renormalization group and</span> <span class="cmti-10">to critical phenomena. </span>London: Wiley.</li>
<li class="bibitem">
    Zinn-Justin, J. 2007. <span class="cmti-10">Phase transitions and renormalization group. </span>New York: Oxford University Press. ISBN
    : 0199227195.
  </li>
</ol>
<div class="indent footnotes">
  <hr />
  <p class="indent"><a class="Footnote_20_Symbol" href="#body_ftn1" id="ftn1">1</a><span class="Footnote_20_Characters"> </span>La cosmologie est une exception. Voilà une des raisons pour lesquelle elle pose tant de problèmes à l’interface relativiste/quantique.</p>
  <p class="indent"><a class="Footnote_20_Symbol" href="#body_ftn2" id="ftn2">2</a> Chez Darwin, elle est au coeur de la spéciation allopatrique.</p>
  <p class="indent"><a class="Footnote_20_Symbol" href="#body_ftn3" id="ftn3">3</a> Et si on relâche cette contrainte, dans les expériences de reproduction cellulaire en microgravité (Testa et al., 2013), la prolifération produit des variations importants dans la structure des cytosquelettes. Moins canalisée, la
reproduction donne lieu à des variantes.</p>
</div>
