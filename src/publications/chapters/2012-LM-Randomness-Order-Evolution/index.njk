---
excerpt: We revisit the analysis of anti-entropy. In particular, we analyze how randomness stemming from variability leads to the growth of biological organization.
illust: BiophysicsGiusMael1-img002.jpg
tags: 
    - anti-entropy
---

<!--CompileMaths-->


<div class="maketitle">
  <p class="titleHead" id="randomness-increases-order-in-biological-evolution">Randomness Increases Order in Biological Evolution</p>
  <p class="authors" >Giuseppe Longo and Maël Montévil</p>
</div>
<h3 class="abstract">Abstract</h3>
<p class="indent">
  In this text<span class="footnote-mark"><a href="#fn3x0" id="fn3x0-bk"><sup class="textsuperscript" id="x1-3f3">[3]</sup></a></span>, we revisit part of the analysis of anti-entropy in Bailly and Longo (<a id="pagee4" href="#X0-bailly2009">2009</a>) and develop further theoretical reflections. In particular, we analyze how randomness, an essential component of biological variability, is associated to the growth of biological organization, both in ontogenesis and in evolution. This approach, in particular, focuses on the role of global entropy production and provides a tool for a mathematical understanding of some fundamental observations by Gould on the increasing phenotypic complexity along evolution. Lastly, we analyze the situation in terms of theoretical symmetries, in order to further specify the biological meaning of anti-entropy as well as its strong link with randomness.
</p>
<h2 class="likesectionHead" id="introduction">Introduction</h2>
<p class="noindent">
  Notions of entropy are present in different branches of physics, but also in information theory, biology … even economics. Sometimes, they are equivalent under suitable transformations from one (more or less mathematized) domain to
  another. Sometimes, the relation is very mild, or may be at most due to a similar formal expressions. For example, one often finds formulas describing a linear dependence of entropy from a quantity formalized as
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mo class="MathClass-op">∑</mo>
        
      </mrow>
      <mrow>
        <mi>i</mi>
      </mrow>
    </msub>
    <msub>
      <mrow>
        <mi>p</mi>
      </mrow>
      <mrow>
        <mi>i</mi>
      </mrow>
    </msub>
    <mo class="qopname">log</mo>
    
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <msub>
          <mrow>
            <mi>p</mi>
          </mrow>
          <mrow>
            <mi>i</mi>
          </mrow>
        </msub>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
  </math>, where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>p</mi>
      </mrow>
      <mrow>
        <mi>i</mi>
      </mrow>
    </msub>
  </math>
  is the “probability” of the system to be in the
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>i</mi>
  </math>-th (micro-)state. Yet, different theoretical frames may give very different meanings to these formulas: somehow like a wave equation describing water
  movement has a similar mathematical formulation as Schrödinger’s wave equation (besides some crucial coefficients), yet water waves and quantum state functions have nothing to do with each other. Another element seems though to be
  shared by the different meanings given to entropy. The production of entropy is strictly linked to <span class="cmti-10">irreversible</span> processes.
</p>
<p class="indent">
  But … what is entropy? The notion originated in thermodynamics. The first law of thermodynamics is a conservation principle for energy. The second law states that the total entropy of a system will not decrease other than by
  increasing the entropy of some other system. Hence, in a system isolated from its environment, the entropy of that system will tend not to decrease.
</p>
<p class="indent">
  More generally, increasing entropy corresponds to <span class="cmti-10">energy dispersion</span>. And here we have the other element shared by the different views on entropy: in all of its instances, it is linked to randomness, since
  diffusions, in physics, are based on <span class="cmti-10">random walks</span>. Thus, energy, while being globally preserved, diffuses, randomly. In particular, heat flows from a hotter body to a colder body, never the inverse. Only the
  application of work (the imposition of order) may reverse this flow. As a matter of fact, entropy may be locally reversed, by pumping energy. For example, a centrifuge may separate two gazes, which mixed up by diffusion. This separation
  reduces the ergodicity (the amount of randomness, so to say) of the system, as well as its entropy.
</p>
<p class="indent">
  Living beings construct order by absorbing energy. In Schrödinger’s audacious little book, <span class="cmti-10">What is life? </span>(Schrödinger <a id="pagee5"  href="#X0-schrodinger">1944</a>), it is suggested that organisms <span class="cmti-10">also </span>use order to produce order, which he calls <span class="cmti-10">negentropy</span>, that is
  entropy with a negative sign. And this order is produced by using the order of the chromosomes’ a-periodic structure (his audacious conjecture) <span class="cmti-10">and </span>by absorbing organized nutrients (don’t we, the animal, eat
  mostly organized fibers?). Of course, a lot can be said, now, against these tentative theorizations by the great physicist.
</p>
<p class="indent">
  But is really entropy the same as disorder? There is a long lasting and sound critique, in physics, of the “myth” of entropy as disorder. F. L. Lambert (see Lambert (<a href="#X0-Lambert">2007</a>))
  is a firm advocate of this critical attitude. This is perfectly fair since entropy is “just” energy dispersal in physics, regardless of whether the system is open or closed<span class="footnote-mark"><a href="#fn4x0" id="fn4x0-bk"><sup class="textsuperscript" id="x1-1001f4">[4]</sup></a></span>. Yet, as explained in Hayflick (<a href="#X0-hayflick">2007</a>), <span class="cmti-10">“in physics, a lowered energy state is not</span>
  <span class="cmti-10">necessarily disorder, because it simply results in the identical molecule with a lowered</span> <span class="cmti-10">energy state. The fact that such a molecule might be biologically inactive may not</span>
  <span class="cmti-10">concern the physicist, but it definitely does concern the biologist </span><span class="cmti-10">….” </span>In this perspective, it is then sound to relate entropy to disorder in biological dynamics: a lesser
  activity of a molecule may mean metabolic instability, or, more generally, less coherent chemical activities of all sorts. As a consequence, this may result in less bio-chemical and biological order.
</p>
<p class="indent">
  In either case, though, and by definition, entropy has to be related to energy dispersal. As a matter of fact, the analysis of heat diffusion in animals and humans has a long history that dates back to the ‘30s (Hardy
  <a href="#X0-Hardy_1934">1934</a>). Since then, several approaches tried to bridge the conceptual gap between the purely physical perspective and the biologist’s concern with organization and with its
  opposite, disorder, in particular when increasing, in aging typically (Aoki <a href="#X0-Aoki1994">1994</a>; Hayflick <a href="#X0-hayflick">2007</a>; Marineo and
  Marotta <a href="#X0-Marineo">2005</a>; Pezard et al. <a href="#X0-Pezard_1998">1998</a>).
</p>
<p class="indent">
  Let’s now summarize the perspective of this paper in a very synthetic way: Evo/devo processes (Evolution and development or ontogenesis) may be globally understood as the “never identical iteration of a morphogenetic process”.
  Randomness is at the core of that “ <span class="cmti-10">never identical </span>iteration”. By adding selection and following Gould’s remarkable insight, we will in particular understand below the increasing compexity of organisms
  along Evolution, as the result of a purely random diffusion in a suitable phase space (and its defintion is the crucial issue).
</p>
<h2 class="sectionHead" id="1-entropy-in-ontogenesis"><span class="titlemark" id="x1-20001">1 </span>Entropy in ontogenesis.</h2>
<p class="noindent">
  In an organism, the internal entropy production has “in primis” a physical nature, related to all thermodynamic processes, that is to the transformation and exchange of matter and energy. Yet, we will add to this a properly biological
  production for entropy, the production due to all <span class="cmti-10">irreversible </span>processes, including biological (re-)construction, that is both embryogenesis and cell replacement and repair (ontogenesis, globally).
</p>
<p class="indent">
  Observe first that, in a monocellular organism, entropy is mostly released in the exterior environment and there are less signs of increasing disorder within the cell. Yet, changes in proteome and membranes are recorded and may be
  assimilated to aging, see Lindner et al. (<a id="pagee6"  href="#X0-Lindner_2008">2008</a>) and Nyström (<a href="#X0-Nystrom_2007">2007</a>). In a metazoan,
  instead, <span class="cmti-10">the</span> <span class="cmti-10">entropy produced, under all of its forms, is also and inevitably transferred to the</span>
  <span class="cmti-10">environing cells, to the tissue, to the organism, </span>(Bailly and Longo <a href="#X0-bailly2009">2009</a>). Thus, besides the internal forms of entropy (or disorder)
  production, a cell in a tissue, the structure of the tissue itself … the organism, is affected by this dispersal of energy, as increasing disorder, received from the (other) cells composing the tissue (or the organism). Aging,
  thus, is also or mostly a tissular and organismic process: in an organism, it is the network of interactions that is affected and that may have a fall-out also in the cellular activities (metabolism, oxidative stress … , see
  below).
</p>
<p class="indent">
  Moreover, the effect of the accumulation of entropy during life contributes, mathematically, to its <span class="cmti-10">exponential increase </span>in time. Thus, with aging, this increase exceeds the reconstructive activities, which
  oppose global entropy growth in earlier stages of life (this theory, articulated in four major life periods, is proposed in Bailly and Longo (<a href="#X0-bailly2009">2009</a>)). Now, we insist,
  entropy production, in all its forms, implies increasing disorganization of cells, tissues, and the organism. This, in turn, may be physically and biologically implemented by increasing metabolic instability, oxidative effects,
  weakening of the structure and coherence of tissues (matrix, collagene’s links, tensegrity) … and many more forms of progressive disorganization (Demetrius
  <a href="#X0-Demetrius01092004">2004</a>; d’Alessio <a href="#X0-dAlessio_2004">2004</a>; Sohal and Weindruch
  <a href="#X0-Sohal_1996">1996</a>; Olshansky and Rattan <a href="#X0-Olshansky_2005">2005</a>). Of course, there may be other causes of aging, but the entropic
  component should not be disregarded and may also help in proposing a unified understanding of different phenomena.
</p>
<p class="indent">
  Our second observation is that entropy production is due to <span class="cmti-10">all irreversible</span> <span class="cmti-10">processes</span>, both the thermodynamic ones and the permanent, irreversible, (re-)construction of the
  organism itself. This generating and re-generating activity, from embryogenesis to repair and turnover, is typically biological and it has been mathematically defined as “anti-entropy” (see Bailly and Longo (<a id="pagee7" href="#X0-bailly2009">2009</a>) and below<span class="footnote-mark"><a href="#fn5x0" id="fn5x0-bk"><sup class="textsuperscript" id="x1-2001f5">[5]</sup></a></span>). In other words, irreversibility in biology is not only due to thermodynamic effects, related to the production of energy, typically, but also to all processes that establish and maintain biological organization
  — that is, it is concomitantly due to entropy production and its biological opposite, anti-entropy production: embryogenesis, for example, is an organizing and highly irreversible process “per se”. And it produces entropy not only by
  the thermodynamic effects due to energy dispersion, but also, in our view, by the very biological constitutive activities.
</p>
<p class="indent">
  Cell mitosis is <span class="cmti-10">never an identical “reproduction”</span>, including the non-identity of proteomes and membranes. Thus, it induces an <span class="cmti-10">unequal diffusion of energy </span>by largely random
  effects (typically, the never identical bipartition of the proteome). That is, biological reproduction, as morphogenesis, is <span class="cmti-10">intrinsically joint to variability </span>and,
  <span class="cmti-10">thus, it produces entropy also by lack of (perfect) symmetries</span>. By this, it induces <span class="cmti-10">its</span> <span class="cmti-10">proper irreversibility</span>, beyond thermodynamics.
</p>
<p class="indent">
  As a comparison, consider an industrial construction of computers. The aim is to produce, in the same production chain, identical computers. Any time a computer is doubled, an identical one (up to observability) is produced and
  “organization” (locally) grows, at the expenses of energy. Entropy is then produced, in principle, only by the required use and inevitable dispersal of energy, while the construction “per se” just increases organization, along the
  production chain. Moreover, if, in the construction chain of computers, one destroys the second computer, you are back with one computer and you can iterate identically the production of the second. The process can be reverted (destroy
  one computer) and iteratable (produce again an identical machine), by importing a suitable amount of energy, of course. Imperfection should be (and are for
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>9</mn>
    <mn>9</mn>
    <mi>%</mi>
  </math> of the machine) below observability and functionality: they are errors and “noise”.
</p>
<p class="indent">
  As we said, it is instead a fundamental feature of life that a cell is <span class="cmti-10">never </span>identical to the “mother” cell. This is at the core of biological variability, thus of diversity, along Evolution as well as in
  embryogenesis (and ontogenesis, as permanent renewal of the organism, never identically). In no epistemic nor objective way this may be considered a result of errors nor noise: variability and diversity are the main “invariants” in
  biology, jointly to structural stability, which is never identity, and, jointly, they all make life possible.
</p>
<p class="indent">
  Thus, while producing new order (anti-entropy), life, as iteration of a never identical and an always <span class="cmti-10">slightly disordered </span>morphogenetic process, generates also entropy (disorder), by the reproductive process
  itself. In a metazoan, each mitosis produces two slightly different cells, both different also from the “mother” cell: the asymmetry is a form of disorder and, thus, of entropy growth, within the locally increasing order. And this, of
  course, in addition to the entropy due to free energy consumption. It is this variability that gives this further, and even more radical, form of irreversibility to all biological dynamics (in Evolution and ontogenesis). There is no way
  to neither revert nor iterate an evolutionary or embryognetic process: if you kill a cell after mitosis, you are not back to the same original cell and this cell will not iterate its reproduction,
  <span class="cmti-10">identically</span><span class="footnote-mark"><a href="#fn6x0" id="fn6x0-bk"><sup class="textsuperscript" id="x1-2002f6">[6]</sup></a></span>.
</p>
<p class="indent">
  It should be clear that this theoretical frame concerning the overall increase of entropy in biology says nothing about how this disorganization takes place in the various processes, nor anything about its “timetable”. The analyses of
  the detailed phenomena that implement it in ontogenesis are ongoing research projects. So far, we could apply these principles to an analysis of growing complexity in Evolution, as summarized next.
</p>
<h2 class="sectionHead" id="2-randomness-and-complexification-in-evolution"><span class="titlemark" id="x1-30002">2 </span>Randomness and Complexification in Evolution.</h2>
<p class="noindent">
  Available energy production and consumption are the unavoidable physical processes underlying reproduction and variability. At the origin of life, bacterial reproduction was (relatively) free, as other forms of life did not contrast it.
  Diversity, even in bacteria, by random differentiation, produced competition and a slow down of the exponential growth (see diagram <a href="#x1-40113">3</a>). Simultaneously, though, this started the
  early variety of live, a process never to stop.
</p>
<p class="indent">
  Gould, in several papers and in two books (Gould <a href="#X0-gould1989wonderful">1989</a>, <a href="#X0-gould1997full">1997</a>), uses this idea of random
  diversification in order to understand a blatant but too often denied fact: the increasing “complexification” of life. The increasing complexity of biological structures has been often denied in order to oppose finalistic and
  anthropocentric perspectives, which viewed life as <span class="cmti-10">aiming </span>at <span class="cmti-10">Homo sapiens </span>as the “highest” result of the (possibly intelligent) evolutionary path.
</p>
<p class="indent">
  Yet, it is a fact that, under many reasonable measures, an eukaryotic cell is more “complex” than a bacterium; a metazoan, with its differentiated tissues and its organs, is more “complex” than a cell … and that, by counting also
  neurons and connections, cell networks in mammals are more complex that in early triploblast (which have three tissues layers) and these have more complex networks of all sorts than diplobasts (like jellyfish, a very ancient animal).
  This non-linear increase can be quantified by counting tissue differentiations, networks and more, as hinted by Gould and more precisely proposed in Bailly and Longo (
  <a href="#X0-bailly2009">2009</a>), that we will extensively summarize and comment, next. The point is: how to understand this change towards complexity without invoking global aims? Gould provides a
  remarkable answer based on the analysis of the <span class="cmti-10">asymmetric </span>random diffusion of life. Asymmetric because, by principle, life cannot be less complex than bacterial life<span class="footnote-mark"><a href="#fn7x0" id="fn7x0-bk"><sup class="textsuperscript" id="x1-3001f7">[7]</sup></a></span>. So, reproduction by variability, along evolutionary time and space, randomly produces, just as <span class="cmti-10">possible paths</span>, also more complex individuals. Some happen to be compatible with the
  environment, resist and proliferate (a few even very successfully) and keep going, further and randomly producing <span class="cmti-10">also </span>more complex forms of life. <span class="cmti-10">Also</span>, since the random
  exploration of possibilities may, of course, decrease the complexity, no matter how this is measured. Yet, by principle: <span class="cmti-10">any asymmetric</span>
  <span class="cmti-10">random diffusion propagates, by local interactions, the original symmetry</span> <span class="cmti-10">breaking along the diffusion. </span>Thus there is no need for a global design or aim: the random paths that
  compose <span class="cmti-10">any </span>diffusion, also in this case help to understand a random growth of complexity, <span class="cmti-10">on average</span>. On average, as, of course, there may be local inversion in complexity; yet,
  the asymmetry randomly forces to the “right”. This is beautifully made visible by figure <a href="#x1-30041">1</a>, after Gould (
  <a href="#X0-gould1989wonderful" id="pagee9">1989</a>), page 205. The image explains the difference between a random, but oriented development (on the right,
  <a href="#x1-3003r2">1b</a>), and the non-biased, purely random diffusive bouncing of life expansion on the left wall, on the left <a href="#x1-3002r1">1a</a>.
</p>
<figure class="figure" id="x1-30041">
  <div  class="picgrid">
    <img alt=" Passive trend, there are more trajectories near 0." src="Ae-calude-GiusMael1-figure0-.png" width="400" class="img1 zoom darkFilter darkFilterT" />
    <img alt="Driven trend, the trajectories have a drift towards an increased mean." src="Ae-calude-GiusMael1-figure1-.png" width="400" class="img2 zoom darkFilter darkFilterT " />
    <p class="sub1" id="x1-3002r1">(a) Passive trend, there are more trajectories near 0.</p>
    <p class="sub2" id="x1-3003r2">
      (b) Driven trend, the trajectories have
      a drift towards an increased mean.
    </p>
  </div>
  <figcaption class="caption">
    <span class="id">Figure 1:</span>
    <span class="content"><span class="cmti-10">Passive and driven trends</span>. In one case, the <span class="cmti-10">boundary condition</span>, materialized by a left wall, is the only reason why the mean increases over time, and this increase is
therefore slow. In the case of a driven trend, or biased evolution, however, it is the <span class="cmti-10">rule </span>of the random walk that leads to an increase of the mean over time (there is an intrinsic trend in
evolution). Gould’s and our approach are based on passive trends, which means that we do ume that there is an intrinsic bias for increasing complexity in the process of evolution.</span>
  </figcaption>
</figure>
<p class="indent">
  Of course, time runs on the vertical axis, but … what is in the horizontal one? Anything or, more precisely, anywhere the random diffusion takes place or the intended phenomenon “diffuses in”. In particular, the horizontal axis may
  quantify “biological complexity” whatever this may mean. The point Gould wants to clarify is in the difference between a fully random vs. a random <span class="cmti-10">and </span>biased evolution. The biased right image does not
  apply to evolution: bacteria are still on Earth and very successfully. Any finalistic bias would instead separate the average random complexification from the left wall.
</p>
<p class="indent">
  Note that, in both cases, complexity may <span class="cmti-10">locally </span>decrease: tetrapodes may go back to the sea and lose their podia (the number of folding decreases, the overall body structure simplifies). Some cavern fishes
  may loose their eyes, in their new dark habitat; others, may lose their red blood cells (Ruud <a id="pagee12"  href="#X0-ruud1954vertebrates">1954</a>). Thus, the local propagation of the
  original asymmetry may be biologically understood as follows: on average, variation by simplification leads towards a biological niches that has <span class="cmti-10">more</span> <span class="cmti-10">chances </span>to be already
  occupied. Thus, <span class="cmti-10">global </span>complexity increases <span class="cmti-10">as a purely</span> <span class="cmti-10">random effect of variability </span>and on the grounds of
  <span class="cmti-10">local effects</span>: the greater chances, for a “simpler” organism, to bump against an already occupied niche. Thus, more complex variants have just slightly more probabilities to survive and reproduce — but this
  slight difference is enough to produce, in the long run, very complex biological organisms. And, of course, variability and, thus, diversity are grounded on randomness, in biology. No need for finalism nor a priori “global aim” nor
  “design” at all, just a consequence of an original symmetry breaking in a random diffusion on a very peculiar phase space: biomass times complexity times time (see figure <a href="#x1-40113">3</a> for a
  complete diagram)<span class="footnote-mark"><a href="#fn8x0" id="fn8x0-bk"><sup class="textsuperscript" id="x1-3005f8">[8]</sup></a></span>.
</p>
<p class="indent">
  Similarly to embryogenesis, the complexification is a form of local reversal of entropy. The global entropy of the Universe increases (or does not decrease), but locally, by using energy of course, life inverses the entropic trend and
  creates organization of increasing complexity. Of course, embryogenesis is a more canalized process, while evolution seems to explore all “possible” paths, within the ecosystem-to-be. Most turn out to be incompatible with the
  environment, thus they are eliminated by selection. In embryogenesis increasing complexity seems to follow an expected path and it is partly so. But only in part as failures, in mammals say, reach
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>5</mn>
    <mn>0</mn>
    <mi>%</mi>
  </math> or more: the constraints imposed, at least, by the inherited
  <span class="cmcsc-10"><span class="small-caps">dna</span> </span>and zygote, limit the random exploration due to cell mitosis. Yet, their variability, joint to the many
  constraints added to development (first, a major one: <span class="cmcsc-10"><span class="small-caps">dna</span></span>), is an essential component of cell differentiation.
  Tissue differentiation is, for our point of view, a form of (strongly) regulated/canalized variability along cell reproduction.
</p>
<p class="indent">
  Thus, by different but correlated effects, complexity as organization increases, on average, and reverts, locally, entropy. We called <span class="cmti-10">anti-entropy</span>, Bailly and Longo (
  <a href="#X0-bailly2009">2009</a>), this observable opposing entropy, both in evolution and embryogenesis; its peculiar nature is based on reproduction with random variation, submitted to constraints.
  As observed in the footnote above, anti-entropy differs from negentropy, which is just entropy with a negative sign, also because, when added to entropy, it never gives
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>0</mn>
  </math>, but it is realized in a very different singularity (different from
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>0</mn>
  </math>): extended criticality (Bailly and Longo <a href="#X0-bailly2011" id="pagee13">2011</a>; Longo and Montévil
  <a href="#X0-longo2011c">2011</a>). In the next section, we will use this notion to provide a mathematical frame for a further insight by Gould.
</p>
<h2 class="sectionHead" id="3-anti-entropy-in-evolution"><span class="titlemark" id="x1-40003">3 </span>(Anti-)Entropy in Evolution.</h2>
<p class="noindent">
  In yet another apparently naïve drawing, Gould proposes a further visualization of the increasing complexity of organisms along Evolution. It is just a qualitative image that the paleontologist draws on the grounds of his experience. It
  contains though a further remarkable idea: it suggests the “phase space” (the space of description) where one can analyze complexification. It is <span class="cmti-10">bio-mass density</span> that diffuses over
  <span class="cmti-10">complexity</span>, that is, figure <a href="#x1-40012">2</a> qualitatively describes the diffusion of the frequency of occurrences of individual organisms per unity of complexity.
</p>
<figure class="figure">
  <img alt="Evolution of complexity as understood by Gould." src="BiophysicsGiusMael1-img002.jpg" width="650" class="zoom darkFilter" id="x1-40012" />
  <figcaption class="caption">
    <span class="id">Figure 2:</span>
    <span class="content"><span class="cmti-10">Evolution of complexity as understood by Gould. </span>This illustration is borrowed from (Gould <a id="pagee15"  href="#X0-gould1997full">1997</a>), page 171.
This account is provided on the basis of paleontological observations.</span>
  </figcaption>
</figure>
<p class="indent">
  This is just a mathematically naive, global drawing of the paleontologist on the basis of data. Yet, it poses major mathematical challenges. The diffusion, here, is not along a spatial dimension. Physical observables usually diffuse
  over space in time; or, within other physical matter (which also amounts to diffusing in space). Here, diffusion takes place over an abstract dimension, “complexity”. But what does biological complexity exactly mean? Hints are given in
  Gould (<a id="pagee16"  href="#X0-gould1997full">1997</a>): the addition of a cellular nucleus (from bacteria to eukaryotes), the formation of metazoa, the increase in body size, the formation
  of fractal structures (usually — new — organs) and a few more…. In a sense, any added novelty provided by the random “bricolage” of Evolution and at least for some time compatible with the environment, contributes to complexity. Only a
  few organisms become more complex over time, but, by the original symmetry breaking mentioned above, this is enough to increase the global complexity. Of course, the figure above is highly unsatisfactory. It gives two slices over time
  where the second one is somewhat inconsistent: where are dinosaurs at present time? It is just a sketch, but an audacious one if analyzed closely. Mathematics may help us to consistently add the third missing dimension: time.
</p>
<p class="indent">
  A simple form of diffusion equation of
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>q</mi>
  </math> in time
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>t</mi>
  </math> over space
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>x</mi>
  </math> is:
</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
  <mtable class="align" columnalign="left">
    <mtr>
      <mtd class="align-odd" columnalign="right">
        <mfrac>
          <mrow>
            <mi>∂</mi>
            <mi>q</mi>
          </mrow>
          <mrow>
            <mi>∂</mi>
            <mi>t</mi>
          </mrow>
        </mfrac>
        <mo class="MathClass-rel">=</mo>
        <mi>D</mi>
        <mfrac>
          <mrow>
            <msup>
              <mrow>
                <mi>∂</mi>
              </mrow>
              <mrow>
                <mn>2</mn>
              </mrow>
            </msup>
            <mi>q</mi>
          </mrow>
          <mrow>
            <mi>∂</mi>
            <msup>
              <mrow>
                <mi>x</mi>
              </mrow>
              <mrow>
                <mn>2</mn>
              </mrow>
            </msup>
          </mrow>
        </mfrac>
        <mo class="MathClass-bin">+</mo>
        <mi>Q</mi>
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mi>t</mi>
            <mo class="MathClass-punc">,</mo>
            <mi>x</mi>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
      </mtd>
      <mtd class="align-even">
        <mspace width="2em"></mspace>
      </mtd>
      <mtd class="align-label" columnalign="right">
        <mstyle class="label" id="x1-4002r1"></mstyle>
        <mstyle class="maketag">
          <mtext>(1)</mtext>
        </mstyle>
      </mtd>
    </mtr>
  </mtable>
</math>
<p class="noindent">
  where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Q</mi>
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <mi>t</mi>
        <mo class="MathClass-punc">,</mo>
        <mi>x</mi>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
  </math>
  is a source term describing the situation at the origin of the process. Yet, in our case, the diffusion of this strange quantity,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>m</mi>
  </math>, a <span class="cmti-10">bio-mass density</span>, takes place over an even more unusual “space”, biological complexity, whatever the latter may mean. In
  Bailly and Longo (<a href="#X0-bailly2009">2009</a>), we dared to further specify Gould’s hints for biological complexity, as a quantity
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
    <mo class="MathClass-rel">=</mo>
    <mi>α</mi>
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
      </mrow>
    </msub>
    <mo class="MathClass-bin">+</mo>
    <mi>β</mi>
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>m</mi>
      </mrow>
    </msub>
    <mo class="MathClass-bin">+</mo>
    <mi>γ</mi>
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>f</mi>
      </mrow>
    </msub>
  </math>
  where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>α</mi>
  </math>,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>β</mi>
  </math>, and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>γ</mi>
  </math> are the respective “weights” of the different types of complexity within the total complexity. The details are in Bailly and Longo (
  <a href="#X0-bailly2009">2009</a>), let’s just summarize the basic ideas. So,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
      </mrow>
    </msub>
  </math>
  (“combinatorial” complexity) corresponds to the possible cellular combinatoric;
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>m</mi>
      </mrow>
    </msub>
  </math>
  (“morphological” complexity) is associated to the forms which arise (connexity and fractal structures);
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>f</mi>
      </mrow>
    </msub>
  </math>
  (“functional” complexity) is associated to the relational structures supporting biological functions (metabolic and neuronal relations). We will discuss this approach in section <a href="#x1-50004">4</a>.
</p>
<p class="indent">
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
  </math> is a tentative quantification of complexity as <span class="cmti-10">anti-entropy</span>, in particular in biological evolution: the increase of each of
  its components (more cellular differentiation, more or higher dimensional fractal structures, richer networks … yield a more “complex” individual). Of course, many more observables and parameters may be taken into account in order
  to evaluate the complexity of an organism: Bailly and Longo (<a id="pagee17"  href="#X0-bailly2009">2009</a>) provides just a mathematical basis and a biological core for a preliminary analysis
  (an application to ontogenesis as an analysis of <span class="cmti-10">C.</span> <span class="cmti-10">Elegans </span>development is also presented). They suffice though for a qualitative (geometric) reconstruction of Gould’s curve,
  with a sound extension to the time dimension.
</p>
<p class="indent">
  As mentioned above, anti-entropy opposes, locally, to entropy: it has the same dimension, yet it differs from negentropy, since it does not sum up to
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>0</mn>
  </math>, in presence of an equal quantity of entropy. It differs also from information theoretic frame, where negentropy has been largely used, as negentropy (=
  information) is <span class="cmti-10">independent from coding and Cartesian dimensions</span>. This is crucial for Shannon as well as for Kolmogorof-Chaitin information theories. Anti-entropy, instead, as defined above, depends on
  foldings, singularities, fractality … it is a <span class="cmti-10">geometric </span>notion, thus, by definition, it is <span class="cmti-10">sensitive to codings </span>(and to dimension).
</p>
<p class="indent">
  The next step is to adapt eq. <a href="#x1-4002r1">1</a> to these new dimensions. Just use Gould’s observables and parameters,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>m</mi>
  </math> and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
  </math>, that we specified some more, and write:
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mfrac>
            <mrow>
              <mi>∂</mi>
              <mi>m</mi>
            </mrow>
            <mrow>
              <mi>∂</mi>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-rel">=</mo>
          <mi>D</mi>
          <mfrac>
            <mrow>
              <msup>
                <mrow>
                  <mi>∂</mi>
                </mrow>
                <mrow>
                  <mn>2</mn>
                </mrow>
              </msup>
              <mi>m</mi>
            </mrow>
            <mrow>
              <mi>∂</mi>
              <msup>
                <mrow>
                  <mi>K</mi>
                </mrow>
                <mrow>
                  <mn>2</mn>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
          <mo class="MathClass-bin">+</mo>
          <mi>Q</mi>
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mi>t</mi>
              <mo class="MathClass-punc">,</mo>
              <mi>K</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4003r2"></mstyle>
          <mstyle class="maketag">
            <mtext>(2)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  But what is here
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Q</mi>
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <mi>t</mi>
        <mo class="MathClass-punc">,</mo>
        <mi>K</mi>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
  </math>, the source term? In order to instantiate
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Q</mi>
  </math> by a specific function, but also in order to see the biological system from a different perspective (and get to the equation also by an “operatorial
  approach”), we then gave a central role, as an observable, to the “global entropy production”.
</p>
<p class="indent">
  Now, in physics, energy,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>E</mi>
  </math>, is the “main” observable, since Galileo inertia, a principle of energy conservation, to Noether’s theorems and Schrödinger’s equation. Equilibria,
  geodetic principles etc directly or indirectly refer to energy and are understood in terms of symmetry principles (see Bailly and Longo (<a id="pagee18"  href="#X0-bailly2011">2011</a>)). At
  least since Schrödinger and his equation, in (quantum) physics, one may view energy as an operator and time as a parameter<span class="footnote-mark"><a href="#fn9x0" id="fn9x0-bk"><sup class="textsuperscript" id="x1-4004f9">[9]</sup></a></span>.
</p>
<p class="indent">
  As hinted above, in biology, also constitutive processes, such as anti-entropy growth (the construction and reconstruction of organization), <span class="cmti-10">produce entropy</span>, since they also produce some (new) disorder
  (recall: at least the proteome, after a mitosis, is non-uniformly and randomly distributed in the new cells). In these far form equilibrium, dissipative (possibly even non-stationary) processes, such as Evolution and ontogenesis, energy
  turns out to be just one (very important) observable, a parameter to be precise. One eats (and this is essential) and gets fatter: production and maintenance of organization requires energy, but it yields a different observable, one
  that has a different dimension, tentatively defined by
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
  </math> above, as organization. Typically, in allometric equations, so relevant in biology, energy or mass appear as a parameter. Thus, in our approach, the key
  observable is organization that is formed or renewed (anti-entropy production).
</p>
<p class="indent">
  Moreover, <span class="cmti-10">entropy</span>, as associated to all irreversible processes, from energy flows to anti-entropy production, is the observable which summarizes all ongoing phenomena; by its irreversibility, it is strongly
  linked (conjugated) to time.
</p>
<p class="indent">
  In summary, we proposed to change the conceptual frame and the conceptual priorities: we associated the global entropy production
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>σ</mi>
  </math> to the differential operator given by time,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>∂</mi>
    <mo class="MathClass-bin">∕</mo>
    <mi>∂</mi>
    <mi>t</mi>
  </math> (Schrödinger does this for energy, which is conjugated to time, in quantum physics). Thus, our
  approach allows to consider biological time as an “operator”, both in this technical sense and in the global perspective of attributing to time a key constitutive role in biological phenomena, from evolution to ontogenesis. But how to
  express this global observable?
</p>
<p class="indent">
  In a footnote to Schrödinger (<a href="#X0-schrodinger" id="pagee19">1944</a>), Schrödinger proposes to analyze his notion of negative entropy as a form of Gibbs free energy
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>G</mi>
  </math>. We applied this idea to our anti-entropy
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow>
        <mi>S</mi>
      </mrow>
      <mrow>
        <mo class="MathClass-bin">-</mo>
      </mrow>
    </msup>
  </math>, where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow>
        <mi>S</mi>
      </mrow>
      <mrow>
        <mo class="MathClass-bin">-</mo>
      </mrow>
    </msup>
    <mo class="MathClass-rel">=</mo>
    <mo class="MathClass-bin">-</mo>
    <mi>k</mi>
    <mi>K</mi>
  </math>
  (
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>k</mi>
  </math> is a positive dimensional constant and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
  </math> is the phenotypic complexity). Now,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>G</mi>
    <mo class="MathClass-rel">=</mo>
    <mi>H</mi>
    <mo class="MathClass-bin">-</mo>
    <mi>T</mi>
    <mi>S</mi>
  </math>, where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
  </math> is temperature,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>S</mi>
  </math> is entropy and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>H</mi>
    <mo class="MathClass-rel">=</mo>
    <mi>U</mi>
    <mo class="MathClass-bin">+</mo>
    <mi>P</mi>
    <mi>V</mi>
  </math> is the system’s enthalpy (
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>U</mi>
  </math> is the internal energy,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>P</mi>
  </math> and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>V</mi>
  </math> are respectively pressure and volume). By definition, the <span class="cmti-10">metabolism</span>
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>R</mi>
  </math> has the physical dimension of a power and corresponds to the difference between the fluxes of <span class="cmti-10">generalized free</span>
  <span class="cmti-10">energy </span>
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>G</mi>
  </math> through the surface
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Σ</mi>
  </math>:
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mi>R</mi>
          <mo class="MathClass-rel">=</mo>
          <mo mathsize="big"> ∑</mo>
          <mrow>
            <mo class="MathClass-open">[</mo>
            <mrow>
              <msub>
                <mrow>
                  <mi>J</mi>
                </mrow>
                <mrow>
                  <mi>G</mi>
                </mrow>
              </msub>
              <mrow>
                <mo class="MathClass-open">(</mo>
                <mrow>
                  <mi>x</mi>
                </mrow>
                <mo class="MathClass-close">)</mo>
              </mrow>
              <mo class="MathClass-bin">-</mo>
              <msub>
                <mrow>
                  <mi>J</mi>
                </mrow>
                <mrow>
                  <mi>G</mi>
                </mrow>
              </msub>
              <mrow>
                <mo class="MathClass-open">(</mo>
                <mrow>
                  <mi>x</mi>
                  <mo class="MathClass-bin">+</mo>
                  <mi>d</mi>
                  <mi>x</mi>
                </mrow>
                <mo class="MathClass-close">)</mo>
              </mrow>
            </mrow>
            <mo class="MathClass-close">]</mo>
          </mrow>
          <mo class="MathClass-rel">=</mo>
          <mo class="MathClass-bin">-</mo>
          <mo mathsize="big">∑</mo>
          <mi>d</mi>
          <mi>x</mi>
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mo class="qopname">Div</mo>
              
              <msub>
                <mrow>
                  <mi>J</mi>
                </mrow>
                <mrow>
                  <mi>G</mi>
                </mrow>
              </msub>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4005r3"></mstyle>
          <mstyle class="maketag">
            <mtext>(3)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">Locally the conservation (or balance) equation is expressed in the general form:</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mi>R</mi>
          <mo class="MathClass-rel">=</mo>
          <mo class="MathClass-bin">-</mo>
          <mo class="qopname">Div</mo>
          
          <msub>
            <mrow>
              <mi>J</mi>
            </mrow>
            <mrow>
              <mi>G</mi>
            </mrow>
          </msub>
          <mo class="MathClass-rel">=</mo>
          <mfrac>
            <mrow>
              <mi>d</mi>
              <mi>G</mi>
            </mrow>
            <mrow>
              <mi>d</mi>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-bin">+</mo>
          <mi>T</mi>
          <mi>σ</mi>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4006r4"></mstyle>
          <mstyle class="maketag">
            <mtext>(4)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>σ</mi>
  </math> represents the speed of global production of entropy, that is
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>σ</mi>
  </math> is the entropy produced by <span class="cmti-10">all </span>irreversible processes, including the production of biological organization or anti-entropy.
  Thus, the global balance of metabolism for the “system of life” (the biosphere) has the following form, where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow>
        <mi>S</mi>
      </mrow>
      <mrow>
        <mo class="MathClass-bin">-</mo>
      </mrow>
    </msup>
  </math>
  and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow>
        <mi>S</mi>
      </mrow>
      <mrow>
        <mo class="MathClass-bin">+</mo>
      </mrow>
    </msup>
  </math>
  are anti-entropy and entropy, respectively:
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mi>R</mi>
          <mo class="MathClass-rel">=</mo>
          <mfrac>
            <mrow>
              <mi>d</mi>
              <mi>H</mi>
            </mrow>
            <mrow>
              <mi>d</mi>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-bin">-</mo>
          <mi>T</mi>
          <mfenced close=")" open="(" separators>
            <mrow>
              <mfrac>
                <mrow>
                  <mi>d</mi>
                  <msup>
                    <mrow>
                      <mi>S</mi>
                    </mrow>
                    <mrow>
                      <mo class="MathClass-bin">-</mo>
                    </mrow>
                  </msup>
                </mrow>
                <mrow>
                  <mi>d</mi>
                  <mi>t</mi>
                </mrow>
              </mfrac>
              <mo class="MathClass-bin">+</mo>
              <mfrac>
                <mrow>
                  <mi>d</mi>
                  <msup>
                    <mrow>
                      <mi>S</mi>
                    </mrow>
                    <mrow>
                      <mo class="MathClass-bin">+</mo>
                    </mrow>
                  </msup>
                </mrow>
                <mrow>
                  <mi>d</mi>
                  <mi>t</mi>
                </mrow>
              </mfrac>
            </mrow>
          </mfenced>
          <mo class="MathClass-bin">+</mo>
          <mi>T</mi>
          <mi>σ</mi>
          <mo class="MathClass-rel">≃</mo>
          <mi>a</mi>
          <mfrac>
            <mrow>
              <mi>d</mi>
              <mi>M</mi>
            </mrow>
            <mrow>
              <mi>d</mi>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-bin">-</mo>
          <mi>T</mi>
          <mfenced close=")" open="(" separators>
            <mrow>
              <mfrac>
                <mrow>
                  <mi>d</mi>
                  <msup>
                    <mrow>
                      <mi>S</mi>
                    </mrow>
                    <mrow>
                      <mo class="MathClass-bin">-</mo>
                    </mrow>
                  </msup>
                </mrow>
                <mrow>
                  <mi>d</mi>
                  <mi>t</mi>
                </mrow>
              </mfrac>
              <mo class="MathClass-bin">+</mo>
              <mfrac>
                <mrow>
                  <mi>d</mi>
                  <msup>
                    <mrow>
                      <mi>S</mi>
                    </mrow>
                    <mrow>
                      <mo class="MathClass-bin">+</mo>
                    </mrow>
                  </msup>
                </mrow>
                <mrow>
                  <mi>d</mi>
                  <mi>t</mi>
                </mrow>
              </mfrac>
            </mrow>
          </mfenced>
          <mo class="MathClass-bin">+</mo>
          <mi>T</mi>
          <mi>σ</mi>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4007r5"></mstyle>
          <mstyle class="maketag">
            <mtext>(5)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>H</mi>
    <mo class="MathClass-rel">≃</mo>
    <mi>a</mi>
    <mi>M</mi>
  </math>, for a mass
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>M</mi>
  </math> and a coefficient
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>a</mi>
  </math>, which has the magnitude of a speed squared.
</p>
<p class="indent">
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
    <mi>σ</mi>
  </math> is a crucial quantity: it contains our
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>σ</mi>
  </math>, modulo the temperature
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
  </math>, since
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>R</mi>
  </math> is a power.
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
    <mi>σ</mi>
  </math> corresponds to the product of forces by fluxes (of matter, of energy — chemical energy, for instance — etc.). Now, a flux is proportional to a
  force, thus to a mass, and hence
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
    <mi>σ</mi>
  </math> is proportional to a mass squared. It can then be written, up to a coefficient
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>ζ</mi>
      </mrow>
      <mrow>
        <mi>b</mi>
      </mrow>
    </msub>
  </math>
  and a constant term
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
    <msub>
      <mrow>
        <mi>σ</mi>
      </mrow>
      <mrow>
        <mn>0</mn>
      </mrow>
    </msub>
  </math>
  as:
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mi>T</mi>
          <mi>σ</mi>
          <mspace class="nbsp" width="1em"></mspace>
          <mo class="MathClass-rel">≈</mo>
          <mspace class="nbsp" width="1em"></mspace>
          <msub>
            <mrow>
              <mi>ζ</mi>
            </mrow>
            <mrow>
              <mi>b</mi>
            </mrow>
          </msub>
          <msup>
            <mrow>
              <mi>M</mi>
            </mrow>
            <mrow>
              <mn>2</mn>
            </mrow>
          </msup>
          <mo class="MathClass-bin">+</mo>
          <mi>T</mi>
          <msub>
            <mrow>
              <mi>σ</mi>
            </mrow>
            <mrow>
              <mn>0</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4008r6"></mstyle>
          <mstyle class="maketag">
            <mtext>(6)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>ζ</mi>
      </mrow>
      <mrow>
        <mi>b</mi>
      </mrow>
    </msub>
  </math>
  is a constant that depends only on the global nature of the biological system under study and it is
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>0</mn>
  </math> in absence of living matter.
</p>
<p class="indent">
  Without entering into further details, by using as “state function” a <span class="cmti-10">bio-mass diffusion function </span>over complexity
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
  </math>, that is the bio-mass density
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>m</mi>
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <mi>t</mi>
        <mo class="MathClass-punc">,</mo>
        <mi>K</mi>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
  </math>
  in
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>t</mi>
  </math> and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>K</mi>
  </math>, the operatorial approach applied to equation <a href="#x1-4008r6">6</a> gave us the equation, with a linear source
  function
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>α</mi>
      </mrow>
      <mrow>
        <mi>b</mi>
      </mrow>
    </msub>
    <mi>m</mi>
  </math>
  :
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mfrac>
            <mrow>
              <mi>∂</mi>
              <mi>m</mi>
            </mrow>
            <mrow>
              <mi>∂</mi>
              <mi>t</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-rel">=</mo>
          <msub>
            <mrow>
              <mi>D</mi>
            </mrow>
            <mrow>
              <mi>b</mi>
            </mrow>
          </msub>
          <mfrac>
            <mrow>
              <msup>
                <mrow>
                  <mi>∂</mi>
                </mrow>
                <mrow>
                  <mn>2</mn>
                </mrow>
              </msup>
              <mi>m</mi>
            </mrow>
            <mrow>
              <mi>∂</mi>
              <msup>
                <mrow>
                  <mi>K</mi>
                </mrow>
                <mrow>
                  <mn>2</mn>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
          <mo class="MathClass-bin">+</mo>
          <msub>
            <mrow>
              <mi>α</mi>
            </mrow>
            <mrow>
              <mi>b</mi>
            </mrow>
          </msub>
          <mi>m</mi>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4009r7"></mstyle>
          <mstyle class="maketag">
            <mtext>(7)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">Its solution, bellow, yields the diagram in figure <a href="#x1-40113">3</a>.</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mi>m</mi>
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mi>t</mi>
              <mo class="MathClass-punc">,</mo>
              <mi>K</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mo class="MathClass-rel">=</mo>
          <mfrac>
            <mrow>
              <mi>A</mi>
            </mrow>
            <mrow>
              <msqrt>
                <mrow>
                  <mi>t</mi>
                </mrow>
              </msqrt>
            </mrow>
          </mfrac>
          <mo class="qopname">exp</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mi>a</mi>
              <mi>t</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mo class="qopname">exp</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mo class="MathClass-bin">-</mo>
              <msup>
                <mrow>
                  <mi>K</mi>
                </mrow>
                <mrow>
                  <mn>2</mn>
                </mrow>
              </msup>
              <mo class="MathClass-bin">∕</mo>
              <mn>4</mn>
              <mi>D</mi>
              <mi>t</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
        </mtd>
        <mtd class="align-even">
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-4010r8"></mstyle>
          <mstyle class="maketag">
            <mtext>(8)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<figure class="figure" id="x1-40113">
  <img alt="Time evolution of mass repartition over anti-entropy. " src="BiophysicsGiusMael1-img003.png" width="800" class="zoom darkFilterT darkFilter" />
  <figcaption class="caption">
    <span class="id">Figure 3:</span>
    <span class="content"><span class="cmti-10">Time evolution of mass</span> <span class="cmti-10">repartition over anti-entropy</span>. The initial condition is a finite mass at almost
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>0</mn>
      </math> anti-entropy, thus having the shape of a pulse.</span>
  </figcaption>
</figure>
<p class="indent">
  In summary, by skipping all the technical details in Bailly and Longo (<a id="pagee25"  href="#X0-bailly2009">2009</a>), we could derive, by mathematics and starting from Gould’s informal hints,
  a general understanding as well as the behavior of the “Evolution of complexity function” w. r. to time. And this fits data: at the beginning the linear source term gives an exponential growth of free bacteria. Then, they complexify and
  compete. Of course, this diagram, similarly to Gould’s, is a global one: it only gives a qualitative, geometric, understanding of the process. It is like looking at life on Earth from Sirius. Analogously to Gould’s diagram, the
  “punctuated equilibria”, say, and the major extinctions are not visible: the insight is from too far and too synthetic to appreciate them. It only theoretically justifies Gould’s proposal and soundly extends it to time dependence, by
  mathematically deriving it from general principles: the dynamics of a diffusion by random paths, with an asymmetric origin. Its source is given by an exponential growth. Life expansion is then bounded, canalized, selected in the
  interaction with the ever changing, co-constituted ecosystem. The core random complexification persists, while its “tail” exponentially decreases, see equation <a href="#x1-4010r8">8</a> and figure
  <a href="#x1-40113">3</a>. In that tail, some neotenic big primates, with a huge neural network, turn out to be the random complexification of bacteria, a result of variability and of the immense
  massacres imposed by selection.
</p>
<p class="indent">
  Another (important) analogy can be made with Schrödinger’s approach (his famous equation, not his book on life) and further justifies the reference to it for the analysis of this (rather ordinary) diffusion equation. Schrödinger dared
  to describe the deterministic evolution of the wave function in Quantum Mechanics as the <span class="cmti-10">dynamics of a law of probability </span>(and this gives the intrinsic indetermination of the quantum system). We
  synthetically represented Biological Evolution as the <span class="cmti-10">dynamics</span> <span class="cmti-10">of a potential of variability</span>, under the left wall constraint. Again, this idea is essentially Gould’s idea in his
  1996 book: he sees Evolution just as an asymmetric diffusion of random variability. We just made this point explicit and developed some computations as a consequences of the analogy with the determination in Quantum Mechanics and the
  operatorial approach of Schrödinger.
</p>
<h2 class="sectionHead" id="4-anti-entropy-as-a-measure-of-symmetry-changes"><span class="titlemark" id="x1-50004">4 </span>Anti-entropy as a measure of symmetry changes</h2>
<p class="noindent">
  In Longo and Montévil (<a href="#X0-longo2011c">2011</a>), we proposed to understand biological phenomena, in comparison and contrast with physical theories, as a situation where the theoretical
  symmetries are “constantly” broken. We will now show that such considerations allows us to interpret anti-entropy, somewhat in the spirit of Boltzmann’s approach of physical entropy. In Bailly and Longo (
  <a href="#X0-bailly2009">2009</a>), premises of these aspects are considered from a strictly combinatorial point of view, leading to a “constructive” definition of the three components of
  anti-entropy, we recalled in section <a href="#x1-40003">3</a>. To show how symmetries come into play we will analyze now these components.
</p>
<dl class="description">
  <dt class="description">
    Combinatorial complexity,
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
        <mrow>
          <mi>K</mi>
        </mrow>
        <mrow>
          <mi>c</mi>
        </mrow>
      </msub>
    </math>:
  </dt>
  <dd class="description">
    For a total number of cells
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>N</mi>
    </math> and for a number
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
        <mrow>
          <mi>n</mi>
        </mrow>
        <mrow>
          <mi>j</mi>
        </mrow>
      </msub>
    </math>
    of cells of cell type
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>j</mi>
    </math>, the combinatorial complexity is defined as:
    
    <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
      <mtable class="align" columnalign="left">
        <mtr>
          <mtd class="align-odd" columnalign="right">
            <msub>
              <mrow>
                <mi>K</mi>
              </mrow>
              <mrow>
                <mi>c</mi>
              </mrow>
            </msub>
            <mo class="MathClass-rel">=</mo>
            <mo class="qopname"> log</mo>
            
            <mfenced close=")" open="(" separators>
              <mrow>
                <mfrac>
                  <mrow>
                    <mi>N</mi>
                    <mo class="MathClass-punc">!</mo>
                  </mrow>
                  <mrow>
                    <munder class="msub">
                      <mrow>
                        <mo mathsize="big">∏</mo>
                      </mrow>
                      <mrow>
                        <mi>j</mi>
                      </mrow>
                    </munder>
                    <msub>
                      <mrow>
                        <mi>n</mi>
                      </mrow>
                      <mrow>
                        <mi>j</mi>
                      </mrow>
                    </msub>
                    <mo class="MathClass-punc">!</mo>
                  </mrow>
                </mfrac>
              </mrow>
            </mfenced>
          </mtd>
          <mtd class="align-even">
            <mspace width="2em"></mspace>
          </mtd>
          <mtd class="align-label" columnalign="right">
            <mstyle class="label" id="x1-5001r9"></mstyle>
            <mstyle class="maketag">
              <mtext>(9)</mtext>
            </mstyle>
          </mtd>
        </mtr>
      </mtable>
    </math>
    <p >
      A classical combinatorial point of view consists in saying that it is the number of ways to classify
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
      </math> cells in
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>j</mi>
      </math> categories each of sizes
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>n</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
      </math>. More precisely, we recognize, inside the logarithm, the cardinal,
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
        <mo class="MathClass-punc">!</mo>
      </math>, of the symmetry group
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>S</mi>
          </mrow>
          <mrow>
            <mi>N</mi>
          </mrow>
        </msub>
      </math>, that is the group of transformations, called permutations, that exchange the labels of
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
      </math> elements. Similarly,
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>n</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo class="MathClass-punc">!</mo>
      </math>
      is the number of permutations among
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>n</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
      </math>
      units, which has the biological meaning of permutations of cells within a cell type: in other words, permuting cells <span class="cmti-10">within the same cell type</span> is a combinatorial invariant of the complexity of an
      organism. Thus, the group of permutations leaving the cell types invariants is the group
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>G</mi>
          </mrow>
          <mrow>
            <mi>t</mi>
            <mi>y</mi>
            <mi>p</mi>
            <mi>e</mi>
          </mrow>
        </msub>
        <mo class="MathClass-rel">=</mo>
        <mo class="MathClass-op"> ∏</mo>
        
        <msub>
          <mrow>
            <mi>S</mi>
          </mrow>
          <mrow>
            <msub>
              <mrow>
                <mi>n</mi>
              </mrow>
              <mrow>
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
      </math>, that is the group obtained as direct product of the symmetries corresponding to permutations within each cell type. Formally, this group corresponds to the change of labels in each cell type, which can all be performed
      independently and conserve the classification by cell types. The cardinal of this group is
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mo class="MathClass-op">∏</mo>
            
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mrow>
            <mi>n</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo class="MathClass-punc">!</mo>
      </math>.
    </p>
    <p >
      Then, the number of cell type configurations is the number of orbits generated by the right action of
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>G</mi>
          </mrow>
          <mrow>
            <mi>t</mi>
            <mi>y</mi>
            <mi>p</mi>
            <mi>e</mi>
          </mrow>
        </msub>
      </math>
      on
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>S</mi>
          </mrow>
          <mrow>
            <mi>N</mi>
          </mrow>
        </msub>
      </math>. In other words, a cell type configuration is first given by a permutation of
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>⟦</mi>
        <mn>1</mn>
        <mo class="MathClass-punc">,</mo>
        <mi>N</mi>
        <mi>⟧</mi>
      </math>, which gives the random determination for
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
      </math> cells. Moreover, these transformations must be computed modulo any transformation of
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>G</mi>
          </mrow>
          <mrow>
            <mi>t</mi>
            <mi>y</mi>
            <mi>p</mi>
            <mi>e</mi>
          </mrow>
        </msub>
      </math>
      that gives the same configuration (as we said, cells within each cell type are combinatorially equivalent — we will discuss below this hypothesis, in more biological terms). Lagrange theorem then gives the number of remaining
      transformations
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
        <mo class="MathClass-punc">!</mo>
        <mo class="MathClass-bin">∕</mo>
        <msub>
          <mrow>
            <mo class="MathClass-op">∏</mo>
            
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mrow>
            <mi>n</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo class="MathClass-punc">!</mo>
      </math>, which is the number of possible configurations. Clearly, an organism with just one cell type (typically, a unicellular being) has combinatorial complexity
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>0</mn>
      </math>. As a result, this measure of combinatorial complexity depends on the total number
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>N</mi>
      </math> of cells, but is actually <span class="cmti-10">a measure of the symmetry breaking induced by the</span>
      <span class="cmti-10">differentiation in cell types</span>.
    </p>
    <p class="noindent">
      Let’s compare the situation with Boltzmann approach of entropy. If one has a number of microscopic phase space states
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>Ω</mi>
      </math> having the same energy, the corresponding entropy is defined as
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>S</mi>
        <mo class="MathClass-rel">=</mo>
        <msub>
          <mrow>
            <mi>k</mi>
          </mrow>
          <mrow>
            <mi>b</mi>
          </mrow>
        </msub>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mi>Ω</mi>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
      </math>. In the case of gases, one considers that the particles are indiscernible. This means that one does not count twice situations which differ only by permuting particles. In other words one formally understands the situation by
      saying that labels attached to particles are arbitrary. Thus, more soundly,
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>S</mi>
      </math> is defined by
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>S</mi>
        <mo class="MathClass-rel">=</mo>
        <msub>
          <mrow>
            <mi>k</mi>
          </mrow>
          <mrow>
            <mi>b</mi>
          </mrow>
        </msub>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mi>Ω</mi>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
        <mo class="MathClass-bin">-</mo>
        <msub>
          <mrow>
            <mi>k</mi>
          </mrow>
          <mrow>
            <mi>b</mi>
          </mrow>
        </msub>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mi>N</mi>
            <mo class="MathClass-punc">!</mo>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
        <mo class="MathClass-rel">&gt;</mo>
        <mn>0</mn>
      </math>. This symmetry by permutation reduces the size of the microscopic possibility space, and, as a result, entropy.
    </p>
    <p class="noindent">
      In our approach, we have
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>K</mi>
          </mrow>
          <mrow>
            <mi>c</mi>
          </mrow>
        </msub>
        <mo class="MathClass-rel">=</mo>
        <mo class="qopname"> log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mi>N</mi>
            <mo class="MathClass-punc">!</mo>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
        <mo class="MathClass-bin">-</mo>
        <msub>
          <mrow>
            <mo class="qopname">∑</mo>
            
          </mrow>
          <mrow>
            <mi>i</mi>
          </mrow>
        </msub>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <msub>
              <mrow>
                <mi>n</mi>
              </mrow>
              <mrow>
                <mi>i</mi>
              </mrow>
            </msub>
            <mo class="MathClass-punc">!</mo>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
      </math>
      which is greater than
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>0</mn>
      </math>, as soon as there is more than one cell type. Thus, the increase of the possibility space (the diversity or the differentiations) increases the
      complexity. More precisely, the complexity, as absolute value of anti-entropy, is decreased by the remaining symmetries, quantified by the term
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mo class="MathClass-op">∑</mo>
            
          </mrow>
          <mrow>
            <mi>i</mi>
          </mrow>
        </msub>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <msub>
              <mrow>
                <mi>n</mi>
              </mrow>
              <mrow>
                <mi>i</mi>
              </mrow>
            </msub>
            <mo class="MathClass-punc">!</mo>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
      </math>. We understand then that anti-entropy can be analyzed, at least in this case, as an account of how much biological symmetries are broken by the cascade of differentiations. Formally, we can sum the situation up by saying that
      the combinatorial complexity and its contribution to anti-entropy are based on a group of transformations,
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>S</mi>
          </mrow>
          <mrow>
            <mi>N</mi>
          </mrow>
        </msub>
      </math>, and a subgroup,
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>G</mi>
          </mrow>
          <mrow>
            <mi>t</mi>
            <mi>y</mi>
            <mi>p</mi>
            <mi>e</mi>
          </mrow>
        </msub>
      </math>. The biologically relevant quantity is then the ratio of sizes of the groups
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>S</mi>
          </mrow>
          <mrow>
            <mi>N</mi>
          </mrow>
        </msub>
      </math>
      and
      
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
          <mrow>
            <mi>G</mi>
          </mrow>
          <mrow>
            <mi>t</mi>
            <mi>y</mi>
            <mi>p</mi>
            <mi>e</mi>
          </mrow>
        </msub>
      </math>.
    </p>
  </dd>
  <dt class="description">
    Morphological complexity, 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
        <mrow>
          <mi>K</mi>
        </mrow>
        <mrow>
          <mi>m</mi>
        </mrow>
      </msub>
    </math>:
  </dt>
  <dd class="description">
    This complexity is associated to the geometrical description of biologically relevant shapes. It is computed in particular by counting the number of connex areas. Note that this number corresponds to
    <span class="cmti-10">space symmetry</span> <span class="cmti-10">breakings </span>for motions covering this space — or ergodic motions. Then, one has to consider the number of shape singularities, in the mathematical sense, where
    singularities are invariants by action of diffeomorphisms. The fractal-like structures are particularly relevant since they correspond to an exponential increase of the number of geometrical singularities with the range of scales
    involved. Thus, fractal-like structures lead to a linear growth of anti-entropy with the order of magnitudes where fractality is observed.
  </dd>
  <dt class="description">Functional complexity, 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
        <mrow>
          <mi>K</mi>
        </mrow>
        <mrow>
          <mi>f</mi>
        </mrow>
      </msub>
    </math>
  </dt>
  <dd class="description">
    (the last quantity proposed in Bailly and Longo (<a id="pagee28"  href="#X0-bailly2009">2009</a>)): This quantity is given by the number of possible graphs of interaction. As a result, the
    corresponding component of anti-entropy is given by the choice of one graph structure (with distinguished nodes) among the possible graphs. This involves the selection of the structure of possible graphs and, correspondingly, which
    resulting graphs are considered equivalent. In terms of symmetries, we first have a symmetry among the possible graphs which is reduced to a smaller symmetry, by the equivalence relation. For example, in Bailly and Longo (
    <a href="#X0-bailly2009">2009</a>), the case is considered where the number of edges is fixed, so the considered symmetry group is engendered by the transformations which combine the deletion of
    an edge and the creation of another one. The orbits preserve the total number of edges, so that the orbit of a graph with
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mrow>
        <mo class="MathClass-open">⟨</mo>
        <mrow>
          <mi>k</mi>
        </mrow>
        <mo class="MathClass-close">⟩</mo>
      </mrow>
      <mi>N</mi>
    </math>
    edges are the graphs with this number of edges.
  </dd>
</dl>
<p class="indent">
  We understand then that anti-entropy, or at least its proposed decomposition in Bailly and Longo (<a href="#X0-bailly2009">2009</a>), is strictly correlated to the amount of symmetry changes. We will
  now look more closely at the case of combinatorial complexity since it involves only the groups of permutations and their subgroups, but at the same time will also allow us to express a crucial conceptual and mathematical point.
</p>
<p class="indent">
  We indeed encounter a paradox in the case of combinatorial complexity. On one side, we have an assumption that cells of the same cell type are symmetric (interchangeable). On the other, in section
  <a href="#x1-20001">1</a>, we stressed that each cell division consists in a symmetry change. This apparent paradox depends on the scale we use to analyze the problem, as well as on the “plasticity” of
  the cells in a tissue or organ, as the possibility to be interchanged and/or to modify their individual organization. Typically, one can assume that liver cells function statistically (what matters is their average contribution to the
  function of the organ), while neurons may have strong specific activities, yet they may also deeply modify their structure (change number, forms and functionality of synaptic connections, for example). Thus, we will next consider the
  individual contribution of cells to the combinatorial complexity of an organism at different scales.
</p>
<p class="indent">
  If we consider an organism with a large number of cells,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>N</mi>
  </math>, and the proportion
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>q</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
  </math>
  for cell type
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>j</mi>
  </math> we get two different quantities for the combinatorial complexities,
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
  and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mn>2</mn>
      </mrow>
    </msub>
  </math>
  :
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mfrac>
            <mrow>
              <msub>
                <mrow>
                  <mi>K</mi>
                </mrow>
                <mrow>
                  <mi>c</mi>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi>N</mi>
            </mrow>
          </mfrac>
        </mtd>
        <mtd class="align-even">
          <mo class="MathClass-rel">=</mo>
          <mfrac>
            <mrow>
              <mo class="qopname">log</mo>
              
              <mrow>
                <mo class="MathClass-open">(</mo>
                <mrow>
                  <mi>N</mi>
                  <mo class="MathClass-punc">!</mo>
                </mrow>
                <mo class="MathClass-close">)</mo>
              </mrow>
            </mrow>
            <mrow>
              <mi>N</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-rel">≃</mo>
          <mo class="qopname"> log</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mi>N</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-odd" columnalign="right">
          <mspace class="quad" width="1em"></mspace>
          <mfrac>
            <mrow>
              <msub>
                <mrow>
                  <mi>K</mi>
                </mrow>
                <mrow>
                  <mi>c</mi>
                  <mn>2</mn>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi>N</mi>
            </mrow>
          </mfrac>
        </mtd>
        <mtd class="align-even">
          <mo class="MathClass-rel">=</mo>
          <mfrac>
            <mrow>
              <mo class="qopname">log</mo>
              
              <mfenced close=")" open="(" separators>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mi>N</mi>
                      <mo class="MathClass-punc">!</mo>
                    </mrow>
                    <mrow>
                      <munder class="msub">
                        <mrow>
                          <mo mathsize="big">∏</mo>
                        </mrow>
                        <mrow>
                          <mi>j</mi>
                        </mrow>
                      </munder>
                      <mrow>
                        <mo class="MathClass-open">(</mo>
                        <mrow>
                          <msub>
                            <mrow>
                              <mi>q</mi>
                            </mrow>
                            <mrow>
                              <mi>j</mi>
                            </mrow>
                          </msub>
                          <mi>N</mi>
                        </mrow>
                        <mo class="MathClass-close">)</mo>
                      </mrow>
                      <mo class="MathClass-punc">!</mo>
                    </mrow>
                  </mfrac>
                </mrow>
              </mfenced>
            </mrow>
            <mrow>
              <mi>N</mi>
            </mrow>
          </mfrac>
          <mo class="MathClass-rel">≃</mo>
          <munder class="msub">
            <mrow>
              <mo mathsize="big">∑</mo>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </munder>
          <msub>
            <mrow>
              <mi>q</mi>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </msub>
          <mo class="qopname">log</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mn>1</mn>
              <mo class="MathClass-bin">∕</mo>
              <msub>
                <mrow>
                  <mi>q</mi>
                </mrow>
                <mrow>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-5002r10"></mstyle>
          <mstyle class="maketag">
            <mtext>(10)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  We propose to understand the situation as follows. Basically, both levels of cellular individuation are valid; but they have to be arranged in the right order. Cellular differentiation is the first and main aspect of the ability of
  cells to individuate in a metazoan, so we can assume that the main determinant of combinatorial complexity is
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mn>2</mn>
      </mrow>
    </msub>
  </math>. It is only after this contribution that the further process of cellular individuation occurs. The latter leads to a mean contribution which is
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mo class="MathClass-op">∑</mo>
        
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
    <mfenced close=")" open="(" separators>
      <mrow>
        <msub>
          <mrow>
            <mi>q</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <msub>
              <mrow>
                <mi>q</mi>
              </mrow>
              <mrow>
                <mi>j</mi>
              </mrow>
            </msub>
            <mi>N</mi>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
        <mo class="MathClass-bin">-</mo>
        <mn>1</mn>
      </mrow>
    </mfenced>
  </math>
  per cell, where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
  </math>
  quantifies the ability of each cell type to change their organization. It seems reasonable to expect that the
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
  </math>
  are high in the cases, for example, of neurons or of cells of the immune system. On the contrary, the
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
  </math>
  should be especially low for red blood cells. The reason for this is not only their lack of <span class="cmcsc-10"><span class="small-caps">dna</span></span>, but also
  their relatively simple and homogeneous cytoplasmic organization. Similarly, liver cells may have statistically irrelevant changes in their individual structure.
</p>
<p class="indent">
  Thus, the contribution of cell types to anti-entropy derives first from the formation of new cell types, while considering the ability of cells to reproduce, with changes, within a cell type as a further important (numerically dominant)
  aspect of their individuation process. Note that this analysis does not suppose that a cell type for a cell is irreversibly determined, but it means that the contribution of cell type changes to anti-entropy are understood as changes of
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mn>2</mn>
      </mrow>
    </msub>
  </math>.
</p>
<p class="indent">
  We can then provide a refined version of
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mrow>
        <mi>S</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
      </mrow>
      <mrow>
        <mo class="MathClass-bin">-</mo>
      </mrow>
    </msubsup>
  </math>, where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mi>t</mi>
      </mrow>
    </msub>
  </math>
  is the “weight” accorded to the formation of different cell types:
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <mfrac>
            <mrow>
              <msubsup>
                <mrow>
                  <mi>S</mi>
                </mrow>
                <mrow>
                  <mi>c</mi>
                </mrow>
                <mrow>
                  <mo class="MathClass-bin">-</mo>
                </mrow>
              </msubsup>
            </mrow>
            <mrow>
              <mo class="MathClass-bin">-</mo>
              <mi>N</mi>
              <msub>
                <mrow>
                  <mi>k</mi>
                </mrow>
                <mrow>
                  <mi>b</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mtd>
        <mtd class="align-even">
          <mo class="MathClass-rel">=</mo>
          <msub>
            <mrow>
              <mi>a</mi>
            </mrow>
            <mrow>
              <mi>c</mi>
              <mi>t</mi>
            </mrow>
          </msub>
          <munder class="msub">
            <mrow>
              <mo mathsize="big">∑</mo>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </munder>
          <msub>
            <mrow>
              <mi>q</mi>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </msub>
          <mo class="qopname">log</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mn>1</mn>
              <mo class="MathClass-bin">∕</mo>
              <msub>
                <mrow>
                  <mi>q</mi>
                </mrow>
                <mrow>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mo class="MathClass-bin">+</mo>
          <munder class="msub">
            <mrow>
              <mo mathsize="big">∑</mo>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </munder>
          <msub>
            <mrow>
              <mi>a</mi>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </msub>
          <mfenced close=")" open="(" separators>
            <mrow>
              <msub>
                <mrow>
                  <mi>q</mi>
                </mrow>
                <mrow>
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo class="qopname">log</mo>
              
              <mrow>
                <mo class="MathClass-open">(</mo>
                <mrow>
                  <msub>
                    <mrow>
                      <mi>q</mi>
                    </mrow>
                    <mrow>
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mi>N</mi>
                </mrow>
                <mo class="MathClass-close">)</mo>
              </mrow>
              <mo class="MathClass-bin">-</mo>
              <mn>1</mn>
            </mrow>
          </mfenced>
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-5003r11"></mstyle>
          <mstyle class="maketag">
            <mtext>(11)</mtext>
          </mstyle>
        </mtd>
      </mtr>
      <mtr>
        <mtd class="align-odd" columnalign="right"></mtd>
        <mtd class="align-even">
          <mo class="MathClass-rel">=</mo>
          <mfenced close=")" open="(" separators>
            <mrow>
              <msub>
                <mrow>
                  <mi>a</mi>
                </mrow>
                <mrow>
                  <mi>c</mi>
                  <mi>t</mi>
                </mrow>
              </msub>
              <mo class="MathClass-bin">-</mo>
              <mrow>
                <mo class="MathClass-open">⟨</mo>
                <mrow>
                  <msub>
                    <mrow>
                      <mi>a</mi>
                    </mrow>
                    <mrow>
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
                <mo class="MathClass-close">⟩</mo>
              </mrow>
            </mrow>
          </mfenced>
          <mfenced close="⟩" open separators>
            <mrow>
              <mo class="qopname">log</mo>
              
              <mrow>
                <mo class="MathClass-open">(</mo>
                <mrow>
                  <mn>1</mn>
                  <mo class="MathClass-bin">∕</mo>
                  <msub>
                    <mrow>
                      <mi>q</mi>
                    </mrow>
                    <mrow>
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
                <mo class="MathClass-close">)</mo>
              </mrow>
            </mrow>
          </mfenced>
          <mo class="MathClass-bin">+</mo>
          <mfenced close="⟩" open separators>
            <mrow>
              <mfenced close=")" open="(" separators>
                <mrow>
                  <mrow>
                    <mo class="MathClass-open">⟨</mo>
                    <mrow>
                      <msub>
                        <mrow>
                          <mi>a</mi>
                        </mrow>
                        <mrow>
                          <mi>j</mi>
                        </mrow>
                      </msub>
                    </mrow>
                    <mo class="MathClass-close">⟩</mo>
                  </mrow>
                  <mo class="MathClass-bin">-</mo>
                  <msub>
                    <mrow>
                      <mi>a</mi>
                    </mrow>
                    <mrow>
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfenced>
              <mo class="qopname">log</mo>
              
              <mrow>
                <mo class="MathClass-open">(</mo>
                <mrow>
                  <mn>1</mn>
                  <mo class="MathClass-bin">∕</mo>
                  <msub>
                    <mrow>
                      <mi>q</mi>
                    </mrow>
                    <mrow>
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
                <mo class="MathClass-close">)</mo>
              </mrow>
            </mrow>
          </mfenced>
          <mo class="MathClass-bin">+</mo>
          <mrow>
            <mo class="MathClass-open">⟨</mo>
            <mrow>
              <msub>
                <mrow>
                  <mi>a</mi>
                </mrow>
                <mrow>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mo class="MathClass-close">⟩</mo>
          </mrow>
          <mo class="qopname">log</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mi>N</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-5004r12"></mstyle>
          <mstyle class="maketag">
            <mtext>(12)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>x</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
  </math>
  is the mean of
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>x</mi>
  </math> among all cells (so that the contribution of each cell type is proportional to its proportion in the organism). Both equations
  <a href="#x1-5003r11">11</a> and <a href="#x1-5004r12">12</a> are biologically meaningful. The terms in equation
  <a href="#x1-5003r11">11</a> correspond, by order of appearance, to the contribution of the categorization by cell types and to the contribution of individuation among a cell type. In equation
  <a href="#x1-5004r12">12</a>, we have obtained terms that can be assimilated to
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
  (last term) and to
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mn>2</mn>
      </mrow>
    </msub>
  </math>
  (first term), the latter being positive only if
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>c</mi>
        <mi>t</mi>
      </mrow>
    </msub>
    <mspace class="thinspace" width="0.3em"></mspace>
    <mo class="MathClass-bin">-</mo>
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <msub>
          <mrow>
            <mi>a</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
    <mspace class="thinspace" width="0.3em"></mspace>
    <mo class="MathClass-rel">&gt;</mo>
    <mn>0</mn>
  </math>, meaning that the contribution associated to cell types is positive only if it is greater than the mean cellular individuation. This is logical since cell types make a positive contribution to the complexity only if the amount of
  cellular diversity they introduce is greater than the one that cellular individuation alone would introduce.
</p>
<p class="indent">
  Last but not least, the second term has the sign of an anti-correlation between
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
  </math>
  and
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mo class="qopname">log</mo>
    
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <mn>1</mn>
        <mo class="MathClass-bin">∕</mo>
        <msub>
          <mrow>
            <mi>q</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
  </math>, meaning that this term is positive when there are many low complexity cell types<span class="footnote-mark"><a href="#fn10x0" id="fn10x0-bk"><sup class="textsuperscript" id="x1-5005f10">[10]</sup></a></span> and few high complexity cell types. More precisely, using the Cauchy-Schwartz equality case, we get that maximizing (and minimizing) this term (everything else being kept constant), leads to
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <msub>
          <mrow>
            <mi>a</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
    <mo class="MathClass-bin">-</mo>
    <msub>
      <mrow>
        <mi>a</mi>
      </mrow>
      <mrow>
        <mi>j</mi>
      </mrow>
    </msub>
    <mo class="MathClass-rel">∝</mo>
    <mo class="qopname"> log</mo>
    
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <mn>1</mn>
        <mo class="MathClass-bin">∕</mo>
        <msub>
          <mrow>
            <mi>q</mi>
          </mrow>
          <mrow>
            <mi>j</mi>
          </mrow>
        </msub>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
    <mo class="MathClass-bin">-</mo>
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mo class="qopname">log</mo>
        
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mn>1</mn>
            <mo class="MathClass-bin">∕</mo>
            <msub>
              <mrow>
                <mi>q</mi>
              </mrow>
              <mrow>
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
  </math>. Then this optimization <span class="cmti-10">a priori </span>leads to maximizing the <span class="cmti-10">variance </span>of information (in informational terms), at constant entropy (=mean information).
</p>
<p class="indent">
  Here, the issue derived from looking with an increasing finer resolution at the individuation potential. However, the reciprocal situation can also occur. Let’s consider the functional complexity, understood as the possibility of
  interactions between cells (the paradigmatic example is neurons). Then, by assuming that there are
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>N</mi>
  </math> neurons with
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>k</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
  </math>
  average number of synapses for each neuron (where
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>k</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
  </math>
  is between 10<sup class="textsuperscript">3</sup> and 10<sup class="textsuperscript">4</sup> for humans), as presented in Bailly and Longo (<a id="pagee31" href="#X0-bailly2009">2009</a>), we get:
</p>
<p>
  <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
    <mtable class="align" columnalign="left">
      <mtr>
        <mtd class="align-odd" columnalign="right">
          <msub>
            <mrow>
              <mi>N</mi>
            </mrow>
            <mrow>
              <mi>G</mi>
            </mrow>
          </msub>
        </mtd>
        <mtd class="align-even">
          <mo class="MathClass-rel">=</mo>
          <mfenced close=")" open="(" separators>
            <mfrac linethickness="0.0pt">
              <mrow>
                <mfenced close=")" open="(" separators>
                  <mfrac linethickness="0.0pt">
                    <mrow>
                      <mi>N</mi>
                    </mrow>
                    <mrow>
                      <mn>2</mn>
                    </mrow>
                  </mfrac>
                </mfenced>
              </mrow>
              <mrow>
                <mrow>
                  <mo class="MathClass-open">⟨</mo>
                  <mrow>
                    <mi>k</mi>
                  </mrow>
                  <mo class="MathClass-close">⟩</mo>
                </mrow>
                <mi>N</mi>
              </mrow>
            </mfrac>
          </mfenced>
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-odd" columnalign="right">
          <mfrac>
            <mrow>
              <msub>
                <mrow>
                  <mi>K</mi>
                </mrow>
                <mrow>
                  <mi>f</mi>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi>N</mi>
            </mrow>
          </mfrac>
        </mtd>
        <mtd class="align-even">
          <mo class="MathClass-rel">≃</mo>
          <mrow>
            <mo class="MathClass-open">⟨</mo>
            <mrow>
              <mi>k</mi>
            </mrow>
            <mo class="MathClass-close">⟩</mo>
          </mrow>
          <mo class="qopname">log</mo>
          
          <mrow>
            <mo class="MathClass-open">(</mo>
            <mrow>
              <mi>N</mi>
            </mrow>
            <mo class="MathClass-close">)</mo>
          </mrow>
          <mspace width="2em"></mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
          <mstyle class="label" id="x1-5006r13"></mstyle>
          <mstyle class="maketag">
            <mtext>(13)</mtext>
          </mstyle>
        </mtd>
      </mtr>
    </mtable>
  </math>
</p>
<p class="noindent">
  However, if we postulate that <span class="cmti-10">any </span>graph of interaction is possible, then we get a total number of possible interactions which corresponds to a choice between interaction or no interaction for each entry of
  the interaction matrix (
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow>
        <mi>N</mi>
      </mrow>
      <mrow>
        <mn>2</mn>
      </mrow>
    </msup>
  </math>
  cells). However, the latter is symmetric; and we do not count the self-interactions (because they correspond to the complexity of the cell) so we obtain
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>N</mi>
    <mrow>
      <mo class="MathClass-open">(</mo>
      <mrow>
        <mi>N</mi>
        <mo class="MathClass-bin">-</mo>
        <mn>1</mn>
      </mrow>
      <mo class="MathClass-close">)</mo>
    </mrow>
    <mo class="MathClass-bin">∕</mo>
    <mn>2</mn>
  </math>
  binary choices, so
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow>
        <mn>2</mn>
      </mrow>
      <mrow>
        <mi>n</mi>
        <mrow>
          <mo class="MathClass-open">(</mo>
          <mrow>
            <mi>n</mi>
            <mo class="MathClass-bin">-</mo>
            <mn>1</mn>
          </mrow>
          <mo class="MathClass-close">)</mo>
        </mrow>
        <mo class="MathClass-bin">∕</mo>
        <mn>2</mn>
      </mrow>
    </msup>
  </math>
  possibilities:
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>f</mi>
        <mn>2</mn>
      </mrow>
    </msub>
    <mo class="MathClass-bin">∕</mo>
    <mi>N</mi>
    <mo class="MathClass-rel">≃</mo>
    <mi>N</mi>
    <mo class="MathClass-bin">∕</mo>
    <mn>2</mn>
  </math>.
</p>
<p class="indent">
  There is two main lines of reasoning we can follow to understand the situation. The first is to look at the time structure of symmetry changes. Indeed, the symmetry changes occur as a temporal cascade. As a result, the temporal
  hierarchy of individuation is crucial. Here, we can refer to some phenomena concerning the graph of interaction of neurons. A crude description of the formation of neural networks is the following. First, a large number of “disordered”
  connections take place. Only after, the functional organization really increases by the decay of unused synapses (see for example Luo and O’Leary (<a href="#X0-luo2005axon">2005</a>). Then, the
  “bigger” symmetry group involved in the description is of the form
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow>
        <mi>K</mi>
      </mrow>
      <mrow>
        <mi>f</mi>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>, with
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>k</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
  </math>
  mean number of connections; but then this symmetry group is reduced to obtain a smaller symmetry group with
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>l</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
  </math>
  mean number of connections. This operation can be seen as a change of symmetry groups, from the transformations preserving the number of connections with
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>k</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
    <mi>N</mi>
  </math>
  connections to those preserving
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow>
      <mo class="MathClass-open">⟨</mo>
      <mrow>
        <mi>l</mi>
      </mrow>
      <mo class="MathClass-close">⟩</mo>
    </mrow>
    <mi>N</mi>
  </math>
  connections.
</p>
<p class="indent">
  Of course there are many other possible components for a measure of biological complexity. This proposal, defined as anti-entropy, provides just a tentative backbone for transforming the informal notion of “biological organizational
  complexity” into a mathematical observable, that is into a real valued function defined over a biological phenomenon. It should be clear that, once enriched well beyond the definition and the further details given in Bailly and Longo (
  <a id="pagee32"  href="#X0-bailly2009">2009</a>), this is a proper (and fundamental) biological observable. It radically differs from the rarely quantified, largely informal, always discrete
  (informally understood as a map from topologically trivial structures to integer numbers) notion of “information”, still dominating in molecular circles, see Longo et al. (
  <a href="#X0-longo2012">2012</a>) for a critique of this latter notion.
</p>
<h2 class="sectionHead" id="5-theoretical-consequences-of-this-interpretation"><span class="titlemark" id="x1-60005">5 </span>Theoretical consequences of this interpretation</h2>
<p class="noindent">
  In the section above, we have been focused on technical aspects of the “microscopic” definition of anti-entropy. Using this method, we have seen that anti-entropy can mainly be understood in terms of symmetry changes. We will now
  consider the theoretical meaning of this situation in a more general way. As we exposed in Longo and Montévil (<a href="#X0-longo2011c">2011</a>), we propose to understand biological systems as
  characterized by a cascade of symmetry changes. Now, our understanding of a “biological trajectory”, a philogenetic and ontogenetic path, as a cascade of symmetry changes yields a proper form of randomness to be associated to the
  construction and maintenance of biological organization. This perspective is particularly relevant for us, since it links the two theoretical approaches of the living state of matter that our team has introduced: anti-entropy (Bailly
  and Longo <a href="#X0-bailly2009">2009</a>) and extended criticality (Bailly and Longo <a href="#X0-bailly2008">2008</a>; Longo and Montévil
  <a href="#X0-longo2011c">2011</a>).
</p>
<p class="indent">
  More precisely, in phylogenesis, the randomness is associated to the “choice” of different organizational forms, which occurs even when the biological objects are confronted with remarkably similar physical environment and physiological
  constraints. For example, the lungs of birds and mammals have the same function in similar conditions; but they have phylogenetic histories which diverged long ago and, extremely different structures.
</p>
<p class="indent">
  This example is particularly prone to lead to approximate common symmetries, since it relates to a vital function (respiration and therefore gas exchanges) shared by a wide class of organisms. It is noteworthy that numerous theoretical
  studies have analyzed lungs by optimality principles (Horsfield <a id="pagee33"  href="#X0-horsfield1977">1977</a>; West, Brown, and Enquist
  <a href="#X0-west1999">1999</a>; Gheorghiu et al. <a href="#X0-Gheorghiu2005">2005</a>). However, the optimality principles differ in these studies (minimum entropy
  production, maximum energetic efficiency, maximum surface/volume ratio, …). Accordingly, even among mammals, structural variability remains high. For example, Nelson, West, and Goldberger (
  <a href="#X0-Nelson_1990">1990</a>) describe the differences in the geometrical scaling properties of human lungs on one side, and of rats, dogs and hamsters lungs on the other side. Moreover, Mauroy
  et al. (<a href="#X0-Mauroy2004">2004</a>) show that the criteria of energetic optimality and of robustness for the gas exchanges, with respect to geometric variations, are incompatible. More
  generally, optimization criteria are not particularly theoretically stable. In particular robustness is a relative notion: it depends on the property considered as well as on the transformations with respect to which we expect it to be
  robust (Lesne <a href="#X0-AnnickLesne08">2008</a>).
</p>
<p class="indent">
  Similarly, the theoretical symmetries constituted in ontogenesis are the result of the interactions with the environment and of the developmental trajectory already followed at a given time. In our perspective, this trajectory must then
  be understood as a history of symmetry changes. And, of course, the situation at a given moment does not “determine” the symmetry changes that the object will undergo. This is a crucial component of the randomness of the biological
  dynamics, as we consider that random events are associated to symmetry changes. These events are given by the interplay of the organism with its own physiology (and internal milieu) and with its environment, the latter being partially
  co-constituted by the theoretical symmetries of the organism, since the relevant aspects of the environment depend also on the organism.
</p>
<p class="indent">
  In other terms, the conservation, in biology, is not associated to the biological <span class="cmti-10">proper observables</span>, the phenotype, and the same (physical) interface (e.g. energy exchange) with the environment may yield
  very different phenotypes; thus, there is no need to preserve a specific phenotype. In short, the symmetry changes occurring in an organism can only be analyzed in terms of the previous theoretical symmetries (biology is, first, an
  historical science) and the differences of the possible changes can be associated to different forms of randomness.
</p>
<p class="indent">
  In the cases of symmetry breakings, the symmetry change corresponds to the passage to a subgroup of the original symmetry group. As a result, the theoretical possibilities are predefinable (as the set of subgroups of the original
  group). This typically occurs in the case of physical phase transitions, and the result is then a randomness associated to the choice of how the symmetry gets broken. For example, if an organism has an approximate rotational symmetry,
  this symmetry can be broken in a subgroup, for example by providing a particular oriented direction. We then have a rotational symmetry along an axis. This can again be broken, for example into a discrete subgroup of order
  
  <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
    <mn>5</mn>
  </math> (starfish).
</p>
<p class="indent">
  Another situation corresponds to the case where the symmetry changes are constituted on the basis of already determined theoretical symmetries (which can be altered in the process). This can be analyzed as the formation of additional
  observables which are attached to or the result of already existing ones. Then these symmetry changes are associated with already determined properties, but their specific form is nevertheless not predetermined. A typical example is the
  case of physically non-generic behaviours that can be found in the physical analysis of some biological situations, see Lesne and Victor (<a id="pagee34"  href="#X0-Lesne06">2006</a>). From the
  point of view of the theoretical structure of determination, it is then a situation where there are predetermined attachment points, prone to lead the biological system to develop its further organization on them. The form of the
  biological response to these organizational opportunities of complexification is not, however, predetermined and then generates an original form of randomness. This theoretical account is close to the notion of next adjacent niche,
  proposed in Kauffman (<a href="#X0-kauffman2002investigations">2002</a>); however, we emphasize, here, that the theoretical determination of these next organizational possibilities is only partially
  predetermined. For example imagine that a biological dynamic has approximately certain symmetries, which leads to a non-generic singular point; then it is possible (and maybe probable) that this point will be stabilized in evolution, in
  an unknown way.
</p>
<p class="indent">
  The former case is constituted, in a sense, by a <span class="cmti-10">specific </span>organizational opportunity. We can, however, consider cases where such opportunities are not theoretical predetermined. Now, the constitution of
  symmetry changes should be understood as even more random, and the associated predictability is extremely low. Gould’s most quoted example of “exaptation’, the formation of the bones of the internal hear from the double jaw of some
  tetrapods, some two hundred million years ago can fit in this category.
</p>
<p class="indent">
  We have seen that the symmetry changes lead to a strong form of randomness. This randomness and its iterative accumulation are, however, the very fabric of biological organization. Therefore, we have a theoretical situation where order
  (biological organization) is a direct consequence of randomness. Its global analysis allowed us to give mathematical sense to Gould’s evolutionary complexification along evolution, as a consequence of the random paths of a asymmetric
  diffusion (sections <a href="#x1-30002">2</a> and <a href="#x1-40003">3</a>). A finer (or local) analysis suggested a way to understand also ontogenetic changes in these
  terms, that is as a random dynamics of symmetry changes. This situation should be not confused with the cases of order by fluctuations or statistical stabilization (for example, by the central limit theorem). In our case, indeed, the
  order is not the result of a statistical regularization of random dynamics into a stable form, which would transform them into a deterministic frame. On the contrary, the random path of a cascade of symmetry changes yields the
  theoretical symmetries of the object (its specific phenotypes), which also determine its behaviour.
</p>
<p class="indent">
  In this context, the irreversibility of these random processes is taken into account by entropy production. The latter, or more precisely a part of the latter, is then associated to the ability of biological objects to generate
  variability, thus adaptability. In ontogenesis, this point confirms our analysis of the contribution of anti-entropy regeneration to entropy production, in association with variability, including cellular differentiation. This situation
  is also consistent with our analysis of anti-entropy as a measure of symmetry changes. Notice that the symmetry changes, considered as relevant with respect to anti-entropy, may be taken into account, for example, in the coefficients
  corresponding to the individuation capacity of different cell types in our discussion above (see section <a href="#x1-50004">4</a>).
</p>
<h2 class="sectionHead" id="references">References</h2>
<ol class="thebibliography">
  <li class="bibitem" id="X0-Aoki1994">
    Aoki, I. 1994. “Entropy production in human life span: A thermodynamical measure for aging.” <span class="cmti-10">AGE </span>17 (1): 29–31.
      <span class="cmcsc-10"><span class="small-caps" id="pagee35">i</span><span class="small-caps">ssn</span></span>: 0161-9152. doi:
      <a href="http://dx.doi.org/10.1007/BF02435047">10.1007/BF02435047</a>. (Cited on page <a href="#pagee6">6</a>).
    </li>
  <li class="bibitem" id="X0-bailly2007">
      Bailly, F., and G. Longo. 2007. “Randomness and determinism in the interplay between the continuum and the discrete.” <span class="cmti-10">Mathematical Structures</span> <span class="cmti-10">in Computer Science </span>17 (02):
      289–305. doi:<a href="http://dx.doi.org/10.1017/S0960129507006007">10.1017/S0960129507006007</a>. (Cited on page <a href="#pagee41">41</a>).
    </li>
  <li class="bibitem" id="X0-bailly2008">
      _________. 2008. “Extended critical situations: the physical singularity of life phenomena.” <span class="cmti-10">Journal of Biological Systems </span>16 (2): 309. doi:
      <a href="http://dx.doi.org/10.1142/S0218339008002514">10.1142/S0218339008002514</a>. (Cited on page <a href="#pagee33">33</a>).
    </li>
  <li class="bibitem" id="X0-bailly2009">
      _________. 2009. “Biological organization and anti-entropy.” <span class="cmti-10">Journal of</span> <span class="cmti-10">Biological Systems </span>17 (1): 63–96. doi:
      <a href="http://dx.doi.org/10.1142/S0218339009002715">10.1142/S0218339009002715</a>. (Cited on pages <a href="#pagee4">4</a>, <a href="#pagee6">6</a> <a href="#pagee8">sqq.</a>, <a href="#pagee12">12</a>,
      <a href="#pagee16">16</a> <a href="#pagee17">sq.</a>, <a href="#pagee25">25</a>, <a href="#pagee28">28</a>, <a href="#pagee31">31</a> <a href="#pagee32">sq.</a>, <a href="#pagee41">41</a>).
    </li>
  <li class="bibitem" id="X0-bailly2011">
      _________. 2011. <span class="cmti-10">Mathematics and the natural sciences; The Physical</span> <span class="cmti-10">Singularity of Life. </span>London: Imperial College Press. (Cited on pages <a href="#pagee13">13</a>,
      <a href="#pagee18">18</a>).
    </li>
  <li class="bibitem" id="X0-dAlessio_2004">
      d’Alessio, P. 2004. “Aging and the endothelium.” <span class="cmti-10">Experimental Gerontology</span> 39 (2): 165–171.
      <span class="cmcsc-10"><span class="small-caps">issn</span></span>: 0531-5565. doi:
      <a href="http://dx.doi.org/10.1016/j.exger.2003.10.025">10.1016/j.exger.2003.10.025</a>. (Cited on page <a href="#pagee7">7</a>).
    </li>
  <li class="bibitem" id="X0-Demetrius01092004">
      Demetrius, L. 2004. “Caloric Restriction, Metabolic Rate, and Entropy.” <span class="cmti-10">The Journals of Gerontology Series A: Biological Sciences and Medical</span> <span class="cmti-10">Sciences </span>59 (9): B902–B915.
      doi:<a href="http://dx.doi.org/10.1093/gerona/59.9.B902">10.1093/gerona/59.9.B902</a>. (Cited on page <a href="#pagee7">7</a>).
    </li>
  <li class="bibitem" id="X0-duffin1969geometric">
      Duffin, R.J., and C. Zener. 1969. “Geometric programming, chemical equilibrium, and the anti-entropy function.” <span class="cmti-10">Proceedings of the National</span> <span class="cmti-10">Academy of Sciences </span>63 (3):
      629. (Cited on page <a href="#pagee41">41</a>).
    </li>
  <li class="bibitem" id="X0-Gheorghiu2005">
      Gheorghiu, S., S. Kjelstrup, P. Pfeifer3, and M.-O. Coppens. 2005. “Is the Lung an Optimal Gas Exchanger?” In <span class="cmti-10">Fractals in Biology and Medicine,</span> edited by Gabriele A. Losa, Danilo Merlini, Theo F.
      Nonnenmacher, and Ewald R. Weibel, 31–42. Mathematics and Biosciences in Interaction. Birkhäuser Basel.
      <span class="cmcsc-10"><span class="small-caps">isbn</span></span>: 978-3-7643-7412-9. doi:
      <a href="http://dx.doi.org/10.1007/3-7643-7412-8_3">10.1007/3-7643-7412-8˙3</a>. (Cited on page <a href="#pagee33">33</a>).
    </li>
  <li class="bibitem" id="X0-gould1989wonderful">
      Gould, S. J. 1989. <span class="cmti-10">Wonderful life: The Burgess Shale and the Nature of</span> <span class="cmti-10">History. </span>New York, USA<span class="cmti-10">: </span>Norton. (Cited on pages
      <a href="#pagee8">8</a> <a href="#pagee9">sq.</a>).
    </li>
  <li class="bibitem" id="X0-gould1997full">_________. 1997. <span class="cmti-10">Full house: The spread of excellence from Plato to Darwin.</span> Three Rivers Pr. (Cited on pages <a href="#pagee8">8</a>, <a href="#pagee15">15</a> <a href="#pagee16">sq.</a>).</li>
  <li class="bibitem" id="X0-Hardy_1934">
      Hardy, J. D. 1934. “The radiation of heat from the human body iii.” <span class="cmti-10">Journal</span> <span class="cmti-10">of Clinical Investigation </span>13 (4): 615–620. doi:
      <a href="http://dx.doi.org/10.1172/JCI100609.">10.1172/JCI100609.</a> (Cited on page <a href="#pagee5">5</a>).
    </li>
  <li class="bibitem" id="X0-hayflick">
      Hayflick, L. 2007. “Entropy Explains Aging, Genetic Determinism Explains Longevity, and Undefined Terminology Explains Misunderstanding Both.” <span class="cmti-10">PLoS Genet </span>3, no. 12 (December): e220. doi:
      <a href="http://dx.doi.org/10.1371/journal.pgen.0030220">10.1371/journal.pgen.0030220</a>. (Cited on pages <a href="#pagee5">5</a> <a href="#pagee6">sq.</a>).
    </li>
  <li class="bibitem" id="X0-horsfield1977">
      Horsfield, K. 1977. “Morphology of branching trees related to entropy.” <span class="cmti-10">Respiration Physiology </span>29 (2): 179. doi:
      <a href="http://dx.doi.org/10.1016/0034-5687(77)90090-1">10.1016/0034-5687(77)90090-1</a>. (Cited on page <a href="#pagee33">33</a>).
    </li>
  <li class="bibitem" id="X0-kauffman2002investigations">Kauffman, S.A. 2002. <span class="cmti-10">Investigations. </span>Oxford University Press, USA. (Cited on page <a href="#pagee34">34</a>).</li>
  <li class="bibitem" id="X0-Lambert">Lambert, F.L. 2007. <span class="cmti-10">Entropy and the second law of thermodynamics. </span>(Cited on page <a href="#pagee5">5</a>).</li>
  <li class="bibitem" id="X0-AnnickLesne08">
      Lesne, A. 2008. “Robustness: Confronting lessons from physics and biology.” <span class="cmti-10">Biol Rev Camb Philos Soc</span> 83 (4): 509–532. doi:
      <a href="http://dx.doi.org/10.1111/j.1469-185X.2008.00052.x">10.1111/j.1469-185X.2008.00052.x</a>. (Cited on page <a href="#pagee33">33</a>).
    </li>
  <li class="bibitem" id="X0-Lesne06">
      Lesne, A., and J.-M. Victor. 2006. “Chromatin fiber functional organization: Some plausible models.” <span class="cmti-10">Eur Phys J E Soft Matter </span>19 (3): 279–290. doi:
      <a href="http://dx.doi.org/10.1140/epje/i2005-10050-6">10.1140/epje/i2005-10050-6</a>. (Cited on page <a href="#pagee34">34</a>).
    </li>
  <li class="bibitem" id="X0-Lindner_2008">
      Lindner, A.B., R. Madden, A. Demarez, E.J. Stewart, and F. Taddei. 2008. “Asymmetric segregation of protein aggregates is associated with cellular aging and rejuvenation.”
      <span class="cmti-10">Proceedings of the National Academy of Sciences</span> 105 (8): 3076–3081. doi:<a href="http://dx.doi.org/10.1073/pnas.0708931105">10.1073/pnas.0708931105</a>. (Cited on page <a href="#pagee6">6</a>).
    </li>
  <li class="bibitem" id="X0-longocri">
      Longo, G. 2009. “Critique of Computational Reason in the Natural Sciences.” In <span class="cmti-10">Fundamental Concepts in Computer Science, </span>edited by E. Gelenbe and J.-P. Kahane. Imperial College Press/World
      Scientific. (Cited on page <a href="#pagee41">41</a>).
    </li>
  <li class="bibitem" id="X0-longo2012">
      Longo, G., P.-A. Miquel, C. Sonnenschein, and A. M. Soto. 2012. “Is information a proper observable for biological organization?” <span class="cmti-10">Progress</span>
      <span class="cmti-10">in Biophysics and Molecular biology </span>109 (3): 108–114.
      <span class="cmcsc-10"><span class="small-caps">issn</span></span>: 0079-6107. doi:
      <a href="http://dx.doi.org/10.1016/j.pbiomolbio.2012.06.004">10.1016/j.pbiomolbio.2012.06.004</a>. (Cited on page <a href="#pagee32">32</a>).
    </li>
  <li class="bibitem" id="X0-longo2011c">
      Longo, G., and M. Montévil. 2011. “From physics to biology by extending criticality and symmetry breakings.” <span class="cmti-10">Progress in Biophysics and Molecular</span> <span class="cmti-10">Biology </span>106 (2):
      340–347. <span class="cmcsc-10"><span class="small-caps">issn</span></span>: 0079-6107. doi:
      <a href="http://dx.doi.org/10.1016/j.pbiomolbio.2011.03.005">10.1016/j.pbiomolbio.2011.03.005</a>. (Cited on pages <a href="#pagee13">13</a>, <a href="#pagee25">25</a>, <a href="#pagee32">32</a> <a href="#pagee33">sq.</a>).
    </li>
  <li class="bibitem" id="bib-23">
      _________. 2012. “Randomness Increases Order in Biological Evolution.” In <span class="cmti-10">Computation, Physics and Beyond, </span>edited by M. Dinneen, B. Khoussainov, and A. Nies, 7160:289–308. Lecture Notes in Computer
      Science. Invited paper, Auckland, New Zealand, February 21-24, 2012. Springer Berlin / Heidelberg.
      <span class="cmcsc-10"><span class="small-caps">isbn</span></span>: 978-3-642-27653-8. doi:
      <a href="http://dx.doi.org/10.1007/978-3-642-27654-5_22">10.1007/978-3-642-27654-5˙22</a>. (Cited on page <a href="#pagee41">41</a>).
    </li>
  <li class="bibitem" id="X0-luo2005axon">
      Luo, L., and D.M. O’Leary. 2005. “Axon retraction and degeneration in development and disease.” <span class="cmti-10">Annual Review of Neuroscience </span>28:127–156. doi:
      <a href="http://dx.doi.org/10.1146/annurev.neuro.28.061604.135632">10.1146/annurev.neuro.28.061604.135632</a>. (Cited on page <a href="#pagee32">32</a>).
    </li>
  <li class="bibitem" id="X0-Marineo">
      Marineo, G., and F. Marotta. 2005. “Biophysics of aging and therapeutic interventions by entropy-variation systems.” <span class="cmti-10">Biogerontology </span>6 (1): 77–79.
      <span class="cmcsc-10"><span class="small-caps">issn</span></span>: 1389-5729. doi:
      <a href="http://dx.doi.org/10.1007/s10522-004-7387-6">10.1007/s10522-004-7387-6</a>. (Cited on page <a href="#pagee6">6</a>).
    </li>
  <li class="bibitem" id="X0-Mauroy2004">
      Mauroy, B., M. Filoche, E.R. Weibel, and B. Sapoval. 2004. “An optimal bronchial tree may be dangerous.” <span class="cmti-10">Nature </span>427:633–636. doi:
      <a href="http://dx.doi.org/10.1038/nature02287">10.1038/nature02287</a>. (Cited on page <a href="#pagee33">33</a>).
    </li>
  <li class="bibitem" id="X0-McShea2010">
      McShea, D.W., and R.N. Brandon. 2010. <span class="cmti-10">Biology’s first law: the tendency</span> <span class="cmti-10">for diversity and complexity to increase in evolutionary systems. </span>University of Chicago Press.
      (Cited on page <a href="#pagee41">41</a>).
    </li>
  <li class="bibitem" id="X0-Nelson_1990">
      Nelson, T.R., B.J. West, and A.L. Goldberger. 1990. “The fractal lung: Universal and species-related scaling patterns.” <span class="cmti-10">Cellular and Molecular Life</span> <span class="cmti-10">Sciences </span>46:251–254.
      doi:<a href="http://dx.doi.org/10.1007/BF01951755">10.1007/BF01951755</a>. (Cited on page <a href="#pagee33">33</a>).
    </li>
  <li class="bibitem" id="X0-Nystrom_2007">
      Nyström, T. 2007. “A Bacterial Kind of Aging.” <span class="cmti-10">PLoS Genet </span>3 (12): e224. doi:<a href="http://dx.doi.org/10.1371/journal.pgen.0030224">10.1371/journal.pgen.0030224</a>. (Cited on page
      <a href="#pagee6">6</a>).
    </li>
  <li class="bibitem" id="X0-Olshansky_2005">
      Olshansky, S., and S. Rattan. 2005. “At the Heart of Aging: Is it Metabolic Rate or Stability?” <span class="cmti-10">Biogerontology </span>6:291–295.
      <span class="cmcsc-10"><span class="small-caps">issn</span></span>: 1389-5729. doi:
      <a href="http://dx.doi.org/10.1007/s10522-005-2627-y">10.1007/s10522-005-2627-y</a>. (Cited on page <a href="#pagee7">7</a>).
    </li>
  <li class="bibitem" id="X0-Pezard_1998">
      Pezard, J., L.and Martinerie, F.J. Varela, F. Bouchet, D. Guez, C. Derouesné, and B. Renault. 1998. “Entropy maps characterize drug effects on brain dynamics in Alzheimer’s disease.”
      <span class="cmti-10">Neuroscience Letters </span>253 (1): 5–8. <span class="cmcsc-10"><span class="small-caps">issn</span></span>:
      0304-3940. doi:<a href="http://dx.doi.org/10.1016/S0304-3940(98)00603-X">10.1016/S0304-3940(98)00603-X</a>. (Cited on page <a href="#pagee6">6</a>).
    </li>
  <li class="bibitem" id="X0-ruud1954vertebrates">Ruud, J.T. 1954. “Vertebrates without erythrocytes and blood pigment.” <span class="cmti-10">Nature </span>173 (4410): 848. (Cited on page <a href="#pagee12">12</a>).</li>
  <li class="bibitem" id="X0-schrodinger">Schrödinger, E. 1944. <span class="cmti-10">What Is Life? </span>Cambridge U.P. (Cited on pages <a href="#pagee5">5</a>, <a href="#pagee19">19</a>).</li>
  <li class="bibitem" id="X0-Sohal_1996">
      Sohal, R.S., and R. Weindruch. 1996. “Oxidative stress, caloric restriction, and aging.” <span class="cmti-10">Science </span>273 (5271): 59–63. doi:
      <a href="http://dx.doi.org/10.1126/science.273.5271.59">10.1126/science.273.5271.59</a>. (Cited on page <a href="#pagee7">7</a>).
    </li>
  <li class="bibitem" id="X0-west1999">
      West, G.B., J.H. Brown, and B.J. Enquist. 1999. “The Fourth Dimension of Life: Fractal Geometry and Allometric Scaling of Organisms.” <span class="cmti-10">Science </span>284 (5420): 1677–1679. doi:
      <a href="http://dx.doi.org/10.1126/science.284.5420.1677">10.1126/science.284.5420.1677</a>. (Cited on page <a href="#pagee33">33</a>).
  </li>
</ol>

<aside class="footnotes">
  <p class="noindent"><span class="footnote-mark"><a href="#fn3x0-bk" id="fn3x0"><sup class="textsuperscript">3</sup> </a></span> Published as: <span id="citee0:Longo_2012_Randomness" class="cmr-9">G. Longo and M. Mont</span>évil. 2012. “Randomness Increases Order in Biological Evolution.” In <span class="cmti-10">Computation, Physics and Beyond, </span>edited by M. Dinneen, B. Khoussainov, and A. Nies, 7160:289–308. Lecture Notes in Computer Science. Invited paper, Auckland, New Zealand, February 21-24, 2012. Springer Berlin / Heidelberg. <span class="cmcsc-10x-x-90"><span class="small-caps">isbn</span></span>: 978-3-642-27653-8. doi:
<a href="http://dx.doi.org/10.1007/978-3-642-27654-5_22">10.1007/978-3-642-27654-5˙22</a>.</p>
  <p class="indent"><span class="footnote-mark"><a href="#fn4x0-bk" id="fn4x0"><sup class="textsuperscript">4</sup> </a></span> However, the argument that disorder is an epistemic notion, not suitable to physics, is less convincing, since classical randomness, at the core of entropy, is also epistemic (see above and Bailly and Longo (
    <a href="#X0-bailly2007">2007</a>)).
  </p>
  <p class="indent"><span class="footnote-mark"><a href="#fn5x0-bk" id="fn5x0"><sup class="textsuperscript">5</sup> </a></span> The word anti-entropy has already been used, apparently only once and in physics, as the mathematical dual of entropy: its minimum coincides with the entropy maximum at the equilibrium, in mixture of gases at constant temperature and volume (Duffin and Zener <a href="#X0-duffin1969geometric" id="pagee41">1969</a>). This is a specific and a very different context from ours. Our anti-entropy is a new concept and observable with respect to both negentropy and the mathematical dual of entropy: typically, it does not add to an equal quantity of entropy to give
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mn>0</mn>
    </math> (as negentropy), nor satisfies minimax equations, but it refers to the quantitative approach to “biological organization”, as opposing entropy by the various forms of biological morphogenesis, replacement and repair.
  </p>
  <p class="indent"><span class="footnote-mark"><a href="#fn6x0-bk" id="fn6x0"><sup class="textsuperscript">6</sup> </a></span> The incompetent computationalist (incompetent in Theory of Computation), who would say that also computers are not identical and misses the point: the <span class="cmti-10">theory </span>of programming is based on identical iteration of software processes on reliable hardware, i.e. functionally equivalent hardware (and it works, even in computer networks, see the analysis of primitive recursion and portability of software in Longo (<a id="pagee8"  href="#X0-longocri">2009</a>))). Any biological theory, instead, must deal with variability, <span class="cmti-10">by principle</span>. As recalled above, variability as never identical iteration, in biology, is not an error.
  </p>
  <p class="indent"><span class="footnote-mark"><a href="#fn7x0-bk" id="fn7x0"><sup class="textsuperscript">7</sup> </a></span> Some may prefer to consider viruses as the least form of life. The issue is controversial, but it would not change at all Gould’s and ours perspective: we only need a minimum which differs from inert matter.
  </p>
  <p class="indent"><span class="footnote-mark"><a href="#fn8x0-bk" ><sup class="textsuperscript" id="fn8x0">8</sup> </a></span> By our approach, proposed in Bailly and Longo (<a href="#X0-bailly2009">2009</a>), we provide a theoretical/mathematical justification of the ZFEL principle in McShea and Brandon (
    <a href="#X0-McShea2010">2010</a>), at the core of their very interesting biological analysis: “ZFEL (Zero Force Evolutionary Law, general formulation): In any evolutionary system in which there is variation and heredity, there is a tendency for diversity and complexity to increase, one that is always present but may be opposed or augmented by natural selection, other forces, or constraints acting on diversity or complexity.”
  </p>
  <p class="indent"><span class="footnote-mark"><a href="#fn9x0-bk" id="fn9x0"><sup class="textsuperscript">9</sup> </a></span> In short, Schrödinger transforms an equation with the structure
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>E</mi>
      <mo class="MathClass-rel">=</mo>
      <mfrac>
        <mrow>
          <msup>
            <mrow>
              <mi>p</mi>
            </mrow>
            <mrow>
              <mn>2</mn>
            </mrow>
          </msup>
        </mrow>
        <mrow>
          <mn>2</mn>
          <mi>m</mi>
        </mrow>
      </mfrac>
      <mo class="MathClass-bin">+</mo>
      <mi>V</mi>
      <mrow>
        <mo class="MathClass-open">(</mo>
        <mrow>
          <mi>x</mi>
        </mrow>
        <mo class="MathClass-close">)</mo>
      </mrow>
    </math>, where
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>V</mi>
      <mrow>
        <mo class="MathClass-open">(</mo>
        <mrow>
          <mi>x</mi>
        </mrow>
        <mo class="MathClass-close">)</mo>
      </mrow>
    </math>
    is a potential, by associating 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>E</mi>
    </math> and
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>p</mi>
    </math> to the differential operators 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>∂</mi>
      <mo class="MathClass-bin">∕</mo>
      <mi>∂</mi>
      <mi>t</mi>
    </math> and 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>∂</mi>
      <mo class="MathClass-bin">∕</mo>
      <mi>∂</mi>
      <mi>x</mi>
    </math>, respectively, seeBailly and Longo (
    <a href="#X0-bailly2009">2009</a>)
  </p>
  <p class="indent"><span class="footnote-mark"><a href="#fn10x0-bk" id="fn10x0"><sup class="textsuperscript">10</sup> </a></span> In theory of information, 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mo class="qopname">log</mo>
      
      <mrow>
        <mo class="MathClass-open">(</mo>
        <mrow>
          <mn>1</mn>
          <mo class="MathClass-bin">∕</mo>
          <msub>
            <mrow>
              <mi>q</mi>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </msub>
        </mrow>
        <mo class="MathClass-close">)</mo>
      </mrow>
    </math>
    is the information associated to 
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>j</mi>
    </math>: it quantifies its scarcity. If one assume that
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
        <mrow>
          <mi>a</mi>
        </mrow>
        <mrow>
          <mi>j</mi>
        </mrow>
      </msub>
      <mo class="MathClass-rel">=</mo>
      <mrow>
        <mo class="MathClass-open">⟨</mo>
        <mrow>
          <msub>
            <mrow>
              <mi>a</mi>
            </mrow>
            <mrow>
              <mi>j</mi>
            </mrow>
          </msub>
        </mrow>
        <mo class="MathClass-close">⟩</mo>
      </mrow>
      <mo class="MathClass-bin">±</mo>
      <mi>a</mi>
    </math>
    and that we keep the mean complexity of cells, the anti-correlation is typically obtained when we have more low complexity cell types, with fewer cells, than high complexity cell types (which have therefore more cells). If one consider again the
    
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
        <mrow>
          <mi>a</mi>
        </mrow>
        <mrow>
          <mi>j</mi>
        </mrow>
      </msub>
    </math>
    as a degree of freedom, the same result can be achieved high complexity cell types with very high complexity and therefore a high number of bellow average complexity cell types.
  </p>
</aside>
